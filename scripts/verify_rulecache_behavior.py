#!/usr/bin/env python3
"""
Verification script for RuleCache 4x pattern analysis.

This script demonstrates that the 4x log pattern is due to parallel execution
but actual file loading happens only once thanks to @cached decorator.
"""

import logging
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

# Configure logging to see the 4x pattern
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s.%(msecs)03d - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)


def simulate_rule_module(module_name: str, thread_id: int) -> dict:
    """
    Simulate a rule module loading rules.

    Args:
        module_name: Name of the module (e.g., 'ARAI', 'CON')
        thread_id: Thread identifier for logging

    Returns:
        Dictionary of rules
    """
    logger.info(f"[Thread {thread_id}] {module_name}RulesModule starting...")

    # Import and call the cached manager (same as real modules do)
    from toetsregels.cached_manager import get_cached_toetsregel_manager

    manager = get_cached_toetsregel_manager()

    logger.info(
        f"[Thread {thread_id}] {module_name}RulesModule calling get_all_regels()..."
    )
    start = time.perf_counter()

    all_rules = manager.get_all_regels()

    elapsed_ms = (time.perf_counter() - start) * 1000

    logger.info(
        f"[Thread {thread_id}] {module_name}RulesModule got {len(all_rules)} rules in {elapsed_ms:.2f}ms"
    )

    return all_rules


def main():
    """Run verification test."""
    logger.info("=" * 80)
    logger.info("RuleCache 4x Pattern Verification")
    logger.info("=" * 80)
    logger.info("")

    # Clear cache for clean test
    from toetsregels.rule_cache import get_rule_cache

    cache = get_rule_cache()
    cache.clear_cache()
    logger.info("‚úÖ Cache cleared for clean test")
    logger.info("")

    # Simulate 4 rule modules executing in parallel (like PromptOrchestrator does)
    modules = ["ARAI", "CON", "ESS", "SAM"]

    logger.info(
        f"üöÄ Simulating {len(modules)} rule modules in parallel (max_workers=4)..."
    )
    logger.info("Watch for 4x 'Loading 53 regel files' but only 1x success log!")
    logger.info("")

    start_time = time.perf_counter()

    with ThreadPoolExecutor(max_workers=4) as executor:
        # Submit all 4 modules simultaneously
        future_to_module = {
            executor.submit(simulate_rule_module, module, i + 1): module
            for i, module in enumerate(modules)
        }

        # Collect results
        results = []
        for future in as_completed(future_to_module):
            module_name = future_to_module[future]
            try:
                result = future.result()
                results.append((module_name, result))
            except Exception as e:
                logger.error(f"Module {module_name} failed: {e}")

    total_time_ms = (time.perf_counter() - start_time) * 1000

    logger.info("")
    logger.info("=" * 80)
    logger.info("VERIFICATION RESULTS")
    logger.info("=" * 80)

    # Verify all results are the same dict reference
    if len(results) == 4:
        first_result = results[0][1]
        all_same = all(result is first_result for _, result in results)

        if all_same:
            logger.info("‚úÖ PASS: All 4 modules got the SAME dict reference")
            logger.info("   ‚Üí This proves only 1 actual load happened!")
        else:
            logger.error("‚ùå FAIL: Modules got different dict references")
            logger.error("   ‚Üí This would indicate cache is not working!")

        logger.info(f"‚úÖ Total execution time: {total_time_ms:.2f}ms")
        logger.info(f"‚úÖ Rules loaded: {len(first_result)}")

        # Analyze timing
        if total_time_ms < 30:
            logger.info("‚úÖ Fast execution indicates cache is working efficiently")
        else:
            logger.warning(
                f"‚ö†Ô∏è  Slow execution ({total_time_ms:.0f}ms) might indicate cache issues"
            )

    else:
        logger.error(f"‚ùå FAIL: Expected 4 results, got {len(results)}")

    logger.info("")
    logger.info("=" * 80)
    logger.info("CACHE STATISTICS")
    logger.info("=" * 80)

    stats = cache.get_stats()
    logger.info(f"Total get_all_calls: {stats['get_all_calls']}")
    logger.info(f"Total rules cached: {stats['total_rules_cached']}")
    logger.info(f"Cache directory: {stats['cache_dir']}")

    if "monitoring" in stats:
        mon = stats["monitoring"]
        logger.info(f"Cache hit rate: {mon['hit_rate']:.1%}")
        logger.info(f"Average operation: {mon['avg_operation_ms']:.2f}ms")
        logger.info(f"Hits: {mon['hits']}, Misses: {mon['misses']}")

    logger.info("")
    logger.info("=" * 80)
    logger.info("EXPECTED BEHAVIOR IN LOGS")
    logger.info("=" * 80)
    logger.info("You should see ABOVE:")
    logger.info("  ‚úÖ 4x 'CachedToetsregelManager ge√Ønitialiseerd'")
    logger.info("  ‚úÖ 4x 'Loading 53 regel files van ...'")
    logger.info("  ‚úÖ ONLY 1x '‚úÖ 53 regels succesvol geladen'")
    logger.info("")
    logger.info("This proves:")
    logger.info("  ‚Ä¢ 4 threads entered _load_all_rules_cached() function")
    logger.info("  ‚Ä¢ @cached decorator blocked 3 threads (gave them cached result)")
    logger.info("  ‚Ä¢ Only 1 thread actually loaded from disk")
    logger.info("  ‚Ä¢ All 4 modules got same dict reference (zero duplication)")
    logger.info("")
    logger.info("‚úÖ US-202 fix is working correctly!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
