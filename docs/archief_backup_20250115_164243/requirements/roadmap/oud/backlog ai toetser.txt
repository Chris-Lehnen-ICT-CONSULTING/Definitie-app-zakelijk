# Backlog AI-toetser

## Toekomstige uitbreidingen en suggesties

1. âœ… **Fuzzy match toevoegen voor foute voorbeelden**
   - Fuzzy string matching (zoals Levenshtein-afstand of difflib) gebruiken om ook licht afwijkende foute voorbeelden te detecteren in definitieteksten.
   - Relevante locatie: elke `toets_XXX` functie die `foute_voorbeelden` bevat.

ðŸŸ¡ Uitbreiden toetsers met expliciete check op foute voorbeelden: alle toetsers uitbreiden zodat foute voorbeelden worden meegenomen in de toetsing en terugkoppeling.

ðŸŸ¡ Herstructureer JSON-verwerking om toleranter te zijn voor Xcode-export: detecteer en herstel JSON-bestanden die geÃ«xporteerd zijn als RTF of met dubbele UTF-8 encoding.

ðŸŸ¡ Automatische detectie en verwerking van .rtf-bestanden als JSON: parsing en conversie toevoegen in de loader.

ðŸŸ¡ Tooling voor validatie van toetsregels: script om inconsistenties tussen JSON-regels en implementaties in ai_toetser.py automatisch te detecteren.

ðŸŸ¡ Logging per regel-ID bij toetsresultaten: log naast de uitkomst ook welke regel van toepassing was en welk patroon getriggerd werd.

ðŸŸ¡  Grammaticale analyse voor INT-01 verbeteren: De huidige regex-aanpak voor INT-01 (compacte en begrijpelijke zin) detecteert enkel basale structuren zoals â€˜enâ€™, â€˜ofâ€™, â€˜waarbijâ€™, enz. Overweeg om in de toekomst gebruik te maken van een grammaticaal analysemodel zoals spaCy of een taalmodel om bijzinnen en opsommingen nauwkeuriger te herkennen, inclusief syntactische afhankelijkheden.

âœ… Backlog-item genoteerd: voor elke toetsfunctie in ai_toetser.py voeg je een groene uitlegregel toe, zoals toegepast bij toets_ARAI05. Dit maakt de structuur consistenter en de code beter leesbaar.


Titel: Toetsdefinities via lokale Ã©n GPT-4 methode

Omschrijving:
Voeg in de applicatie een configureerbare mogelijkheid toe om definities te toetsen via:
	1.	Lokale toetsing (met ai_toetser.py en toetsregels.json)
	2.	GPT-4 gebaseerde toetsing (via prompt met alle toetsregels)
	3.	Een gecombineerde modus waarbij beide methoden worden toegepast en de uitkomsten naast elkaar getoond worden

Doel:
Vergelijkbare en complementaire kwaliteitsfeedback bieden aan gebruikers door zowel deterministische als probabilistische toetsing toe te passen. Dit verhoogt de betrouwbaarheid en diversiteit van kwaliteitsfeedback.

Functionaliteiten:
	â€¢	Toggle/selectiemenu in de interface: Toetsmethode kiezen
	â€¢	Indien gecombineerde toetsing geselecteerd: resultaten in twee kolommen tonen (lokaal en GPT-4)
	â€¢	Logging van beide resultaten apart (in JSON en exportbestanden)
	â€¢	Voorkeursmethode opslaan in instellingen

Prioriteit: Midden
Aanbeveling: Optioneel maar waardevol voor geavanceerde gebruikers
Thema: AI-inzet en toetsingsstrategie
Status: Conceptueel
Opmerking: Combineerbaar met het bestaande toetsmechanisme en logging

ðŸŽ¯ Backlogitem: Duidelijke foutuitleg tonen bij CON-01

Titel: Uitleg tonen waarom definitie niet voldoet aan CON-01
Prioriteit: Hoog
Beschrijving:
Wanneer een definitie niet voldoet aan toetsregel CON-01 (â€œEigen definitie voor elke contextâ€), moet het systeem een duidelijke toelichting geven waarom de definitie tekortschiet. Deze uitleg moet verschijnen onder het toetsresultaat in de Streamlit-interface, zowel bij de AI-gegenereerde definitie als bij een aangepaste versie.

Doel:
Gebruiker inzicht geven in het gebrek aan contextonderscheiding, zodat hij/zij gericht kan verbeteren.

Criteria:
	â€¢	Herken dat CON-01 met âŒ is beoordeeld
	â€¢	Toon uitleg als:
â€œDeze definitie maakt geen expliciet onderscheid tussen verschillende contexten (bijv. juridisch, beleidsmatig). Voor correcte toepassing moet per context een aparte formulering worden opgenomen, zoals: â€˜In juridische context betekent Xâ€¦, in operationele context betekent Xâ€¦â€™.â€
	â€¢	Toon dit onder de reguliere regeluitvoer van CON-01

Technisch:
	â€¢	Aanpassing in de toets_CON_01() functie in ai_toetser.py
	â€¢	Conditie toevoegen in definitie_agent_webinterface_logging.py bij het tonen van toetsregels
	â€¢	Uitlegtekst hardcoded of uit JSON uitbreidbaar (optioneel)

Resultaat:
Gebruiker ziet direct waarom CON-01 niet gehaald wordt en wat hij/zij kan verbeteren.


âœ… Backlog item: AI-agent verbeteren via Promptoptimalisatie + Slimmere context (RAG)

Prioriteit: Hoog
Type: Verbetering
Status: Open
Sprint: Volgende iteratie
Doel: Significante kwaliteitsverbetering van gegenereerde definities

â¸»

ðŸ” Beschrijving

De AI-agent moet betrouwbaardere, duidelijkere en beter toetsbare definities genereren. Hiervoor wordt het promptmechanisme geoptimaliseerd en verrijkt met context via RAG-technieken (Retrieval-Augmented Generation).

â¸»

ðŸ“Œ Taken / Subdoelen
	1.	Promptoptimalisatie (prompt engineering):
	â€¢	Voeg expliciete voorbeelden toe van goede en foute definities
	â€¢	Benoem toetsregels (zoals STR-01, INT-07) in de instructie
	â€¢	Duidelijkere aanwijzingen opnemen: wat vermijden, hoe structureren
	2.	Slimmere context via RAG:
	â€¢	Gebruik bestaande module web_lookup.py als basis
	â€¢	Voeg samenvattingslaag toe op opgehaalde teksten
	â€¢	Prioriteer bronnen (bijv. wetten > overheidssites > wiki)
	â€¢	Voeg fallback in als geen webinfo beschikbaar is
	3.	Logische toetsing vooraf:
	â€¢	Laat AI ook expliciet reflecteren: â€œDeze definitie voldoet aan STR-01 omdatâ€¦â€
	â€¢	Integreer dit eventueel als preview voor gebruiker

â¸»

ðŸ§  Verwacht resultaat
	â€¢	Definities die direct beter scoren op toetsregels
	â€¢	Hogere tevredenheid van gebruikers bij hercontrole
	â€¢	Betere explainability (waarom is dit een goede definitie)


âœ… Backlogitem: Ondersteuning voor overstap van GPT-4 API naar lokaal AI-model

Prioriteit: Hoog
Type: Architectuur / Toekomstbestendig maken
Status: Open
Sprint: Volgende iteratie

â¸»

ðŸ” Beschrijving

Momenteel gebruikt de AI-definitieagent het GPT-4 model via de OpenAI API. Hoewel dit uitstekende kwaliteit biedt, zijn er op termijn redenen om te kunnen overstappen naar een lokaal model, zoals:
	â€¢	Verlaging van kosten (API-kosten per call)
	â€¢	Onafhankelijkheid van externe providers
	â€¢	Verbetering van privacy (volledige datacontrole)
	â€¢	Offline kunnen werken

Dit backlogitem beoogt een flexibele architectuur waarmee eenvoudig geschakeld kan worden tussen GPT-4 via API en een lokaal AI-model (zoals via Ollama, LM Studio, of llamacpp).

â¸»

ðŸŽ¯ Doel

Een aanpasbare, modulair opgebouwde promptarchitectuur, zodat je:
	â€¢	Nu effectief werkt met GPT-4 (API)
	â€¢	Later kunt overstappen op een lokaal model zonder herbouw

â¸»

ðŸ§© Taken
	1.	Isolatie van promptfunctionaliteit:
	â€¢	Maak Ã©Ã©n centrale functie zoals stuur_prompt(prompt: str) -> str
	â€¢	Scheid logica voor externe en lokale modellen
	2.	Configuratie-ondersteuning:
	â€¢	Voeg een .env of config-setting toe zoals:
GEBRUIK_LOKAAL_MODEL = True / False
	3.	Implementatie GPT-4 API (bestaand):
	â€¢	Via openai.ChatCompletion.create(...)
	4.	Voorbereiding lokaal model (placeholder):
	â€¢	Stub of voorbeeldimplementatie via:
	â€¢	subprocess.run(...) (bij gebruik van shell CLIâ€™s zoals Ollama)
	â€¢	of requests.post(...) bij lokale API-hosting
	5.	Zorg voor uniforme promptformaten:
	â€¢	Vermijd OpenAI-specifieke opties (function_calling, tool_choice)
	â€¢	Houd de prompt puur tekstueel en begrijpelijk voor alle modellen
	6.	Logging en foutafhandeling generiek houden

â¸»

ðŸ”¬ Testcases
	â€¢	Prompt loopt succesvol via GPT-4 API
	â€¢	Prompt loopt succesvol via lokale mock-functie
	â€¢	Output is tekstueel vergelijkbaar bij beide modellen
	â€¢	Config-wissel schakelt gedrag zonder herstart van de app


