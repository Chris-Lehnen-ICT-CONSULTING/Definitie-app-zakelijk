# DEF-126 Implementation Plan Code Review

**Date:** 2025-11-13
**Reviewer:** Claude Code (Senior Software Developer)
**Review Type:** Implementation Plan & Code Quality Review
**Scope:** Phased Hybrid approach for prompt module consolidation

---

## Executive Summary

**Overall Plan Score: 32/40 (80%)**

| Category | Score | Status |
|----------|-------|--------|
| Clarity | 8/10 | GOOD |
| Completeness | 7/10 | NEEDS IMPROVEMENT |
| Safety | 9/10 | EXCELLENT |
| Effort Accuracy | 8/10 | GOOD |

**Recommendation: REVISE (address critical issues before implementation)**

The implementation plan is fundamentally sound with excellent safety mechanisms (validation gates), realistic effort estimates, and clear phasing. However, there are **3 critical issues** and **5 high-priority improvements** that must be addressed before proceeding.

---

## Code Quality Assessment

### Current Code Quality: 6/10

**Strengths:**
- Clear separation of concerns in module architecture
- Consistent use of `shared_state` pattern in most modules
- Good error handling with try-catch blocks
- Well-documented module interfaces

**Critical Issues:**
1. **Data Access Inconsistency** (Lines 84-98 in definition_task_module.py)
   - Bypasses `shared_state` pattern
   - Inconsistent naming: `juridische_context` vs `juridical_contexts`
   - Multiple fallback chains (`base_ctx.get("X") or base_ctx.get("Y") or []`)

2. **Instruction Redundancy** (context_awareness_module.py)
   - Same instruction appears 2-3x across rich/moderate/minimal methods
   - ~100-150 tokens of duplicate content

3. **Testing Gap**
   - No existing tests for `context.get_shared()` usage in DefinitionTaskModule
   - Tests use mock `enriched_context` but don't validate shared_state

### Proposed Code Quality: 8/10

**Improvements:**
- Consistent data access via `shared_state`
- Single instruction, varied formatting
- Better test coverage for shared_state

**Remaining Concerns:**
- Phase 3 introduces complexity (4 new modules)
- Risk of over-engineering if not carefully managed

---

## CRITICAL ISSUES (Must Fix Before Implementation)

### 1. Phase 1 Code Changes Are INCOMPLETE

**Problem:** The proposed fix only addresses lines 84-98 but misses critical related code.

**Current Bug (lines 84-98):**
```python
# PROBLEMATIC CODE
base_ctx = context.enriched_context.base_context  # Bypasses shared_state
jur_contexts = base_ctx.get("juridische_context") or base_ctx.get("juridisch") or []
wet_basis = base_ctx.get("wettelijke_basis") or base_ctx.get("wettelijk") or []
```

**Proposed Fix (lines 84-98):**
```python
# PROPOSED FIX (INCOMPLETE!)
org_contexts = context.get_shared("organization_contexts", [])
jur_contexts = context.get_shared("juridical_contexts", [])
wet_basis = context.get_shared("legal_basis_contexts", [])
```

**MISSED CODE - Lines 137 also uses jur_contexts/wet_basis:**
```python
# Line 137 - ALSO NEEDS UPDATING
self._build_prompt_metadata(
    begrip, word_type, org_contexts, jur_contexts, wet_basis  # ‚Üê Uses Phase 1 variables
)
```

**Impact:**
- If Phase 1 fix is incomplete, line 137 will reference undefined variables
- This will cause runtime errors immediately

**Required Fix:**
```python
# Phase 1 must also update lines 81-84 to retrieve org_contexts
# Currently, org_contexts is retrieved BEFORE the bug section:

# Line 83 (CURRENT):
org_contexts = context.get_shared("organization_contexts", [])

# Lines 84-98 (PROPOSED FIX):
jur_contexts = context.get_shared("juridical_contexts", [])
wet_basis = context.get_shared("legal_basis_contexts", [])

# Line 137 remains unchanged (already uses org_contexts from line 83)
```

**Correction:** Actually, re-reading the code, `org_contexts` is ALREADY retrieved via `get_shared` on line 83. The plan's proposed fix is missing that line in the "BEFORE" section. The fix should show:

```python
# COMPLETE FIX (lines 81-98):

# BEFORE:
org_contexts = context.get_shared("organization_contexts", [])  # Line 83 - KEEP THIS
# Derive juridical and legal-basis contexts from enriched base_context
try:
    base_ctx = (
        context.enriched_context.base_context
        if context and context.enriched_context
        else {}
    )
except Exception:
    base_ctx = {}
jur_contexts = (
    base_ctx.get("juridische_context") or base_ctx.get("juridisch") or []
)
wet_basis = (
    base_ctx.get("wettelijke_basis") or base_ctx.get("wettelijk") or []
)

# AFTER:
org_contexts = context.get_shared("organization_contexts", [])  # Keep existing
jur_contexts = context.get_shared("juridical_contexts", [])     # New
wet_basis = context.get_shared("legal_basis_contexts", [])       # New
```

**Plan Update Required:**
- Update Phase 1 Section 1 to show complete code transformation (lines 81-98, not 84-98)
- Clarify that `org_contexts` line is preserved
- Add explicit note about line 137 dependency

---

### 2. Phase 1 Will Break Without Prerequisite Changes

**Problem:** The plan assumes `context.get_shared("juridical_contexts")` will return data, but **WHO SETS THIS DATA?**

**Analysis of Code Flow:**

Looking at `context_awareness_module.py` lines 368-395:

```python
def _share_traditional_context(self, context: ModuleContext) -> None:
    """Share all active context types for other modules."""
    base_context = context.enriched_context.base_context

    # Extract alle ACTIEVE contexten
    org_contexts = self._extract_contexts(base_context.get("organisatorisch"))
    jur_contexts = self._extract_contexts(base_context.get("juridisch"))
    wet_contexts = self._extract_contexts(base_context.get("wettelijk"))

    # Deel alle actieve contexten voor andere modules
    if org_contexts:
        context.set_shared("organization_contexts", org_contexts)
    if jur_contexts:
        context.set_shared("juridical_contexts", jur_contexts)  # ‚Üê SETS THE DATA
    if wet_contexts:
        context.set_shared("legal_basis_contexts", wet_contexts)
```

**Critical Dependency:**
- `ContextAwarenessModule._share_traditional_context()` sets the shared_state values
- `DefinitionTaskModule.execute()` reads these values
- **Module execution order matters!**

**Validation Required:**

The plan MUST verify:
1. Module execution order: Does `ContextAwarenessModule` run BEFORE `DefinitionTaskModule`?
2. Dependencies declared: Does `DefinitionTaskModule.get_dependencies()` include `context_awareness`?

**Current Code Check:**

```python
# definition_task_module.py line 170
def get_dependencies(self) -> list[str]:
    return ["semantic_categorisation"]  # ‚Üê MISSING "context_awareness"!
```

**CRITICAL BUG FOUND:**
- `DefinitionTaskModule` declares dependency on `semantic_categorisation` only
- It does NOT declare dependency on `context_awareness`
- This means Phase 1 fix will FAIL if execution order is wrong

**Required Fix for Phase 1:**

```python
# definition_task_module.py line 170
def get_dependencies(self) -> list[str]:
    return ["semantic_categorisation", "context_awareness"]  # Add dependency
```

**Plan Update Required:**
- Add dependency declaration to Phase 1 Tasks
- Add test to verify module execution order
- Document the data flow: `ContextAwarenessModule` ‚Üí `shared_state` ‚Üí `DefinitionTaskModule`

---

### 3. Phase 2 Token Reduction Estimates Are Optimistic

**Problem:** The plan claims 26-39% token reduction (100-150 tokens), but the calculation is flawed.

**Plan's Estimate (lines 556-566):**
```
Scenario              | Before | After  | Reduction
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
rich_context          | 290    | 120    | 58.6%
moderate_context      | 240    | 92     | 61.7%
minimal_context       | 180    | 45     | 75.0%
no_context            | 38     | 10     | 73.7%
multiple_contexts     | 310    | 135    | 56.5%
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AVERAGE               | 212    | 80     | 62.3%
```

**Reality Check:**

1. **"Before" numbers are context-only, not full prompt**
   - The table shows ~38-310 tokens per scenario
   - Full prompts are typically 2000-4000 tokens (including all modules)
   - Context sections are ~5-15% of total prompt

2. **62.3% reduction of context != 62.3% reduction of total prompt**
   - If context is 10% of prompt (200 tokens out of 2000)
   - And you reduce context by 60% (200 ‚Üí 80 = 120 tokens saved)
   - Total prompt reduction is 120/2000 = **6%**, not 60%!

3. **Plan claims "100-150 tokens reduction" but table shows "212 ‚Üí 80 = 132 tokens"**
   - This is consistent with the calculation
   - But as % of TOTAL prompt, this is much less impactful

**Corrected Expectations:**

Assuming average full prompt is ~2500 tokens:
- Context section: ~200 tokens (8% of prompt)
- Context reduction: 200 ‚Üí 80 = 120 tokens saved
- **Total prompt reduction: 120/2500 = 4.8%**

**The 26-39% reduction claim is misleading** because it's relative to context-only, not total prompt.

**Plan Update Required:**
- Recalculate token reduction as % of FULL prompt (not just context section)
- Update Decision Gate 2 thresholds to reflect realistic impact
- Set expectations: Phase 2 likely achieves **5-8% total prompt reduction**, not 26-39%
- Clarify that 26-39% is context-section-only reduction

---

## HIGH-PRIORITY IMPROVEMENTS

### 4. Phase 1 Test Coverage Is Insufficient

**Plan's Test (lines 296-318):**
```python
def test_uses_shared_state_not_direct_access():
    # Create context with shared_state ONLY
    context = create_test_context(begrip="vergunning")
    context.set_shared("juridical_contexts", ["Strafrecht"])
    context.set_shared("legal_basis_contexts", ["Wetboek van Strafrecht"])

    # DO NOT set enriched_context.base_context
    # Module should use shared_state ONLY

    output = module.execute(context)

    # Verify context metadata includes Strafrecht
    assert "Strafrecht" in output.content or "Strafrecht" in str(output.metadata)
```

**Problem:** This test is too weak. It only checks if "Strafrecht" appears somewhere, not if it's used correctly.

**Missing Test Cases:**

1. **Test empty shared_state with populated base_context**
   - Verify module IGNORES base_context
   - Ensures no fallback to old pattern

2. **Test consistency with ErrorPreventionModule**
   - Both modules should read same shared_state keys
   - Verify they get identical data

3. **Test Module.get_dependencies() includes context_awareness**
   - Ensures execution order is guaranteed

4. **Test _build_prompt_metadata() receives correct data**
   - Verify line 137 doesn't break
   - Check metadata section shows context correctly

**Improved Test Suite:**

```python
def test_uses_shared_state_not_direct_access_strict():
    """
    Strict test: Module MUST use shared_state, NOT base_context.

    Even if base_context has data, module should ignore it.
    """
    module = DefinitionTaskModule()
    module.initialize({})

    # Create context with CONFLICTING data sources
    context = create_test_context(begrip="vergunning")

    # Set shared_state (correct source)
    context.set_shared("juridical_contexts", ["Strafrecht"])
    context.set_shared("legal_basis_contexts", ["Wetboek van Strafrecht"])

    # Set base_context (incorrect source - should be ignored)
    context.enriched_context.base_context = {
        "juridische_context": ["Bestuursrecht"],  # Different!
        "wettelijke_basis": ["Algemene wet bestuursrecht"],  # Different!
    }

    output = module.execute(context)

    # MUST use shared_state (Strafrecht), NOT base_context (Bestuursrecht)
    assert "Strafrecht" in str(output.metadata)
    assert "Bestuursrecht" not in str(output.metadata)

    # Verify metadata section is correct
    assert "Wetboek van Strafrecht" in output.content


def test_consistency_with_error_prevention_module():
    """
    DefinitionTaskModule and ErrorPreventionModule must read same data.
    """
    from src.services.prompts.modules.error_prevention_module import ErrorPreventionModule

    context = create_test_context(begrip="vergunning")
    context.set_shared("juridical_contexts", ["Strafrecht", "Bestuursrecht"])
    context.set_shared("legal_basis_contexts", ["Wetboek van Strafrecht"])

    # Execute both modules
    def_module = DefinitionTaskModule()
    def_module.initialize({})
    def_output = def_module.execute(context)

    err_module = ErrorPreventionModule()
    err_module.initialize({})
    err_output = err_module.execute(context)

    # Both should reference same contexts
    assert "Strafrecht" in def_output.metadata
    # (ErrorPreventionModule may not expose metadata, check content)


def test_dependencies_include_context_awareness():
    """
    DefinitionTaskModule MUST depend on ContextAwarenessModule.
    """
    module = DefinitionTaskModule()
    deps = module.get_dependencies()

    assert "context_awareness" in deps, (
        "DefinitionTaskModule depends on shared_state from ContextAwarenessModule. "
        "Dependency must be declared to ensure correct execution order."
    )


def test_prompt_metadata_section_integrity():
    """
    Verify _build_prompt_metadata() receives correct data (line 137 dependency).
    """
    module = DefinitionTaskModule()
    module.initialize({"include_metadata": True})

    context = create_test_context(begrip="vergunning")
    context.set_shared("word_type", "zelfstandig naamwoord")
    context.set_shared("organization_contexts", ["NP", "OM"])
    context.set_shared("juridical_contexts", ["Strafrecht"])
    context.set_shared("legal_basis_contexts", ["Wetboek van Strafrecht"])

    output = module.execute(context)

    # Verify metadata section shows all contexts
    assert "Organisatorische context: NP, OM" in output.content
    assert "Juridische context: Strafrecht" in output.content
    assert "Wettelijke basis: Wetboek van Strafrecht" in output.content
```

**Plan Update Required:**
- Replace weak test with 4 comprehensive tests
- Add tests to Phase 1 validation checklist
- Increase test time estimate from 1h to 1.5h

---

### 5. Phase 2 Consolidation Approach Has Architectural Flaw

**Plan's Approach (lines 418-460):**

Create `_build_unified_context_section()` that:
1. Extracts contexts from `enriched_context.base_context`
2. Shares via `shared_state`
3. Formats based on score

**Architectural Problem:**

This approach **DUPLICATES** the logic already in `ContextAwarenessModule._share_traditional_context()`:

```python
# EXISTING CODE (context_awareness_module.py lines 368-395)
def _share_traditional_context(self, context: ModuleContext) -> None:
    base_context = context.enriched_context.base_context
    org_contexts = self._extract_contexts(base_context.get("organisatorisch"))
    jur_contexts = self._extract_contexts(base_context.get("juridisch"))
    wet_contexts = self._extract_contexts(base_context.get("wettelijk"))
    # Share...

# PROPOSED CODE (lines 432-438)
def _build_unified_context_section(self, context: ModuleContext, context_score: float) -> str:
    base_ctx = context.enriched_context.base_context  # ‚Üê DUPLICATE ACCESS
    org_contexts = self._extract_contexts(base_ctx.get("organisatorisch"))  # ‚Üê DUPLICATE LOGIC
    jur_contexts = self._extract_contexts(base_ctx.get("juridisch"))  # ‚Üê DUPLICATE LOGIC
    # ...
```

**DRY Violation:**
- `_extract_contexts()` is called twice (once in `_share_traditional_context()`, once in `_build_unified_context_section()`)
- Both methods read from `base_context` directly
- Violates Single Source of Truth principle

**Better Approach:**

```python
def _build_unified_context_section(
    self,
    context: ModuleContext,
    context_score: float,
) -> str:
    """
    Build unified context section (DEF-126 Phase 2).

    Uses shared_state populated by _share_traditional_context().
    """
    sections = []

    # READ from shared_state (already populated by _share_traditional_context)
    org_contexts = context.get_shared("organization_contexts", [])
    jur_contexts = context.get_shared("juridical_contexts", [])
    wet_contexts = context.get_shared("legal_basis_contexts", [])

    # ONE instruction
    sections.append("üìå VERPLICHTE CONTEXT INFORMATIE:")
    sections.append(
        "‚ö†Ô∏è Gebruik onderstaande context om de definitie specifiek te maken "
        "voor deze organisatorische, juridische en wettelijke setting, "
        "zonder de context expliciet te benoemen."
    )

    # Vary formatting based on score
    if context_score >= 0.8:
        sections.extend(self._format_rich_context(org_contexts, jur_contexts, wet_contexts))
    elif context_score >= 0.5:
        sections.extend(self._format_moderate_context(org_contexts, jur_contexts, wet_contexts))
    else:
        sections.extend(self._format_minimal_context(org_contexts, jur_contexts, wet_contexts))

    return "\n".join(sections)


def execute(self, context: ModuleContext) -> ModuleOutput:
    # Calculate score
    context_score = self._calculate_context_score(context.enriched_context)

    # Share context FIRST (populates shared_state)
    self._share_traditional_context(context)

    # Then build unified section (reads from shared_state)
    content = self._build_unified_context_section(context, context_score)

    return ModuleOutput(content=content, metadata={"context_score": context_score})
```

**Benefits:**
- No duplication of `_extract_contexts()` logic
- Consistent with Phase 1 pattern (use shared_state)
- Single source of truth for context data
- Easier to test and maintain

**Plan Update Required:**
- Refactor `_build_unified_context_section()` to use `shared_state`
- Remove duplicate `base_context` access
- Update Phase 2 code examples in plan

---

### 6. Missing Edge Case: Empty Shared State Handling

**Problem:** The plan doesn't address what happens if `ContextAwarenessModule` doesn't populate shared_state.

**Scenario:**
```python
# If ContextAwarenessModule fails or is disabled:
org_contexts = context.get_shared("organization_contexts", [])  # Returns []
jur_contexts = context.get_shared("juridical_contexts", [])      # Returns []
wet_basis = context.get_shared("legal_basis_contexts", [])       # Returns []

# All contexts are empty - what happens to has_context?
has_context = bool(org_contexts or jur_contexts or wet_basis)  # False
```

**Current Code (lines 99-104):**
```python
has_context = bool(
    org_contexts
    or jur_contexts
    or wet_basis
    # EPIC-010: domain_contexts verwijderd - is legacy
)
```

**This is correct!** But the plan doesn't test this edge case.

**Missing Test:**

```python
def test_handles_empty_shared_state_gracefully():
    """
    Test behavior when ContextAwarenessModule provides no context.

    This can happen if:
    - ContextAwarenessModule fails
    - No context was provided by user
    - Module execution order is wrong
    """
    module = DefinitionTaskModule()
    module.initialize({})

    context = create_test_context(begrip="vergunning")
    # DO NOT set any shared_state (simulate empty context)

    output = module.execute(context)

    # Should still work
    assert output.success is True
    assert output.metadata["has_context"] is False

    # Should show "geen context" message
    assert "geen" in output.content.lower() or "Geen" in output.content
```

**Plan Update Required:**
- Add edge case test to Phase 1
- Verify `has_context` logic is correct
- Document expected behavior for empty shared_state

---

### 7. Phase 0 Baseline Generation Has Dependency Issue

**Plan's Code (lines 43-141):**

```python
from src.services.definition_generator import get_definition_generator

generator = get_definition_generator()
result = generator.generate_definition(
    begrip=test_case["begrip"],
    organisatorische_context=test_case["organisatorische_context"],
    juridische_context=test_case["juridische_context"],
    wettelijke_basis=test_case["wettelijke_basis"]
)
```

**Problem:** This assumes `definition_generator` API accepts these parameters. Need to verify.

**Validation Required:**

Check if `get_definition_generator().generate_definition()` signature matches:
```python
def generate_definition(
    self,
    begrip: str,
    organisatorische_context: list[str] = None,
    juridische_context: list[str] = None,
    wettelijke_basis: list[str] = None,
) -> GenerationResult:
```

**If signature is different** (e.g., uses `context: dict` parameter), the baseline script will fail immediately.

**Recommendation:**
- Add note in Phase 0 to verify API signature before running script
- Provide alternative if signature differs
- Test baseline script on one case before running all 5

---

### 8. Decision Gates Need Clearer Failure Procedures

**Plan's Gate 1 (lines 360-386):**

```
Decision:
- ‚úÖ PASS: Quality maintained ‚Üí Proceed to Phase 2
- ‚ùå FAIL: Quality degraded ‚Üí Investigate root cause, fix, re-validate
```

**Problem:** "Investigate root cause, fix, re-validate" is vague. No specific steps.

**Better Failure Procedure:**

```markdown
### Decision Gate 1 Failure Procedure:

If quality degraded (score <95% of baseline):

1. **Immediate Actions (30 min):**
   - Run full test suite: `pytest tests/services/prompts/modules/ -v`
   - Check logs for errors/warnings
   - Manually test 3 definitions with rich context

2. **Root Cause Analysis (1 hour):**
   - Compare baseline vs. current definitions side-by-side
   - Identify specific quality degradation (ontological category wrong? Missing context?)
   - Check module execution order (is ContextAwarenessModule running first?)
   - Verify shared_state is populated (add debug logging)

3. **Fix Options:**
   - **If execution order issue:** Add dependency, re-run
   - **If data missing:** Check ContextAwarenessModule._share_traditional_context()
   - **If logic error:** Review Phase 1 changes, rollback if needed

4. **Re-Validation (30 min):**
   - Regenerate baseline with fix
   - Compare again
   - If still failing after 2 attempts ‚Üí ABORT Phase 1, consult team

**Max retries:** 2 attempts per gate
**Max investigation time:** 2 hours per gate
**Escalation:** If unresolved after 2h ‚Üí schedule team review, don't proceed
```

**Plan Update Required:**
- Add detailed failure procedures to all 3 gates
- Set max retry limits
- Define escalation path

---

## RECOMMENDATIONS BY PHASE

### Phase 0 Recommendations

**BEFORE starting Phase 0:**

1. ‚úÖ **Verify definition_generator API signature**
   - Check parameter names match baseline script
   - Test script with 1 case before running all 5

2. ‚úÖ **Install tiktoken in isolated environment first**
   - Test token counting works
   - Verify encoding_for_model("gpt-4") succeeds

3. ‚úÖ **Commit baseline to git BEFORE any code changes**
   - Tag commit: `DEF-126-baseline`
   - Enables easy rollback comparison

### Phase 1 Recommendations

**BEFORE starting Phase 1:**

1. üî¥ **Add ContextAwarenessModule to dependencies** (CRITICAL)
   ```python
   def get_dependencies(self) -> list[str]:
       return ["semantic_categorisation", "context_awareness"]
   ```

2. üî¥ **Expand test suite** (CRITICAL)
   - Add 4 comprehensive tests (see section 4)
   - Test conflicting data sources (shared_state vs base_context)
   - Test empty shared_state edge case

3. üü° **Update plan to show complete code change** (HIGH PRIORITY)
   - Lines 81-98 (not 84-98)
   - Clarify org_contexts is preserved
   - Show line 137 dependency

**DURING Phase 1:**

4. ‚úÖ **Manual smoke test after code change, before running tests**
   - Start app, generate 1 definition with context
   - Verify no runtime errors
   - Catch missing imports early

### Phase 2 Recommendations

**BEFORE starting Phase 2:**

1. üü° **Recalculate token reduction expectations** (HIGH PRIORITY)
   - Measure FULL prompt tokens (not just context section)
   - Update Decision Gate 2 thresholds
   - Set realistic target: 5-8% total prompt reduction

2. üü° **Refactor _build_unified_context_section()** (HIGH PRIORITY)
   - Use `shared_state` instead of `base_context`
   - Remove duplicate `_extract_contexts()` calls
   - Follow Phase 1 pattern

3. ‚úÖ **Add token measurement to Phase 1 validation**
   - Measure baseline tokens after Phase 1
   - Detect if Phase 1 changes prompt size (should be minimal)

**DURING Phase 2:**

4. ‚úÖ **Test incremental changes**
   - Refactor one method at a time (_build_rich_context_section first)
   - Run tests after each refactor
   - Don't refactor all three (rich/moderate/minimal) at once

### Phase 3 Recommendations

**BEFORE deciding to proceed to Phase 3:**

1. ‚úÖ **Re-evaluate necessity based on Phase 2 results**
   - If Phase 2 achieves 5-8% reduction + cleaner code ‚Üí STOP
   - Only proceed if architectural improvement is explicit goal
   - Don't proceed for token reduction alone (diminishing returns)

2. ‚úÖ **Review god object concerns carefully**
   - Current `ContextAwarenessModule` is 433 lines (acceptable for single-responsibility)
   - 4 new modules adds 360 lines + integration overhead
   - Ensure splitting truly improves maintainability

---

## TESTING STRATEGY REVIEW

### Current Plan's Testing (INSUFFICIENT)

**Phase 1 Testing (1 hour):**
- 1 new test: `test_uses_shared_state_not_direct_access()`
- Run existing tests
- Manual QA (2-3 definitions)

**Phase 2 Testing (1 hour):**
- 1 new test: `test_single_context_instruction_phase2()`
- Token measurement
- Baseline comparison

**Problem:** This catches obvious breakage but misses subtle bugs.

### Recommended Testing Strategy (COMPREHENSIVE)

**Phase 1 Testing (1.5 hours):**

1. **Unit Tests (4 tests, 45 min):**
   - `test_uses_shared_state_not_direct_access_strict()` - Conflicting data sources
   - `test_consistency_with_error_prevention_module()` - Cross-module consistency
   - `test_dependencies_include_context_awareness()` - Execution order guarantee
   - `test_prompt_metadata_section_integrity()` - Line 137 dependency check

2. **Integration Tests (30 min):**
   - Full prompt generation with context (5 scenarios from baseline)
   - Verify module execution order (add debug logging)
   - Check shared_state is populated before DefinitionTaskModule runs

3. **Edge Case Tests (15 min):**
   - `test_handles_empty_shared_state_gracefully()` - No context provided
   - `test_handles_module_execution_order_failure()` - Wrong order simulation

**Phase 2 Testing (2 hours):**

1. **Unit Tests (3 tests, 45 min):**
   - `test_single_context_instruction_phase2()` - Instruction appears once
   - `test_formatting_varies_by_score()` - Rich/moderate/minimal formatting
   - `test_unified_section_uses_shared_state()` - No base_context access

2. **Token Measurement (45 min):**
   - Full prompts (not just context sections)
   - 5 scenarios √ó 2 runs (before/after) = 10 measurements
   - Statistical analysis (mean, std dev, % reduction)

3. **Quality Validation (30 min):**
   - Baseline comparison (automated)
   - Manual review (5 definitions)
   - Checklist: context preserved? Instructions clear? Formatting correct?

---

## EFFORT ESTIMATE REVIEW

### Plan's Estimates:

| Phase | Estimated | Buffer | Total |
|-------|-----------|--------|-------|
| Phase 0 | 1h | 0.5h | 1.5h |
| Phase 1 | 3h | 1h | 4h |
| Phase 2 | 4h | 1h | 5h |
| Phase 3 | 8h | 2h | 10h |

**Assessment: MOSTLY REALISTIC**

### Revised Estimates (with improvements):

| Phase | Original | Revised | Reason |
|-------|----------|---------|--------|
| Phase 0 | 1h | 1.5h | Add API verification step |
| Phase 1 | 3h | 4h | Expanded test suite (4 tests vs 1) + dependency fix |
| Phase 2 | 4h | 4.5h | Refactor to use shared_state + better tests |
| Phase 3 | 8h | 8h | Unchanged (already realistic) |

**Total:**
- **Original:** 9h (Phase 1+2) or 17h (all phases)
- **Revised:** 10h (Phase 1+2) or 18h (all phases)

**Recommendation:** Budget 10-12h for Phase 1+2 to account for unexpected issues.

---

## FINAL VERDICT

### Overall Assessment:

**Implementation Plan Quality: 8/10**

**Strengths:**
- Excellent phasing with clear decision gates
- Realistic effort estimates
- Strong safety mechanisms (validation, rollback)
- Good token measurement approach
- Clear documentation

**Critical Weaknesses:**
1. Phase 1 code changes incomplete (missing dependency declaration)
2. Phase 2 architecture violates DRY (duplicate context extraction)
3. Token reduction expectations are misleading (context-only vs total prompt)

**Recommendation: REVISE**

**Required Actions Before Implementation:**

1. üî¥ **Fix Phase 1 dependency declaration** (30 min)
2. üî¥ **Expand Phase 1 test suite** (1 hour)
3. üü° **Refactor Phase 2 to use shared_state** (1 hour)
4. üü° **Recalculate token reduction expectations** (30 min)
5. ‚úÖ **Add detailed failure procedures to gates** (30 min)

**Total revision effort: 3.5 hours**

---

## SCORING BREAKDOWN

### Clarity: 8/10

**Strengths:**
- Phasing is clear and logical
- Code examples are specific
- Decision gates are well-defined
- Timeline is detailed

**Deductions (-2):**
- Phase 1 code change incomplete (lines 81-98 vs 84-98)
- Token reduction calculation unclear (context vs total prompt)

### Completeness: 7/10

**Strengths:**
- Covers all three phases
- Includes validation steps
- Provides rollback procedures
- Has timeline and resource planning

**Deductions (-3):**
- Missing dependency declaration (critical!)
- Missing edge case handling (empty shared_state)
- Test suite too minimal for Phase 1
- No failure procedures for decision gates

### Safety: 9/10

**Strengths:**
- Validation gates at every phase
- Baseline comparison before changes
- Rollback procedures defined
- Quality checks built-in

**Deductions (-1):**
- Gate failure procedures too vague

### Effort Accuracy: 8/10

**Strengths:**
- Time estimates are realistic
- Effort broken down by task
- Buffer time included
- Checkpoint warnings provided

**Deductions (-2):**
- Underestimates Phase 1 testing (1h ‚Üí 1.5h)
- Doesn't account for dependency fix (+30 min)

---

## NEXT STEPS

### Immediate Actions (DO FIRST):

1. **Update implementation plan document** (address critical issues 1-3)
2. **Add missing code to Phase 1 section** (dependency declaration)
3. **Revise Phase 2 architecture** (use shared_state pattern)
4. **Recalculate and document token expectations** (realistic numbers)

### Then:

5. **Get user approval on revised plan**
6. **Execute Phase 0** (baseline generation)
7. **Proceed with Phase 1** (with improved tests)

### Decision Point After Phase 2:

- If 5-8% total prompt reduction + cleaner architecture ‚Üí **STOP** (success!)
- If architectural improvement is explicit goal ‚Üí Proceed to Phase 3
- If token reduction is primary goal ‚Üí **STOP** (Phase 3 has diminishing returns)

---

## APPENDICES

### Appendix A: Complete Phase 1 Code Change

**File:** `src/services/prompts/modules/definition_task_module.py`

**Lines 81-98 (COMPLETE CHANGE):**

```python
# BEFORE (DELETE):
word_type = context.get_shared("word_type", "onbekend")
ontological_category = context.get_shared("ontological_category")
org_contexts = context.get_shared("organization_contexts", [])
# Derive juridical and legal-basis contexts from enriched base_context
try:
    base_ctx = (
        context.enriched_context.base_context
        if context and context.enriched_context
        else {}
    )
except Exception:
    base_ctx = {}
jur_contexts = (
    base_ctx.get("juridische_context") or base_ctx.get("juridisch") or []
)
wet_basis = (
    base_ctx.get("wettelijke_basis") or base_ctx.get("wettelijk") or []
)

# AFTER (REPLACE WITH):
word_type = context.get_shared("word_type", "onbekend")
ontological_category = context.get_shared("ontological_category")
# Use shared_state for all context types (DEF-126 Phase 1)
org_contexts = context.get_shared("organization_contexts", [])
jur_contexts = context.get_shared("juridical_contexts", [])
wet_basis = context.get_shared("legal_basis_contexts", [])
```

**Lines 163-170 (ADD DEPENDENCY):**

```python
# BEFORE:
def get_dependencies(self) -> list[str]:
    """
    Deze module is afhankelijk van SemanticCategorisationModule.

    Returns:
        Lijst met dependency
    """
    return ["semantic_categorisation"]

# AFTER:
def get_dependencies(self) -> list[str]:
    """
    Deze module is afhankelijk van SemanticCategorisationModule
    en ContextAwarenessModule (voor shared_state context data).

    Returns:
        Lijst met dependencies
    """
    return ["semantic_categorisation", "context_awareness"]
```

---

### Appendix B: Realistic Token Reduction Expectations

**Assumptions:**
- Average full prompt: 2500 tokens
- Context section (current): 200 tokens (8% of prompt)
- Other sections: 2300 tokens (unchanged)

**Phase 1 Impact:**
- Expected change: ¬±0 tokens (refactor only, no content change)
- % reduction: 0%

**Phase 2 Impact:**
- Context section: 200 ‚Üí 80 tokens (120 tokens saved, 60% reduction of context)
- Total prompt: 2500 ‚Üí 2380 tokens (120 tokens saved, **4.8% reduction of total**)

**Phase 3 Impact (if executed):**
- Context section: 80 ‚Üí 60 tokens (20 more tokens saved)
- Total prompt: 2380 ‚Üí 2360 tokens (20 tokens saved, **0.8% additional reduction**)

**Cumulative Impact (All Phases):**
- Total tokens: 2500 ‚Üí 2360 tokens
- Total reduction: 140 tokens
- % reduction: **5.6% of full prompt**

**Revised Decision Gate 2 Thresholds:**

```markdown
If total prompt reduction ‚â•4% (100+ tokens):
   ‚úÖ SUFFICIENT - Phase 1+2 achieved meaningful impact
   Decision: STOP HERE (10 hours total)

If total prompt reduction 2-4% (50-100 tokens):
   üü° MODERATE - Some improvement, more possible
   Decision: User choice (STOP or proceed to Phase 3)

If total prompt reduction <2% (<50 tokens):
   ‚ùå INSUFFICIENT - Expected benefit not achieved
   Decision: Investigate issues OR proceed to Phase 3
```

---

**END OF REVIEW**

**Document Status:** ‚úÖ COMPLETE
**Recommendation:** REVISE (3.5 hours) then PROCEED
**Confidence:** HIGH (80%)
**Review Date:** 2025-11-13
