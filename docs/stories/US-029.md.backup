---
id: US-029
epic: EPIC-007-performance-scaling
title: vermindert OpenAI met minimaal 30% Token Gebruik met 60% door Prompt Optimalisatie
status: todo
priority: high
story_points: 8
sprint: current
dependencies: [US-030]
created: 2025-01-29
updated: 2025-09-05
assigned_to: development-team
requirements:
  - REQ-061  # Performance Optimalisatie Vereisten
  - REQ-065  # Token Gebruik Monitoring
  - REQ-073  # Kosten Beheer Controls
---

# US-029: vermindert OpenAI met minimaal 30% Token Gebruik met 60% door Prompt Optimalisatie

## Gebruikersverhaal
**Als** product owner
**Wil ik** het OpenAI API token gebruik verminderen van 7.250 naar minder dan 3.000 tokens per definitie aanvraag
**Zodat** we 60% kostenbesparing realiseren met behoud van definitiekwaliteit en compliance

## Probleemstelling

**Huidige Situatie:**
- Elke definitie aanvraag gebruikt ~7.250 tokens (gemeten in productie)
- Context wordt 3x gedupliceerd in de prompt keten
- Alle 45 validatieregels worden meegenomen terwijl slechts 5-10 relevant zijn
- MaEnelijkse API kosten overschrijden budget met 40%
- Geen prompt caching mechanisme aanwezig

**Gewenste Uitkomst:**
- vermindert token met minimaal 30% gebruik naar < 3.000 per aanvraag
- Implementeer slimme regel selectie (alleen relevante regels)
- Cache en hergebruik prompt templates
- Behoud huidige definitie kwaliteitsscore (>95%)

## Acceptatiecriteria

### Criterium 1: Token Reductie Doel
**Gegeven** een definitie aanvraag voor een juridische term
**Wanneer** de geoptimaliseerde prompt wordt gegenereerd
**Dan** is het totale token aantal onder 3.000 (geverifieerd via tiktoken)

### Criterium 2: Slimme Regel Selectie
**Gegeven** een definitie aanvraag met context "strafrecht"
**Wanneer** validatieregels worden toegevoegd aan de prompt
**Dan** worden alleen strafrechtelijk relevante regels meegenomen (5-8 regels, niet alle 45)

### Criterium 3: Prompt Template Caching
**Gegeven** een prompt template is gegenereerd voor context type X
**Wanneer** een Enere aanvraag met hetzelfde context type binnen 15 minuten binnenkomt
**Dan** wordt het gecachte template hergebruikt (cache hit rate > 70%)

### Criterium 4: Kwaliteitsbehoud
**Gegeven** de geoptimaliseerde prompts zijn in gebruik
**Wanneer** definities worden gegenereerd
**Dan** blijft het validatie slagingspercentage boven 95%

### Criterium 5: Prestatie Impact
**Gegeven** de optimalisatie is geïmplementeerd
**Wanneer** responstijd wordt gemeten
**Dan** blijft de totale generatietijd onder 5 seconden

## Technische Implementatie

### Implementatie Aanpak
1. **Stap 1**: Implementeer token telling in `PromptServiceV2.build_prompt()`
   - Voeg tiktoken bibliotheek toe voor accurate telling
   - Log token aantallen voor analyse

2. **Stap 2**: Creëer context-bewuste regel selectie in `ModularValidationService`
   - Bouw regel relevantie matrix (context → regels mapping)
   - Implementeer `get_relevant_rules(context_type)` methode

3. **Stap 3**: Implementeer prompt template caching
   - Gebruik Redis of in-memory cache met 15-minuten TTL
   - Cache sleutel: hash van (context_type, domein, taal)

4. **Stap 4**: Verwijder prompt duplicaties
   - Audit prompt constructie keten
   - Elimineer redundante context toevoegingen

### Code Locaties
- Primaire bestEnen:
  - `src/services/prompt_service_v2.py`
  - `src/services/validatie/modular_validation_service.py`
  - `src/services/ai_service_v2.py`
- Kern functies:
  - `PromptServiceV2.build_prompt()`
  - `PromptServiceV2._select_relevant_rules()`
  - `ModularValidationService.get_validation_rules()`
- Configuratie bestEnen:
  - `config/prompt_optimization.yaml` (nieuwe config)
  - `config/validation_rules_mapping.json` (nieuwe mapping)

### Technische Beslissingen
- Gebruik tiktoken voor accurate GPT-4 token telling
- Implementeer LRU cache voor prompt templates (max 100 entries)
- Creëer regel relevantie scoring algoritme
- Gebruik dependency injection voor cache provider

## Domein & compliance

### Domein Regels
- ASTRA vereiste: Behoud volledige auditspoor van regel selectie
- NORA richtlijn: Resource optimalisatie zonder kwaliteitsverlies
- Justitieketen: Definities moeten juridisch valide blijven voor OM, DJI, Rechtspraak

### Beveiliging & Privacy
- Beveiliging: Geen gevoelige data in gecachte prompts
- Privacy: Zorg dat PII nooit wordt gecacht
- Audit: Log welke regels zijn uitgesloten en waarom



## Afhankelijkheden

- EPIC-007
## Test Scenario's

### Unit Tests
1. **Test**: `test_token_telling_accuratesse()`
   - Input: Bekende prompt met 1.000 tokens
   - Verwacht: Telling komt overeen met tiktoken output ±5%
   - Assert: `abs(geteld - 1000) < 50`

2. **Test**: `test_relevante_regel_selectie()`
   - Input: Context "strafrecht"
   - Verwacht: Retourneert regels ["STR001", "STR002", "VER003"]
   - Assert: Geen "BES*" (bestuursrecht) regels meegenomen

3. **Test**: `test_prompt_cache_hit()`
   - Setup: Genereer prompt voor context A, dan zelfde context opnieuw
   - Verwacht: Tweede aanroep retourneert gecacht resultaat
   - Assert: Uitvoeringstijd < 10ms voor gecachte aanroep

### Integratie Tests
1. **Test**: `test_end_to_end_token_reductie()`
   - Setup: Genereer definitie met oud en nieuw systeem
   - Meet: Token aantallen voor beide
   - Assert: Nieuw aantal < (oud aantal * 0.4)

2. **Test**: `test_kwaliteitsbehoud()`
   - Setup: Genereer 100 definities met geoptimaliseerde prompts
   - Meet: Validatie slagingspercentage
   - Assert: Slagingspercentage >= 95%

### Prestatie Tests
1. **Test**: `test_cache_prestatie_onder_belasting()`
   - Setup: 1000 gelijktijdige verzoeken, 30% zelfde context
   - Meet: Cache hit rate, responstijden
   - Assert: Hit rate > 70%, p95 respons < 100ms

## Definitie van Gereed
- [ ] Token telling geïmplementeerd met tiktoken
- [ ] Regel relevantie matrix gecreëerd en getest
- [ ] Prompt template caching operationeel
- [ ] Duplicaties verwijderd uit prompt keten
- [ ] Unit tests geschreven (dekking > 90%)
- [ ] Integratie tests slagen
- [ ] Prestatie benchmarks gehaald (< 3.000 tokens)
- [ ] Security review voltooid (geen PII in cache)
- [ ] Documentatie bijgewerkt met optimalisatie details
- [ ] Code review goedgekeurd door senior developer
- [ ] A/B test toont 60% token reductie
- [ ] Uitgerold naar test omgeving
- [ ] Geen degradatie in definitie kwaliteitsmetrieken

## Risico's & Mitigatie

1. **Risico**: Uitsluiten van regels veroorzaakt validatie fouten
   - Kans: Gemiddeld
   - Impact: Hoog
   - Mitigatie: Implementeer gefaseerde uitrol met monitoring

2. **Risico**: Cache invalidatie problemen
   - Kans: Laag
   - Impact: Gemiddeld
   - Mitigatie: Conservatieve TTL (15 min), hEnmatige cache clear optie

3. **Risico**: Verschillende OpenAI model versies tellen tokens Eners
   - Kans: Laag
   - Impact: Laag
   - Mitigatie: Gebruik model-specifieke tiktoken encoding

## Notities & Referenties
- Gerelateerde issues: #342 (Hoge API kosten), #298 (Trage responstijden)
- Spike resultaten: Token analyse toont 65% verspilling in huidige prompts
- OpenAI prijzen: €0.03/1K tokens (GPT-4)
- Verwachte besparing: €500/maEn bij huidige volume
