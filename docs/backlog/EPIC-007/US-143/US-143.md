---
id: US-143
epic: EPIC-007
titel: "US-143: Prompt Token Optimalisatie - 60% reductie (7250→3000 tokens) via slimme regel selectie en template caching"
status: open
prioriteit: high
story_points: 8
sprint: current
aangemaakt: 29-01-2025
bijgewerkt: 05-09-2025
owner: development-team
applies_to: definitie-app@current
canonical: false
last_verified: 2025-10-02
vereisten:
  - REQ-061
  - REQ-065
  - REQ-073
afhankelijkheden:
  - US-030
toegewezen_aan: development-team
---



# US-143: vermindert OpenAI met minimaal 30% Token Gebruik met 60% door Prompt Optimalisatie

## Gebruikersverhaal
**Als** product owner
**wil ik** het OpenAI API token gebruik verminderen van 7.250 naar minder dan 3.000 tokens per definitie aanvraag
**zodat** we 60% kostenbesparing realiseren met behoud van definitiekwaliteit en compliance

## Probleemstelling

**Huidige Situatie:**
- Elke definitie aanvraag gebruikt ~7.250 tokens (gemeten in productie)
- Context wordt 3x gedupliceerd in de prompt keten
- Alle 45 validatieregels worden meegenomen terwijl slechts 5-10 relevant zijn
- MaEnelijkse API kosten overschrijden budget met 40%
- Geen prompt caching mechanisme aanwezig

**Gewenste Uitkomst:**
- vermindert token met minimaal 30% gebruik naar < 3.000 per aanvraag
- Implementeer slimme regel selectie (alleen relevante regels)
- Cache en hergebruik prompt templates
- Behoud huidige definitie kwaliteitsscore (>95%)

## Acceptatiecriteria


### SMART Acceptatiecriteria

- **Specifiek:** [Exact gedrag dat moet worden gerealiseerd]
- **Meetbaar:**
  - Response tijd: < 200ms voor UI acties
  - Processing tijd: < 5 seconden voor generatie
  - Success rate: > 95% voor validaties
- **Acceptabel:** Haalbaar binnen huidige architectuur
- **Relevant:** Direct gerelateerd aan gebruikersbehoefte
- **Tijdgebonden:** Gerealiseerd binnen huidige sprint


### Criterium 1: Token Reductie Doel
**gegeven** een definitie aanvraag voor een juridische term
**wanneer** de geoptimaliseerde prompt wordt gegenereerd
**dan** is het totale token aantal onder 3.000 (geverifieerd via tiktoken)

### Criterium 2: Slimme Regel Selectie
**gegeven** een definitie aanvraag met context "strafrecht"
**wanneer** validatieregels worden toegevoegd aan de prompt
**dan** worden alleen strafrechtelijk relevante regels meegenomen (5-8 regels, niet alle 45)

### Criterium 3: Prompt Template Caching
**gegeven** een prompt template is gegenereerd voor context type X
**wanneer** een Enere aanvraag met hetzelfde context type binnen 15 minuten binnenkomt
**dan** wordt het gecachte template hergebruikt (cache hit rate > 70%)

### Criterium 4: Kwaliteitsbehoud
**gegeven** de geoptimaliseerde prompts zijn in gebruik
**wanneer** definities worden gegenereerd
**dan** blijft het validatie slagingspercentage boven 95%

### Criterium 5: Prestatie Impact
**gegeven** de optimalisatie is geïmplementeerd
**wanneer** responstijd wordt gemeten
**dan** blijft de totale generatietijd onder 5 seconden

## Technische Implementatie

### Implementatie Aanpak
1. **Stap 1**: Implementeer token telling in `PromptServiceV2.build_prompt()`
   - Voeg tiktoken bibliotheek toe voor accurate telling
   - Log token aantallen voor analyse

2. **Stap 2**: Creëer context-bewuste regel selectie in `ModularValidationService`
   - Bouw regel relevantie matrix (context → regels mapping)
   - Implementeer `get_relevant_rules(context_type)` methode

3. **Stap 3**: Implementeer prompt template caching
   - Gebruik Redis of in-memory cache met 15-minuten TTL
   - Cache sleutel: hash van (context_type, domein, taal)

4. **Stap 4**: Verwijder prompt duplicaties
   - Audit prompt constructie keten
   - Elimineer redundante context toevoegingen

### Code Locaties
- Primaire bestEnen:
  - `src/services/prompt_service_v2.py`
  - `src/services/validatie/modular_validation_service.py`
  - `src/services/ai_service_v2.py`
- Kern functies:
  - `PromptServiceV2.build_prompt()`
  - `PromptServiceV2._select_relevant_rules()`
  - `ModularValidationService.get_validation_rules()`
- Configuratie bestEnen:
  - `config/prompt_optimization.yaml` (nieuwe config)
  - `config/validation_rules_mapping.json` (nieuwe mapping)

### Technische Beslissingen
- Gebruik tiktoken voor accurate GPT-4 token telling
- Implementeer LRU cache voor prompt templates (max 100 entries)
- Creëer regel relevantie scoring algoritme
- Gebruik afhankelijkheid injection voor cache provider

## Domein & compliance

### Domein Regels
- ASTRA vereiste: Behoud volledige auditspoor van regel selectie
- NORA richtlijn: Resource optimalisatie zonder kwaliteitsverlies
- Justitieketen: Definities moeten juridisch valide blijven voor OM, DJI, Rechtspraak

### beveiliging & Privacy
- beveiliging: Geen gevoelige data in gecachte prompts
- Privacy: Zorg dat PII nooit wordt gecacht
- Audit: Log welke regels zijn uitgesloten en waarom



## Afhankelijkheden

- EPIC-007
## Test Scenario's

### Unit Tests
1. **Test**: `test_token_telling_accuratesse()`
   - Input: Bekende prompt met 1.000 tokens
   - Verwacht: Telling komt overeen met tiktoken output ±5%
   - Assert: `abs(geteld - 1000) < 50`

2. **Test**: `test_relevante_regel_selectie()`
   - Input: Context "strafrecht"
   - Verwacht: Retourneert regels ["STR001", "STR002", "VER003"]
   - Assert: Geen "BES*" (bestuursrecht) regels meegenomen

3. **Test**: `test_prompt_cache_hit()`
   - Setup: Genereer prompt voor context A, dan zelfde context opnieuw
   - Verwacht: Tweede aanroep retourneert gecacht resultaat
   - Assert: Uitvoeringstijd < 10ms voor gecachte aanroep

### Integratie Tests
1. **Test**: `test_end_to_end_token_reductie()`
   - Setup: Genereer definitie met oud en nieuw systeem
   - Meet: Token aantallen voor beide
   - Assert: Nieuw aantal < (oud aantal * 0.4)

2. **Test**: `test_kwaliteitsbehoud()`
   - Setup: Genereer 100 definities met geoptimaliseerde prompts
   - Meet: Validatie slagingspercentage
   - Assert: Slagingspercentage >= 95%

### Prestatie Tests
1. **Test**: `test_cache_prestatie_onder_belasting()`
   - Setup: 1000 gelijktijdige verzoeken, 30% zelfde context
   - Meet: Cache hit rate, responstijden
   - Assert: Hit rate > 70%, p95 respons < 100ms

## Definitie van Gereed
- [ ] Token telling geïmplementeerd met tiktoken
- [ ] Regel relevantie matrix gecreëerd en getest
- [ ] Prompt template caching operationeel
- [ ] Duplicaties verwijderd uit prompt keten
- [ ] Unit tests geschreven (dekking > 90%)
- [ ] Integratie tests slagen
- [ ] Prestatie benchmarks gehaald (< 3.000 tokens)
- [ ] Beveiliging review voltooid (geen PII in cache)
- [ ] Documentatie bijgewerkt met optimalisatie details
- [ ] Code review goedgekeurd door senior developer
- [ ] A/B test toont 60% token reductie
- [ ] Uitgerold naar test omgeving
- [ ] Geen degradatie in definitie kwaliteitsmetrieken

## Risico's & Mitigatie

1. **Risico**: Uitsluiten van regels veroorzaakt validatie fouten
   - Kans: Gemiddeld
   - Impact: Hoog
   - Mitigatie: Implementeer gefaseerde uitrol met monitoring

2. **Risico**: Cache invalidatie problemen
   - Kans: Laag
   - Impact: Gemiddeld
   - Mitigatie: Conservatieve TTL (15 min), hEnmatige cache clear optie

3. **Risico**: Verschillende OpenAI model versies tellen tokens Eners
   - Kans: Laag
   - Impact: Laag
   - Mitigatie: Gebruik model-specifieke tiktoken encoding

## Notities & Referenties
- Gerelateerde issues: #342 (Hoge API kosten), #298 (Trage responstijden)
- Spike resultaten: Token analyse toont 65% verspilling in huidige prompts
- OpenAI prijzen: €0.03/1K tokens (GPT-4)
- Verwachte besparing: €500/maEn bij huidige volume
