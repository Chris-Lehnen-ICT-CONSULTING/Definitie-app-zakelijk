---
id: US-143
epic: EPIC-030
titel: "US-143: Prompt Token Optimalisatie - 60% reductie (7250→3000 tokens) via slimme regel selectie en template caching"
status: open
prioriteit: high
story_points: 8
sprint: current
aangemaakt: 29-01-2025
bijgewerkt: 05-09-2025
owner: development-team
applies_to: definitie-app@current
canonical: false
last_verified: 2025-10-02
vereisten:
  - REQ-061
  - REQ-065
  - REQ-073
afhankelijkheden:
  - US-030
toegewezen_aan: development-team
---



# US-143: vermindert OpenAI met minimaal 30% Token Gebruik met 60% door Prompt Optimalisatie

## Gebruikersverhaal
**Als** product owner
**wil ik** het OpenAI API token gebruik verminderen van 7.250 naar minder dan 3.000 tokens per definitie aanvraag
**zodat** we 60% kostenbesparing realiseren met behoud van definitiekwaliteit en compliance

## Probleemstelling

**Huidige Situatie:**
- Elke definitie aanvraag gebruikt ~7.250 tokens (gemeten in productie)
- Context wordt 3x gedupliceerd in de prompt keten
- Alle 45 validatieregels worden meegenomen terwijl slechts 5-10 relevant zijn
- MaEnelijkse API kosten overschrijden budget met 40%
- Geen prompt caching mechanisme aanwezig

**Gewenste Uitkomst:**
- vermindert token met minimaal 30% gebruik naar < 3.000 per aanvraag
- Implementeer slimme regel selectie (alleen relevante regels)
- Cache en hergebruik prompt templates
- Behoud huidige definitie kwaliteitsscore (>95%)

## Acceptatiecriteria

### SMART Acceptatiecriteria

- **Specifiek:** [Exact gedrag dat moet worden gerealiseerd]
- **Meetbaar:**
  - Response tijd: < 200ms voor UI acties
  - Processing tijd: < 5 seconden voor generatie
  - Success rate: > 95% voor validaties
- **Acceptabel:** Haalbaar binnen huidige architectuur
- **Relevant:** Direct gerelateerd aan gebruikersbehoefte
- **Tijdgebonden:** Gerealiseerd binnen huidige sprint


### Criterium 1: Token Reductie Doel
**gegeven** een definitie aanvraag voor een juridische term
**wanneer** de geoptimaliseerde prompt wordt gegenereerd
**dan** is het totale token aantal onder 3.000 (geverifieerd via tiktoken)

### Criterium 2: Slimme Regel Selectie
**gegeven** een definitie aanvraag met context "strafrecht"
**wanneer** validatieregels worden toegevoegd aan de prompt
**dan** worden alleen strafrechtelijk relevante regels meegenomen (5-8 regels, niet alle 45)

### Criterium 3: Prompt Template Caching
**gegeven** een prompt template is gegenereerd voor context type X
**wanneer** een Enere aanvraag met hetzelfde context type binnen 15 minuten binnenkomt
**dan** wordt het gecachte template hergebruikt (cache hit rate > 70%)

### Criterium 4: Kwaliteitsbehoud
**gegeven** de geoptimaliseerde prompts zijn in gebruik
**wanneer** definities worden gegenereerd
**dan** blijft het validatie slagingspercentage boven 95%

### Criterium 5: Prestatie Impact
**gegeven** de optimalisatie is geïmplementeerd
**wanneer** responstijd wordt gemeten
**dan** blijft de totale generatietijd onder 5 seconden

## Refinement (2025-10-14)

### Scope
- Reduceer prompt tokens door duplicaties te verwijderen en relevante secties compact te maken.
- Introduceer contextgestuurde regelselectie voor validatie-instructies.
- Voeg lichte caching toe (in-memory) om herhaalde prompts binnen dezelfde sessie sneller te leveren.
- Instrumenteer promptlengte- en cachemetriek voor regressiecontrole.

### Buiten scope
- Geen externe cache (Redis); lokale LRU volstaat.
- Geen herbouw van volledige prompt pipeline; alleen optimalisaties in bestaande V2 componenten.
- Geen UI-wijzigingen behalve het tonen van nieuwe statistieken indien al aanwezig.

### Deliverables
1. Functie `PromptServiceV2.measure_tokens()` met tiktoken fallback en logging.
2. Configuratiebestand `config/prompt_rulescope.yaml` met mapping context → regels.
3. In-memory LRU cache (max 64 entries, TTL 15 minuten) voor prompt templates.
4. Telemetry hooks: totaal tokens, besparing t.o.v. baseline, cache-hit ratio.
5. Technische notitie in `docs/implementation/prompt-optimization.md`.

### Werkpakketten
- [ ] Meet & log tokens
  - Voeg helper toe en schrijf test `test_prompt_token_measurement`.
  - Voeg CLI script `scripts/metrics/prompt_baseline.py` om huidig gemiddelde te bepalen (input sample 20 definities).
- [ ] Regelselectie
  - Introduceer mapping-bestand en loader.
  - Pas `ModularValidationService` aan om geselecteerde regels te filteren; fallback naar volledige set wanneer geen mapping.
  - Voeg unit test voor contexten strafrecht, bestuursrecht, onbekend.
- [ ] Prompt deduplicatie
  - Herstructureer `PromptServiceV2.build_prompt` zodat contextblokken exact één keer voorkomen.
  - Voeg snapshot test die oude/nieuwe prompt vergelijkt en ensures unieke secties.
- [ ] Caching
  - Bouw LRU helper in `src/utils/cache.py` (nieuw) en injecteer via service container.
  - Log cache hit/miss via `logging.getLogger("prompt.optimization")`.
  - Schrijf integratietest met geforceerde cache-invalidation.
- [ ] Monitoring
  - Voeg counters toe aan bestaande metrics (bv. `metrics/prompt_summary.json`).
  - Documenteer nieuwe metrics en gebruik in README.

### Definition of Ready
- [ ] Voorbeeld dataset met 20 bestaande definities is beschikbaar (`data/samples/prompts/*.json`).
- [ ] Relevante contextlabels afgestemd met domeinexpert (minimaal strafrecht, bestuursrecht, civiel).
- [ ] Besluit: LRU cache in-memory (geen extra dependencies).
- [ ] Afhankelijkheid `tiktoken` staat in `requirements.txt`.

## Technische Implementatie

### Implementatie Aanpak
1. **Stap 1**: Implementeer token telling in `PromptServiceV2.build_prompt()`
   - Voeg tiktoken bibliotheek toe voor accurate telling
   - Log token aantallen voor analyse

2. **Stap 2**: Creëer context-bewuste regel selectie in `ModularValidationService`
   - Bouw regel relevantie matrix (context → regels mapping)
   - Implementeer `get_relevant_rules(context_type)` methode

3. **Stap 3**: Implementeer prompt template caching
   - Gebruik Redis of in-memory cache met 15-minuten TTL
   - Cache sleutel: hash van (context_type, domein, taal)

4. **Stap 4**: Verwijder prompt duplicaties
   - Audit prompt constructie keten
   - Elimineer redundante context toevoegingen

### Code Locaties
- Primaire bestEnen:
  - `src/services/prompt_service_v2.py`
  - `src/services/validatie/modular_validation_service.py`
  - `src/services/ai_service_v2.py`
- Kern functies:
  - `PromptServiceV2.build_prompt()`
  - `PromptServiceV2._select_relevant_rules()`
  - `ModularValidationService.get_validation_rules()`
- Configuratie bestEnen:
  - `config/prompt_optimization.yaml` (nieuwe config)
  - `config/validation_rules_mapping.json` (nieuwe mapping)

### Technische Beslissingen
- Gebruik tiktoken voor accurate GPT-4 token telling
- Implementeer LRU cache voor prompt templates (max 100 entries)
- Creëer regel relevantie scoring algoritme
- Gebruik afhankelijkheid injection voor cache provider

## Domein & compliance

### Domein Regels
- ASTRA vereiste: Behoud volledige auditspoor van regel selectie
- NORA richtlijn: Resource optimalisatie zonder kwaliteitsverlies
- Justitieketen: Definities moeten juridisch valide blijven voor OM, DJI, Rechtspraak

### beveiliging & Privacy
- beveiliging: Geen gevoelige data in gecachte prompts
- Privacy: Zorg dat PII nooit wordt gecacht
- Audit: Log welke regels zijn uitgesloten en waarom



## Afhankelijkheden

- EPIC-007
## Test Scenario's

### Unit Tests
1. **Test**: `test_token_telling_accuratesse()`
   - Input: Bekende prompt met 1.000 tokens
   - Verwacht: Telling komt overeen met tiktoken output ±5%
   - Assert: `abs(geteld - 1000) < 50`

2. **Test**: `test_relevante_regel_selectie()`
   - Input: Context "strafrecht"
   - Verwacht: Retourneert regels ["STR001", "STR002", "VER003"]
   - Assert: Geen "BES*" (bestuursrecht) regels meegenomen

3. **Test**: `test_prompt_cache_hit()`
   - Setup: Genereer prompt voor context A, dan zelfde context opnieuw
   - Verwacht: Tweede aanroep retourneert gecacht resultaat
   - Assert: Uitvoeringstijd < 10ms voor gecachte aanroep

### Integratie Tests
1. **Test**: `test_end_to_end_token_reductie()`
   - Setup: Genereer definitie met oud en nieuw systeem
   - Meet: Token aantallen voor beide
   - Assert: Nieuw aantal < (oud aantal * 0.4)

2. **Test**: `test_kwaliteitsbehoud()`
   - Setup: Genereer 100 definities met geoptimaliseerde prompts
   - Meet: Validatie slagingspercentage
   - Assert: Slagingspercentage >= 95%

### Prestatie Tests
1. **Test**: `test_cache_prestatie_onder_belasting()`
   - Setup: 1000 gelijktijdige verzoeken, 30% zelfde context
   - Meet: Cache hit rate, responstijden
   - Assert: Hit rate > 70%, p95 respons < 100ms

## Definitie van Gereed
- [ ] Tokenmeting + logging actief en terug te vinden in auditlog.
- [ ] Regelselectie geconfigureerd en getest voor minimaal 3 contexttypen.
- [ ] Prompt bevat elke contextsectie maximaal één keer (geautomatiseerde test).
- [ ] Cache levert ≥70% hit-rate tijdens testrun van 20 requests (vastgelegd).
- [ ] Testsuite bevat unit + integratie tests voor bovenstaande.
- [ ] Documentatie + changelog bijgewerkt met implementatiestappen en metingen.
- [ ] Validatie slagingspercentage opnieuw gemeten (≥95%) na optimalisatie.

## Risico's & Mitigatie

1. **Risico**: Uitsluiten van regels veroorzaakt validatie fouten
   - Kans: Gemiddeld
   - Impact: Hoog
   - Mitigatie: Implementeer gefaseerde uitrol met monitoring

2. **Risico**: Cache invalidatie problemen
   - Kans: Laag
   - Impact: Gemiddeld
   - Mitigatie: Conservatieve TTL (15 min), hEnmatige cache clear optie

3. **Risico**: Verschillende OpenAI model versies tellen tokens Eners
   - Kans: Laag
   - Impact: Laag
   - Mitigatie: Gebruik model-specifieke tiktoken encoding

## Notities & Referenties
- Gerelateerde issues: #342 (Hoge API kosten), #298 (Trage responstijden)
- Spike resultaten: Token analyse toont 65% verspilling in huidige prompts
- OpenAI prijzen: €0.03/1K tokens (GPT-4)
- Verwachte besparing: €500/maEn bij huidige volume
