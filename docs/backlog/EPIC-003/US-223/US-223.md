---
id: US-223
epic: EPIC-003
titel: Verwijder enable_caching Feature Flag
type: technical-debt
status: open
prioriteit: HOOG
story_points: 5
sprint: backlog
aangemaakt: 19-09-2025
bijgewerkt: 19-09-2025
owner: development-team
applies_to: definitie-app@current
canonical: false
last_verified: 19-09-2025
vereisten:
  - REQ-021
  - REQ-024
Afhankelijkheden: []
toegewezen_aan: development-team
---

# US-223: Verwijder enable_caching Feature Flag

## Gebruikersverhaal

**Als een** ontwikkelaar
**wil ik** dat caching altijd actief is zonder feature flag
**zodat** de applicatie consistent goede performance heeft

## Context

De `enable_caching` flag kan caching volledig uitschakelen wat leidt tot:
- 3-5x langzamere response tijden
- Onnodige API calls naar OpenAI
- Verhoogde kosten
- Slechte user experience

Voor een single-user app is er geen reden om caching uit te kunnen zetten.

## Acceptatiecriteria

### Criterion 1: Caching Altijd Actief
**gegeven** de applicatie draait
**wanneer** data wordt opgevraagd
**dan** wordt caching ALTIJD gebruikt

### Criterion 2: Cache Strategy Configuratie
**gegeven** caching is altijd aan
**wanneer** configuratie nodig is
**dan** kan cache TTL en strategy geconfigureerd worden

### Criterion 3: Performance Garantie
**gegeven** caching is actief
**wanneer** gecachte data bestaat
**dan** is response tijd <100ms

## Technische Implementatie

### Te Wijzigen Bestanden

1. **src/utils/cache.py**
   - Regel 37: Verwijder `enable_cache: bool = True` parameter
   - Regel 103, 124: Verwijder `if not self.config.enable_cache:` checks
   - Regel 322: Verwijder uit decorator parameters

2. **src/services/interfaces.py**
   - Regel 1165: Verwijder `enable_caching: bool = True`

3. **Config updates**
   - Verwijder alle `enable_caching` configuraties
   - Behoud alleen cache strategy settings (TTL, size limits)

### Nieuwe Implementatie

```python
# VOOR (performance risico):
if not self.config.enable_cache:
    return await fetch_fresh_data()  # ALTIJD verse data!

# NA (altijd caching):
class CacheService:
    def __init__(self, ttl: int = 900, max_size: int = 1000):
        self.ttl = ttl  # Configureerbaar
        self.max_size = max_size  # Configureerbaar
        self.cache = {}  # ALTIJD actief

    async def get_or_fetch(self, key: str, fetcher: Callable):
        # Check cache ALTIJD
        if key in self.cache:
            entry = self.cache[key]
            if not self._is_expired(entry):
                return entry.value

        # Fetch en cache
        value = await fetcher()
        self.cache[key] = CacheEntry(value, time.time())
        return value
```

### Cache Strategy Configuratie

```yaml
# config/cache_config.yaml
cache:
  default_ttl: 900  # 15 minuten
  strategies:
    definitions:
      ttl: 3600  # 1 uur voor definities
    web_lookup:
      ttl: 1800  # 30 minuten voor web content
    validation:
      ttl: 300   # 5 minuten voor validaties
  limits:
    max_entries: 1000
    max_size_mb: 100
```

## Test Coverage Vereisten

### Performance Tests
- Test cache hit performance (<100ms)
- Test cache miss performance
- Test TTL expiration

### Functional Tests
- Test dat caching niet uitgeschakeld kan worden
- Test verschillende cache strategies
- Test cache size limits

## Definitie van Gereed

- [ ] `enable_caching` flag volledig verwijderd
- [ ] Caching altijd actief
- [ ] Cache strategy configuratie geÃ¯mplementeerd
- [ ] Performance metrics: >80% cache hit rate
- [ ] Response tijd <100ms voor cache hits
- [ ] Monitoring voor cache effectiveness
- [ ] Tests zonder flag dependencies

## Impact Analyse

### Positief
- **Performance**: Consistente snelle response tijden
- **Kosten**: Minder API calls = lagere kosten
- **UX**: Betere gebruikerservaring
- **Simpliciteit**: Geen cache bypass logic

### Risico's
- **Stale Data**: Oude data mogelijk (mitigatie: goede TTL settings)
- **Memory**: Cache groeit (mitigatie: size limits)
- **Debugging**: Moeilijker met cache (mitigatie: cache clear tools)

## Dependencies

- Memory monitoring voor cache size
- Cache invalidation strategy
- Performance monitoring tools

## Monitoring Requirements

```python
# Add cache metrics
metrics.gauge('cache.size', len(self.cache))
metrics.gauge('cache.hit_rate', self.hit_rate)
metrics.timer('cache.lookup_time', lookup_duration)
```

---

*Deze story is kritiek voor performance*