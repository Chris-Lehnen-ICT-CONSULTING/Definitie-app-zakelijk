---
id: US-212
epic: EPIC-026
titel: "US-212: titel: Import Definitions from CSV/JSON"
type: feature
status: proposed
prioriteit: MEDIUM
story_points: 8
sprint: week-4
aangemaakt: 2025-09-18
bijgewerkt: 2025-09-18
owner: developer
applies_to: definitie-app@phoenix
canonical: false
last_verified: 2025-09-18
vereisten:
  - REQ-020-06
  - REQ-083
toegewezen_aan: development-team
related_stories:
  - US-062
---

# US-212: Import Definitions from CSV/JSON

**Epic:** EPIC-020-PHOENIX - Complete Technical Rebuild (Phoenix)
**Week:** 4 - External Integration
**Related:** US-062 (EPIC-005) - Implement Bulk Import Functionality

## Gebruikersverhaal

**Als een** content manager of data migrator
**wil ik** definities kunnen importeren uit CSV en JSON bestanden
**zodat** ik bestaande definitiesets kan migreren en bulk updates kan uitvoeren

## Probleembeschrijving

Deze story bouwt voort op US-062 uit EPIC-005 die de basis bulk import functionaliteit definieert. Voor de Phoenix rebuild moet deze functionaliteit volledig geÃ¯mplementeerd worden met robuuste validatie, conflict resolution, en transactionele integriteit.

## Acceptatiecriteria

### Functionele Criteria (uitbreiding van US-062)
- [ ] CSV import met configureerbare column mapping
- [ ] JSON import met schema validatie
- [ ] JSON Lines (.jsonl) support voor streaming
- [ ] Excel (.xlsx) import ondersteuning
- [ ] Validatie rapport voor import
- [ ] Preview met eerste 10-20 records
- [ ] Conflict resolution strategies (skip/overwrite/merge/version)
- [ ] Dry-run mode voor validatie zonder import
- [ ] Progress tracking met cancel optie
- [ ] Import history en rollback capability
- [ ] Batch size configuratie voor grote imports

### Technische Criteria
- [ ] Streaming parser voor grote bestanden (>100MB)
- [ ] Transactionele import (all-or-nothing)
- [ ] Incremental import met checkpoint/resume
- [ ] Memory-efficient processing (<500MB RAM voor 1GB file)
- [ ] Async processing met background jobs
- [ ] Import queue management
- [ ] Automatic format detection
- [ ] Character encoding detection en conversion

### Validatie Criteria
- [ ] Schema validatie tegen definition model
- [ ] Verplichte velden controle
- [ ] Data type validatie
- [ ] Business rule validatie (via validation service)
- [ ] Duplicate detection (configurable strategies)
- [ ] Referential integrity checks
- [ ] Format consistency checks

## Implementatie Details

### Import Service Architecture
```python
class DefinitionImportService:
    """Service for importing definitions from various formats"""

    async def import_from_file(
        self,
        file: UploadedFile,
        options: ImportOptions
    ) -> ImportResult:
        # 1. Detect format
        # 2. Validate schema
        # 3. Preview if requested
        # 4. Process in batches
        # 5. Apply conflict strategy
        # 6. Execute import
        # 7. Generate report
        pass

    async def validate_import_data(
        self,
        data: List[Dict],
        schema: ImportSchema
    ) -> ValidationResult:
        """Validate data against schema and business rules"""
        pass

    async def preview_import(
        self,
        file: UploadedFile,
        limit: int = 20
    ) -> PreviewResult:
        """Generate preview of import data"""
        pass

class ImportOptions:
    conflict_strategy: ConflictStrategy
    batch_size: int = 100
    dry_run: bool = False
    validate_only: bool = False
    column_mapping: Dict[str, str] = None
    skip_validation: List[str] = []
    transaction_mode: bool = True
```

### Format Parsers
```python
class CSVImporter:
    """CSV specific import logic"""

    def detect_delimiter(self, sample: str) -> str
    def detect_encoding(self, file_bytes: bytes) -> str
    def parse_with_mapping(
        self,
        file: IO,
        mapping: Dict[str, str]
    ) -> Iterator[Dict]

class JSONImporter:
    """JSON/JSONL specific import logic"""

    def validate_schema(self, data: Dict, schema: Dict) -> bool
    def stream_jsonl(self, file: IO) -> Iterator[Dict]
    def normalize_structure(self, data: Dict) -> Dict

class ExcelImporter:
    """Excel specific import logic"""

    def detect_sheet(self, file: IO) -> str
    def parse_with_headers(self, file: IO) -> Iterator[Dict]
    def handle_formulas(self, cell_value: Any) -> Any
```

### Conflict Resolution Strategies
```python
class ConflictStrategy(Enum):
    SKIP = "skip"           # Skip conflicting records
    OVERWRITE = "overwrite" # Replace existing records
    MERGE = "merge"         # Merge with existing data
    VERSION = "version"     # Create new version
    FAIL = "fail"          # Fail on first conflict

class ConflictResolver:
    def resolve(
        self,
        existing: Definition,
        new: Definition,
        strategy: ConflictStrategy
    ) -> Resolution:
        """Apply conflict resolution strategy"""
        pass
```

### Import Schema Definition
```yaml
# import_schema.yaml
definition_import:
  required_fields:
    - term
    - definition_text
  optional_fields:
    - context
    - examples
    - category
    - status
    - metadata
  field_mappings:
    csv:
      term: ["term", "begrip", "woord"]
      definition_text: ["definitie", "definition", "omschrijving"]
    json:
      term: "$.term"
      definition_text: "$.definition"
  validation_rules:
    - field: term
      min_length: 2
      max_length: 200
    - field: definition_text
      min_length: 10
      max_length: 5000
```

### Database Schema for Import Tracking
```sql
-- Import jobs table
CREATE TABLE import_jobs (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    filename TEXT NOT NULL,
    format TEXT NOT NULL,
    status TEXT DEFAULT 'pending',
    total_records INTEGER,
    processed_records INTEGER DEFAULT 0,
    successful_records INTEGER DEFAULT 0,
    failed_records INTEGER DEFAULT 0,
    conflict_strategy TEXT,
    dry_run BOOLEAN DEFAULT FALSE,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    error_log TEXT,
    import_report JSON,
    rollback_data JSON,
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- Import record tracking
CREATE TABLE import_records (
    id TEXT PRIMARY KEY,
    job_id TEXT NOT NULL,
    record_number INTEGER,
    status TEXT,
    original_data JSON,
    processed_data JSON,
    error_message TEXT,
    definition_id TEXT,
    FOREIGN KEY (job_id) REFERENCES import_jobs(id),
    FOREIGN KEY (definition_id) REFERENCES definitions(id)
);
```

## Definition of Done

- [ ] Import service met format detection
- [ ] CSV parser met column mapping
- [ ] JSON/JSONL parser met schema validation
- [ ] Excel parser implementatie
- [ ] Conflict resolution strategies
- [ ] Transaction management
- [ ] Progress tracking UI
- [ ] Preview functionaliteit
- [ ] Import history tracking
- [ ] Rollback mechanism
- [ ] Unit tests voor alle parsers
- [ ] Integration tests voor import flow
- [ ] Performance test (10k records <60s)
- [ ] Error recovery tests
- [ ] Gebruikersdocumentatie
- [ ] Import templates (CSV/JSON)

## UI Mockup Concept

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ðŸ“¥ Import Definities               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  Step 1: Select File               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ [Choose File] import.csv     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â”‚  Step 2: Configure Import          â”‚
â”‚  Format: [CSV v] Delimiter: [, v]  â”‚
â”‚  Conflict: [Skip existing v]       â”‚
â”‚  â–¡ Dry run (preview only)          â”‚
â”‚                                     â”‚
â”‚  Step 3: Column Mapping            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ CSV Column   â”‚ Maps To      â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ begrip       â”‚ [term     v] â”‚   â”‚
â”‚  â”‚ omschrijving â”‚ [definitiev] â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â”‚  Preview (first 5 records):        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ âœ“ Record 1: Valid            â”‚   â”‚
â”‚  â”‚ âœ“ Record 2: Valid            â”‚   â”‚
â”‚  â”‚ âš  Record 3: Duplicate        â”‚   â”‚
â”‚  â”‚ âœ“ Record 4: Valid            â”‚   â”‚
â”‚  â”‚ âœ— Record 5: Missing field    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â”‚  [Validate] [Import] [Cancel]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Risico's

| Risico | Impact | Mitigatie |
|--------|--------|-----------|
| Data corruption | High | Transactions, validation, backups |
| Memory overflow | Medium | Streaming, batch processing |
| Import failures | Medium | Checkpoint/resume, rollback |
| Encoding issues | Low | Auto-detection, UTF-8 default |
| Duplicate data | Medium | Duplicate detection, merge strategies |

## Performance Requirements

- Parse speed: >1000 records/second
- Memory usage: <500MB voor 1GB file
- Transaction size: Max 1000 records per transaction
- UI responsiveness: Updates every 100ms during import
- Rollback time: <30s voor 10k records

## Integration met US-062

Deze story implementeert en uitbreidt de requirements van US-062:
- Alle acceptance criteria van US-062 worden geÃ¯mplementeerd
- Extra format support (JSON Lines, auto-detection)
- Verbeterde conflict resolution strategies
- Checkpoint/resume voor grote imports
- Import history en audit trail

## Testing Strategie

1. **Unit Tests**: Format parsers, validators
2. **Integration Tests**: Complete import flow
3. **Performance Tests**: Large file handling
4. **Stress Tests**: Memory limits, concurrent imports
5. **Data Integrity Tests**: Transaction rollback
6. **Format Tests**: Various CSV/JSON formats

## Notes

- Implementeer CSV eerst als meest gebruikte format
- Overweeg Apache Arrow voor zeer grote datasets
- Add support voor API-based imports later
- Consider scheduled/recurring imports
- Implement export van import templates
- Add data transformation rules voor complexe mappings