✅ Backlogitem: Ondersteuning voor overstap van GPT-4 API naar lokaal AI-model

Prioriteit: Hoog
Type: Architectuur / Toekomstbestendig maken
Status: Open
Sprint: Volgende iteratie

Beschrijving:
Momenteel gebruikt de AI-definitieagent het GPT-4 model via de OpenAI API. Hoewel dit uitstekende kwaliteit biedt, zijn er op termijn redenen om te kunnen overstappen naar een lokaal model, zoals:
- Verlaging van kosten (API-kosten per call)
- Onafhankelijkheid van externe providers
- Verbetering van privacy (volledige datacontrole)
- Offline kunnen werken

Dit backlogitem beoogt een flexibele architectuur waarmee eenvoudig geschakeld kan worden tussen GPT-4 via API en een lokaal AI-model (zoals via Ollama, LM Studio of llamacpp).

Doel:
Een aanpasbare, modulair opgebouwde promptarchitectuur, zodat je:
- Nu effectief werkt met GPT-4 (API)
- Later kunt overstappen op een lokaal model zonder herbouw

Taken:
1. Isolatie van promptfunctionaliteit:
   - Maak één centrale functie zoals `stuur_prompt(prompt: str) -> str`
   - Scheid logica voor externe en lokale modellen

2. Configuratie-ondersteuning:
   - Voeg een `.env` of config-setting toe zoals:
     GEBRUIK_LOKAAL_MODEL = True / False

3. Implementatie GPT-4 API (bestaand):
   - Via `openai.ChatCompletion.create(...)`

4. Voorbereiding lokaal model (placeholder):
   - Stub of voorbeeldimplementatie via:
     - `subprocess.run(...)` (bij gebruik van shell CLI’s zoals Ollama)
     - of `requests.post(...)` bij lokale API-hosting

5. Zorg voor uniforme promptformaten:
   - Vermijd OpenAI-specifieke opties (function_calling, tool_choice)
   - Houd de prompt puur tekstueel en begrijpelijk voor alle modellen

6. Logging en foutafhandeling generiek houden

Testcases:
- Prompt loopt succesvol via GPT-4 API
- Prompt loopt succesvol via lokale mock-functie
- Output is tekstueel vergelijkbaar bij beide modellen
- Config-wissel schakelt gedrag zonder herstart van de app

Betrokken bestanden:
- definitie_agent_webinterface_logging.py: Modularisatie van stuur_prompt()
- .env of config.py: Toevoegen GEBRUIK_LOKAAL_MODEL
- README.md: Uitleg over schakelen tussen modellen

Inschatting:
- Structurering + config       = 0.5 dag
- Lokale stub + fallback       = 0.5 dag
- Testen en documentatie       = 0.5 dag
Totaal                         = ~1,5 dag
