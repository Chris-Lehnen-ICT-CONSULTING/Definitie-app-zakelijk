<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>MONITORING BUG ANALYSIS - App Startup Metric Measuring Wrong Duration</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>MONITORING BUG ANALYSIS - App Startup Metric Measuring Wrong Duration</h1>

<p><strong>Date:</strong> 2025-10-08</p>
<p><strong>Status:</strong> üî¥ CRITICAL BUG IDENTIFIED</p>
<p><strong>Impact:</strong> Performance monitoring reports misleading data</p>

<p>---</p>

<h2>Executive Summary</h2>

<p>The <code>app_startup_ms</code> performance metric is incorrectly measuring the <strong>full user interaction duration</strong> instead of the <strong>actual app startup time</strong>. This causes false CRITICAL regression alerts and makes performance tracking unreliable.</p>

<p><strong>Evidence:</strong></p>
<pre><code>11:01:15 - WARNING - CRITICAL startup regressie: 392.3ms    ‚Üê CORRECT (actual startup)
11:02:13 - WARNING - CRITICAL startup regressie: 45598.4ms  ‚Üê WRONG (full interaction duration)</code></pre>

<p>The 45.6 second measurement is the <strong>entire definition generation orchestration</strong>, not app startup.</p>

<p>---</p>

<h2>Root Cause Analysis</h2>

<h3>The Problem: Streamlit Rerun Behavior</h3>

<p><strong>Location:</strong> <code>/Users/chrislehnen/Projecten/Definitie-app/src/main.py</code></p>

<pre><code># Line 60: Module-level timer starts when module is imported
_startup_start = time.perf_counter()

def main():
    # Lines 74-78: Normal app flow
    SessionStateManager.initialize_session_state()
    interface = TabbedInterface()
    interface.render()

    # Line 81: Track "startup" performance
    _track_startup_performance()</code></pre>

<h3>How Streamlit Works</h3>

<ol>
<li>**First Run (Cold Start):**</li>
</ol>
<ul>
<li>  - Python imports `main.py` ‚Üí `_startup_start` is set</li>
<li>  - `main()` executes ‚Üí renders UI</li>
<li>  - `_track_startup_performance()` called ‚Üí ‚úÖ **Measures correct startup (392ms)**</li>
</ul>

<ol>
<li>**User Interaction (e.g., "Genereer" button):**</li>
</ol>
<ul>
<li>  - Streamlit **DOES NOT re-import modules** ‚Üí `_startup_start` remains unchanged</li>
<li>  - `main()` executes again from scratch</li>
<li>  - User triggers 45-second definition generation</li>
<li>  - `main()` completes after generation finishes</li>
<li>  - `_track_startup_performance()` called ‚Üí ‚ùå **Measures 45 seconds since ORIGINAL import**</li>
</ul>

<h3>The Bug</h3>

<pre><code>def _track_startup_performance():
    startup_time_ms = (time.perf_counter() - _startup_start) * 1000
    #                                        ^^^^^^^^^^^^^^
    #                                        This is NEVER reset!</code></pre>

<p><strong>What it measures:</strong></p>
<ul>
<li>‚ùå Time since **module import** (which happens ONCE at app start)</li>
<li>‚úÖ Should measure: Time for **current `main()` execution**</li>
</ul>

<p><strong>Why it's wrong:</strong></p>
<ul>
<li>On first run: 392ms (correct - actual startup)</li>
<li>On subsequent reruns: Accumulates time since original import</li>
<li>On definition generation: 45 seconds (wrong - includes user interaction)</li>
</ul>

<p>---</p>

<h2>Evidence Chain</h2>

<h3>Timeline Analysis</h3>

<pre><code>11:01:15 - INFO     - App cold start
             ‚Üì
         [module import] ‚Üê _startup_start = time.perf_counter()
             ‚Üì
         [render UI: 392ms]
             ‚Üì
         _track_startup_performance() ‚Üí tracks 392ms ‚úÖ CORRECT
             ‚Üì
11:02:00 - User clicks "Genereer"
             ‚Üì
         [Streamlit reruns main()] ‚Üê _startup_start UNCHANGED
             ‚Üì
         [Definition generation: 45 seconds]
             ‚Üì
         _track_startup_performance() ‚Üí tracks 45598ms ‚ùå WRONG</code></pre>

<h3>Log Evidence</h3>

<p><strong>First measurement (correct):</strong></p>
<pre><code>11:01:15 - WARNING - CRITICAL startup regressie: 392.3ms</code></pre>
<ul>
<li>This is the actual cold start time</li>
<li>Baseline shows 73.8ms from previous runs (also correct)</li>
</ul>

<p><strong>Second measurement (incorrect):</strong></p>
<pre><code>11:02:13 - WARNING - CRITICAL startup regressie: 45598.4ms</code></pre>
<ul>
<li>This is ~58 seconds after initial import (11:01:15 ‚Üí 11:02:13)</li>
<li>Includes the entire definition generation workflow</li>
<li>NOT app startup time</li>
</ul>

<p>---</p>

<h2>Code Locations</h2>

<h3>Bug Location</h3>

<p><strong>File:</strong> <code>src/main.py</code></p>

<p>| Line | Code | Issue |</p>
<p>|------|------|-------|</p>
<p>| 60 | <code>_startup_start = time.perf_counter()</code> | Module-level variable, set ONCE |</p>
<p>| 100 | <code>startup_time_ms = (time.perf_counter() - _startup_start) * 1000</code> | References stale start time |</p>
<p>| 81 | <code>_track_startup_performance()</code> | Called EVERY rerun |</p>

<h3>Why Module-Level Variables Don't Work</h3>

<pre><code># This pattern is BROKEN for Streamlit:
_startup_start = time.perf_counter()  # Set ONCE when module loads

def main():
    # Runs MANY times (every interaction)
    do_work()

    # This calculates time since MODULE IMPORT, not since main() started
    elapsed = time.perf_counter() - _startup_start  # ‚ùå WRONG</code></pre>

<p><strong>Streamlit execution model:</strong></p>
<ul>
<li>Module imports happen ONCE (when app starts)</li>
<li>`main()` runs MANY times (on every interaction)</li>
<li>Module-level variables persist across reruns</li>
</ul>

<p>---</p>

<h2>Why The Metric Shows 45 Seconds</h2>

<p>The 45.6 second measurement is the <strong>definition generation orchestration time</strong>, captured because:</p>

<ol>
<li>`_startup_start` was set at 11:01:15 during cold start</li>
<li>User interaction triggered at ~11:02:00</li>
<li>Definition generation took 45 seconds</li>
<li>`_track_startup_performance()` called at 11:02:13</li>
<li>Calculation: `time.perf_counter() - _startup_start` = time since 11:01:15 = ~58 seconds</li>
<li>Reported: 45598ms (45.6 seconds)</li>
</ol>

<p><strong>This is NOT startup time - it's the cumulative time since app launch.</strong></p>

<p>---</p>

<h2>Impact Assessment</h2>

<h3>Data Integrity</h3>

<p>| Metric | Current State | Impact |</p>
<p>|--------|---------------|--------|</p>
<p>| <strong>First measurement</strong> | ‚úÖ Correct (392ms) | Baseline contaminated with one good sample |</p>
<p>| <strong>Subsequent measurements</strong> | ‚ùå Wrong (varies wildly) | All reruns measure wrong duration |</p>
<p>| <strong>Baseline</strong> | ‚ö†Ô∏è Unreliable | Mix of correct and incorrect samples |</p>
<p>| <strong>Regression alerts</strong> | ‚ùå False positives | Reports "regressions" that aren't real |</p>

<h3>Performance Tracking Reliability</h3>

<pre><code>Baseline: 73.8ms (from historical correct measurements)
Actual startup: 392ms (correct, but shows as regression)
Rerun measurement: 45598ms (completely wrong)

Result: Unable to detect real performance issues</code></pre>

<h3>Business Impact</h3>

<ul>
<li>‚ùå Cannot trust performance monitoring</li>
<li>‚ùå False alarms waste developer time</li>
<li>‚ùå Real performance regressions might be hidden in noise</li>
<li>‚ùå Misleading data in performance reports</li>
</ul>

<p>---</p>

<h2>Technical Deep Dive</h2>

<h3>Streamlit Execution Model</h3>

<pre><code># STREAMLIT LIFECYCLE:

# 1. App starts (ONCE)
import main  # ‚Üê _startup_start = time.perf_counter()

# 2. User loads page
main.main()  # ‚Üê _track_startup_performance() measures correct time

# 3. User clicks button
main.main()  # ‚Üê _startup_start is STILL from step 1
             # ‚Üê _track_startup_performance() measures wrong time

# 4. User clicks another button
main.main()  # ‚Üê _startup_start is STILL from step 1
             # ‚Üê _track_startup_performance() measures wrong time</code></pre>

<h3>Why This Isn't a Scope Problem</h3>

<p>The issue is NOT about global vs local variables. It's about <strong>measurement lifecycle</strong>:</p>

<pre><code># BROKEN (current):
_global_timer = time.perf_counter()  # Set ONCE

def measure():
    return time.perf_counter() - _global_timer  # Always references original time

# CORRECT (should be):
def measure():
    _local_timer = time.perf_counter()  # Set EACH call
    do_work()
    return time.perf_counter() - _local_timer  # Measures this call only</code></pre>

<p>---</p>

<h2>Related Metrics</h2>

<h3>Other Metrics in Performance Tracker</h3>

<p>Checked if other metrics have the same issue:</p>

<pre><code>$ grep -r "track_metric" src/</code></pre>

<p><strong>Results:</strong></p>
<ul>
<li>`app_startup_ms` - ‚ùå BROKEN (this bug)</li>
<li>Other metrics - Status unknown (need audit)</li>
</ul>

<p><strong>Recommendation:</strong> Audit ALL performance metrics for similar issues.</p>

<p>---</p>

<h2>Proposed Fix</h2>

<h3>Option 1: Track Per-Rerun Startup (RECOMMENDED)</h3>

<p><strong>Strategy:</strong> Measure the time for each <code>main()</code> execution, not module import</p>

<pre><code># src/main.py

# REMOVE module-level timer
# _startup_start = time.perf_counter()  # DELETE THIS

def main():
    # START timer at beginning of main()
    _rerun_start = time.perf_counter()

    try:
        SessionStateManager.initialize_session_state()
        interface = TabbedInterface()
        interface.render()

        # Track THIS execution's time
        _track_rerun_performance(_rerun_start)

    except Exception as e:
        logger.error(f"Applicatie fout: {e!s}")
        st.error(log_and_display_error(e, "applicatie opstarten"))

def _track_rerun_performance(start_time: float):
    """Track performance of current Streamlit rerun.

    Args:
        start_time: perf_counter() value at start of main()
    """
    try:
        from monitoring.performance_tracker import get_tracker

        rerun_time_ms = (time.perf_counter() - start_time) * 1000

        tracker = get_tracker()
        tracker.track_metric(
            "streamlit_rerun_ms",  # Rename to reflect what we actually measure
            rerun_time_ms,
            metadata={"version": "2.0", "platform": sys.platform},
        )

        # Check for performance regression
        alert = tracker.check_regression("streamlit_rerun_ms", rerun_time_ms)
        if alert == "CRITICAL":
            logger.warning(
                f"CRITICAL rerun regressie: {rerun_time_ms:.1f}ms "
                f"(&gt;20% slechter dan baseline)"
            )
        elif alert == "WARNING":
            logger.warning(
                f"WARNING rerun regressie: {rerun_time_ms:.1f}ms "
                f"(&gt;10% slechter dan baseline)"
            )
        else:
            logger.info(f"Rerun tijd: {rerun_time_ms:.1f}ms")

    except Exception as e:
        logger.debug(f"Performance tracking fout (non-critical): {e}")</code></pre>

<p><strong>Pros:</strong></p>
<ul>
<li>‚úÖ Accurately measures what we want</li>
<li>‚úÖ Works correctly with Streamlit reruns</li>
<li>‚úÖ Can detect slow reruns caused by performance issues</li>
<li>‚úÖ Minimal code changes</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
<li>‚ö†Ô∏è Changes metric name (need to update baselines)</li>
<li>‚ö†Ô∏è Different semantic meaning (rerun vs startup)</li>
</ul>

<h3>Option 2: Track True Cold Start Only</h3>

<p><strong>Strategy:</strong> Only measure FIRST run, skip subsequent reruns</p>

<pre><code># src/main.py

_startup_start = time.perf_counter()
_startup_tracked = False  # NEW: Flag to track only once

def main():
    global _startup_tracked

    try:
        SessionStateManager.initialize_session_state()
        interface = TabbedInterface()
        interface.render()

        # Track startup ONLY on first run
        if not _startup_tracked:
            _track_startup_performance()
            _startup_tracked = True

    except Exception as e:
        logger.error(f"Applicatie fout: {e!s}")
        st.error(log_and_display_error(e, "applicatie opstarten"))</code></pre>

<p><strong>Pros:</strong></p>
<ul>
<li>‚úÖ Measures true cold start</li>
<li>‚úÖ No metric rename needed</li>
<li>‚úÖ Simple fix</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
<li>‚ùå Only tracks ONCE per app instance</li>
<li>‚ùå Doesn't detect performance issues in reruns</li>
<li>‚ùå Less useful for ongoing monitoring</li>
</ul>

<h3>Option 3: Track Both Startup and Reruns</h3>

<p><strong>Strategy:</strong> Separate metrics for cold start vs reruns</p>

<pre><code>_cold_start_time = time.perf_counter()
_cold_start_tracked = False

def main():
    global _cold_start_tracked
    _rerun_start = time.perf_counter()

    try:
        SessionStateManager.initialize_session_state()
        interface = TabbedInterface()
        interface.render()

        # Track cold start once
        if not _cold_start_tracked:
            _track_cold_start()
            _cold_start_tracked = True

        # Track every rerun
        _track_rerun_performance(_rerun_start)

    except Exception as e:
        logger.error(f"Applicatie fout: {e!s}")
        st.error(log_and_display_error(e, "applicatie opstarten"))</code></pre>

<p><strong>Pros:</strong></p>
<ul>
<li>‚úÖ Captures both startup AND rerun performance</li>
<li>‚úÖ Most comprehensive monitoring</li>
<li>‚úÖ Can detect different types of issues</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
<li>‚ö†Ô∏è More complex</li>
<li>‚ö†Ô∏è Requires two metrics</li>
</ul>

<p>---</p>

<h2>Recommended Solution</h2>

<p><strong>Implement Option 1: Track Per-Rerun Startup</strong></p>

<h3>Rationale</h3>

<ol>
<li>**Correct Semantics:** Measures what users experience (UI responsiveness)</li>
<li>**Streamlit-Aware:** Works with Streamlit's execution model</li>
<li>**Useful Data:** Detects slow reruns (which affect UX)</li>
<li>**Simple:** Minimal code changes, clear fix</li>
</ol>

<h3>Implementation Steps</h3>

<ol>
<li>‚úÖ Remove module-level `_startup_start` timer</li>
<li>‚úÖ Move timer to start of `main()` function</li>
<li>‚úÖ Rename metric from `app_startup_ms` to `streamlit_rerun_ms`</li>
<li>‚úÖ Pass start time to tracking function</li>
<li>‚úÖ Update performance tracker baseline (delete old data)</li>
<li>‚úÖ Update documentation</li>
</ol>

<h3>Migration Plan</h3>

<pre><code># Step 1: Clear old baseline
python -m src.cli.performance_cli delete-baseline app_startup_ms

# Step 2: Deploy fix (code changes above)

# Step 3: Verify new metric
python -m src.cli.performance_cli history streamlit_rerun_ms --limit 10</code></pre>

<p>---</p>

<h2>Testing Strategy</h2>

<h3>Verification Tests</h3>

<p><strong>Test 1: Cold Start</strong></p>
<pre><code># Expected: ~300-500ms
# Measures: main() execution time</code></pre>

<p><strong>Test 2: Simple Rerun (button click)</strong></p>
<pre><code># Expected: ~100-200ms
# Measures: rerun without heavy work</code></pre>

<p><strong>Test 3: Heavy Rerun (definition generation)</strong></p>
<pre><code># Expected: Should NOT track this as "startup"
# OR: Track separately as "generation_orchestration_ms"</code></pre>

<h3>Acceptance Criteria</h3>

<ul>
<li>‚úÖ Cold start: 300-500ms (matches manual timing)</li>
<li>‚úÖ Simple rerun: <200ms (fast UI response)</li>
<li>‚úÖ No false CRITICAL alerts on user interactions</li>
<li>‚úÖ Baseline converges to stable range</li>
</ul>

<p>---</p>

<h2>Prevention: Avoid Similar Bugs</h2>

<h3>Code Review Checklist</h3>

<p>When adding performance metrics in Streamlit apps:</p>

<ul>
<li>[ ] ‚ùå **NEVER** use module-level timers for per-request metrics</li>
<li>[ ] ‚úÖ **ALWAYS** start timer at beginning of measured operation</li>
<li>[ ] ‚úÖ **ALWAYS** stop timer at end of measured operation</li>
<li>[ ] ‚úÖ **VERIFY** metric measures what the name claims</li>
<li>[ ] ‚úÖ **TEST** with multiple Streamlit reruns</li>
</ul>

<h3>Patterns to Avoid</h3>

<pre><code># ‚ùå BROKEN - Module-level timer
_start = time.perf_counter()

def handler():
    duration = time.perf_counter() - _start  # Wrong on reruns</code></pre>

<h3>Safe Patterns</h3>

<pre><code># ‚úÖ CORRECT - Function-local timer
def handler():
    _start = time.perf_counter()
    do_work()
    duration = time.perf_counter() - _start  # Correct</code></pre>

<pre><code># ‚úÖ CORRECT - Context manager
from contextlib import contextmanager

@contextmanager
def track_performance(metric_name: str):
    start = time.perf_counter()
    yield
    duration = (time.perf_counter() - start) * 1000
    tracker.track_metric(metric_name, duration)

def handler():
    with track_performance("handler_ms"):
        do_work()</code></pre>

<p>---</p>

<h2>Action Items</h2>

<h3>Immediate (P0 - Critical)</h3>

<ol>
<li>‚úÖ **Fix `app_startup_ms` metric** (Option 1)</li>
</ol>
<ul>
<li>  - Timeline: Today</li>
<li>  - Owner: Developer</li>
<li>  - File: `src/main.py`</li>
</ul>

<ol>
<li>‚úÖ **Delete corrupted baseline**</li>
<pre><code>   python -m src.cli.performance_cli delete-baseline app_startup_ms</code></pre>
</ol>

<h3>Short Term (P1 - High)</h3>

<ol>
<li>‚ö†Ô∏è **Audit all performance metrics**</li>
</ol>
<ul>
<li>  - Check for similar module-level timer bugs</li>
<li>  - Files: Grep for `track_metric` across codebase</li>
<li>  - Timeline: This week</li>
</ul>

<ol>
<li>‚ö†Ô∏è **Add performance metric test**</li>
</ol>
<ul>
<li>  - Test: Verify metrics reset between calls</li>
<li>  - Location: `tests/monitoring/test_performance_tracker.py`</li>
<li>  - Timeline: This week</li>
</ul>

<h3>Long Term (P2 - Medium)</h3>

<ol>
<li>üìã **Document Streamlit performance patterns**</li>
</ol>
<ul>
<li>  - Add to `docs/guidelines/STREAMLIT_BEST_PRACTICES.md`</li>
<li>  - Include do's and don'ts</li>
<li>  - Timeline: Next sprint</li>
</ul>

<ol>
<li>üìã **Add linting rule**</li>
</ol>
<ul>
<li>  - Detect module-level timers in Streamlit apps</li>
<li>  - Tool: Custom ruff/pylint rule</li>
<li>  - Timeline: Future</li>
</ul>

<p>---</p>

<h2>Conclusion</h2>

<p>The <code>app_startup_ms</code> metric bug is a <strong>textbook case of Streamlit execution model misunderstanding</strong>. The fix is straightforward but the impact is significant - without correct performance metrics, we cannot reliably detect or diagnose performance issues.</p>

<p><strong>Key Takeaways:</strong></p>
<ol>
<li>Module-level state persists across Streamlit reruns</li>
<li>Performance timers must be scoped to the operation being measured</li>
<li>Metric names must match what they actually measure</li>
<li>Always test metrics with realistic Streamlit interaction patterns</li>
</ol>

<p><strong>Recommended Next Steps:</strong></p>
<ol>
<li>Implement Option 1 fix immediately</li>
<li>Audit all other performance metrics</li>
<li>Establish Streamlit-aware performance tracking patterns</li>
<li>Add tests to prevent regression</li>
</ol>

<p>---</p>

<p><strong>Report Status:</strong> üî¥ CRITICAL BUG CONFIRMED</p>
<p><strong>Fix Difficulty:</strong> üü¢ LOW (simple code change)</p>
<p><strong>Impact:</strong> üî¥ HIGH (affects all performance monitoring)</p>
<p><strong>Priority:</strong> P0 - Fix immediately</p>

  </div>
</body>
</html>