<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DEF-101: Plan B - Detailed Risk Analysis</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>DEF-101: Plan B - Detailed Risk Analysis</h1>
<h2>Complete Risk Assessment with Concrete Mitigation Strategies</h2>

<p><strong>Document Version:</strong> 1.0</p>
<p><strong>Date:</strong> 2025-11-10</p>
<p><strong>Author:</strong> Debug Specialist</p>
<p><strong>Status:</strong> READY FOR REVIEW</p>

<p>---</p>

<h2>üìã Executive Summary</h2>

<p>This document provides comprehensive risk analysis for all 9 issues in Plan B (EPIC-015 Prompt Quality Improvements). Each issue includes:</p>
<ul>
<li>Detailed technical and business risks</li>
<li>Concrete severity/probability scores</li>
<li>Actionable mitigation strategies</li>
<li>Specific detection methods</li>
<li>Clear rollback procedures</li>
<li>Testing strategies</li>
</ul>

<p><strong>Overall Plan Assessment:</strong></p>
<ul>
<li>**Total Duration:** 3 weeks (28 hours effort)</li>
<li>**HIGH RISK Issues:** 2 (DEF-104, DEF-123)</li>
<li>**MEDIUM RISK Issues:** 3 (DEF-102, DEF-126, DEF-124)</li>
<li>**LOW RISK Issues:** 4 (DEF-103, DEF-105, DEF-106, DEF-107)</li>
<li>**Point of No Return:** After DEF-126 (Week 1 end)</li>
<li>**Recommended Approach:** Sequential execution with validation gates</li>
</ul>

<p>---</p>

<h2>üéØ Risk Analysis Per Issue</h2>

<p>---</p>

<h3>DEF-102: Fix 5 Blocking Contradictions</h3>

<p><strong>Effort:</strong> 3 hours | <strong>Risk Level:</strong> MEDIUM | <strong>Week:</strong> 1</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: ESS-02 Exception Clause Too Broad</strong></p>
<ul>
<li>**Severity:** 7/10 (High impact on definition quality)</li>
<li>**Probability:** 40% (Complex linguistic rules, edge cases exist)</li>
<li>**Impact Example:** If exception allows "is een activiteit" universally, non-PROCES definitions might incorrectly use it. Example: "aanhouding is een activiteit..." (should be "handelwijze waarbij...").</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Run test suite with 10 definitions per ontological category (40 total)</li>
<li>Check ESS-02 validation pass rate - should be ~90% (same as baseline)</li>
<li>Manually review 5 PROCES definitions for "is een activiteit" usage</li>
<li>Check if TYPE/RESULTAAT definitions incorrectly use process patterns</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Add ontological category check in ESS-02 exception logic: `if marker == "proces" and "is een activiteit" in definition`</li>
<li>Create test cases: "afspraak is een activiteit" (TYPE - should FAIL), "verhoor is een activiteit waarbij..." (PROCES - should PASS)</li>
<li>Add counter-examples in exception clause docstring</li>
<li>Monitor first 50 definitions post-deploy for false positives</li>
</ol>
<ul>
<li>**Rollback:** `git revert <commit>`, redeploy takes 5 min, no data loss (only validation logic change)</li>
</ul>

<p><strong>Risk 2: Container Exemption Misused</strong></p>
<ul>
<li>**Severity:** 6/10 (Medium - affects definition clarity)</li>
<li>**Probability:** 35% (Ambiguity between vague vs ontological use)</li>
<li>**Impact Example:** Vague definitions like "notificatie is een proces dat plaatsvindt" pass validation (should fail - vague container), while "verhoor is een proces waarbij gecontroleerd wordt" passes (correct - ontological marker).</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Search corpus for "is een proces dat" (vague pattern) vs "is een proces waarbij" (specific pattern)</li>
<li>False positive threshold: < 5% of PROCES definitions should use vague patterns</li>
<li>Manual review: 10 definitions with "proces" - categorize as ontological vs vague</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Clarify exception text: "behalve als ontologische marker gevolgd door specifieke handeling"</li>
<li>Add validation: require "waarbij/waarin" after "is een proces" (not "dat/die")</li>
<li>Test cases: "beoordeling is een proces dat gebeurt" (FAIL), "beoordeling is een proces waarbij beoordeeld wordt" (PASS)</li>
<li>Document in error_prevention_module.py line 138 with examples</li>
</ol>
<ul>
<li>**Rollback:** Simple revert, 5 min, no cascade effects</li>
</ul>

<p><strong>Risk 3: Relative Clause Ambiguity</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - affects readability)</li>
<li>**Probability:** 30% (Grammar rules are nuanced)</li>
<li>**Impact Example:** "waarbij" becomes overused in every definition, reducing variety. Or "die" is still rejected when it adds precision: "persoon die verantwoordelijk is".</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Count "waarbij" frequency in 100 definitions - should be < 60% (not universal)</li>
<li>Check "die" usage in definitions - some should pass if adding precision</li>
<li>Readability score: definitions should vary in structure</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Clarify rule: "Vermijd bijzinnen, behalve wanneer nodig voor precisie"</li>
<li>Add guidance: "Gebruik 'waarbij' voor processen, 'met' voor eigenschappen"</li>
<li>Test edge cases: "medewerker die verantwoordelijk is" (precise - PASS), "document die belangrijk is" (vague - FAIL)</li>
<li>Update error_prevention line 151 with nuanced examples</li>
</ol>
<ul>
<li>**Rollback:** Safe revert, isolated change</li>
</ul>

<p><strong>Risk 4: Cross-Module Inconsistency</strong></p>
<ul>
<li>**Severity:** 8/10 (Critical - modules give conflicting guidance)</li>
<li>**Probability:** 50% (5 modules modified, easy to miss synchronization)</li>
<li>**Impact Example:** ESS-02 says "use 'is een activiteit'", STR-01 says "never start with 'is'", error_prevention says "koppelwerkwoord verboden" ‚Üí AI gets 3 conflicting instructions, produces random results.</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Grep search: "start met 'is'" across all 5 modules (semantic_categorisation, structure_rules, error_prevention, arai_rules)</li>
<li>Generate 20 definitions - check consistency: all PROCES should use "is een activiteit", none should be rejected by STR-01</li>
<li>Check prompt output for contradictory statements (automated via PromptValidator in DEF-106)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Create EXCEPTION_HIERARCHY document in docs/: ESS-02 (Tier 1) > STR-01 (Tier 2) > error_prevention (Tier 3)</li>
<li>Add cross-reference comments in each module: "# Exception: See ESS-02 for ontological markers"</li>
<li>Implement integration test: test_ontological_exception_consistency.py</li>
<li>Test with 10 PROCES definitions - verify all modules allow "is een activiteit"</li>
</ol>
<ul>
<li>**Rollback:** Moderate complexity - need to revert all 5 modules simultaneously (use git revert --no-commit for batch revert)</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: User Confusion from Relaxed Rules</strong></p>
<ul>
<li>**Severity:** 6/10 (Medium - affects trust)</li>
<li>**Probability:** 25% (Users accustomed to strict validation)</li>
<li>**User Impact:** Users see "is een activiteit" in definitions and report as bug ("I thought 'is' was forbidden!"). Confidence in system drops.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>Monitor support tickets for "validation missed error" reports</li>
<li>Track user acceptance rate: % of generated definitions accepted without edit</li>
<li>Survey 5 power users: "Do definitions feel correct?"</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Add tooltip in UI: "‚ö†Ô∏è Ontological exceptions: 'is een activiteit' allowed for PROCES definitions"</li>
<li>Update validation report to show: "‚úì ESS-02 exception applied (ontological marker)"</li>
<li>Create FAQ document: "Why does my definition start with 'is'?"</li>
<li>Gradual rollout: enable for admin user first (1 week), then all users</li>
</ol>
<ul>
<li>**Fallback:** If confusion > 30% in week 1, revert to strict rules + schedule user training session</li>
</ul>

<p><strong>Risk 2: Definition Quality Regression</strong></p>
<ul>
<li>**Severity:** 7/10 (High - core product quality)</li>
<li>**Probability:** 20% (Exceptions might lower the bar)</li>
<li>**User Impact:** More definitions pass validation but are actually lower quality (vague, imprecise). Users don't trust system anymore.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>Compare quality metrics: baseline (current) vs post-DEF-102</li>
<li>Metric: Avg confidence score should stay ‚â• 0.85 (current baseline)</li>
<li>Sample 50 definitions - expert review: % "actually good" should be ‚â• 80%</li>
<li>Track rejection rate in Vaststellen phase - should not increase > 10%</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Implement PromptValidator (DEF-106) BEFORE deploying DEF-102</li>
<li>Add quality gates: if confidence < 0.7, escalate to Expert Review</li>
<li>Monitor for 2 weeks post-deploy - weekly quality report</li>
<li>If quality drops > 15%, immediately revert + root cause analysis</li>
</ol>
<ul>
<li>**Fallback:** Rollback to strict rules + implement graduated exceptions (start with 1 category, expand gradually)</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_ess02_ontological_exceptions.py**: Validate ESS-02 allows "is een activiteit" only for PROCES marker</li>
<li>**test_str01_exception_clause.py**: Verify STR-01 doesn't block ESS-02 patterns</li>
<li>**test_error_prevention_contradictions.py**: Check error_prevention allows ontological markers</li>
<li>**test_cross_module_consistency.py**: Integration test - all 5 modules aligned</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: Create `tests/fixtures/def102_golden_definitions.json`</li>
<li>Sample size: 40 definitions (10 per ontological category)</li>
<li>Categories: TYPE, PARTICULIER, PROCES, RESULTAAT</li>
<li>Pass threshold: ‚â• 90% must pass (same as current baseline)</li>
<li>Example cases:</li>
<li> - PROCES: "verhoor is een activiteit waarbij een verdachte wordt bevraagd" (PASS)</li>
<li> - TYPE: "afspraak is een activiteit" (FAIL - TYPE using PROCES pattern)</li>
<li> - RESULTAAT: "besluit is het resultaat van besluitvorming" (PASS)</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Ambiguous ontological category**: "aanhouding" (could be PROCES or RESULTAAT) - definition should force one category</li>
<li>**Nested containers**: "proces dat een activiteit omvat" - should FAIL (double vague container)</li>
<li>**Legitimate 'is' use**: "is een categorie personen" (TYPE) vs "is een activiteit" (PROCES) - both should PASS</li>
<li>**Cross-category pollution**: TYPE definition using PROCES pattern - should FAIL</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: ESS-02 pass rate ‚Üí Should stay ~90% (¬±5% tolerance)</li>
<li>**Metric 2**: "is een" usage in definitions ‚Üí Should be 40-60% (not 0%, not 100%)</li>
<li>**Metric 3**: False positive rate (vague containers passing) ‚Üí Alert if > 10%</li>
<li>**Metric 4**: User rejection rate (definitions not accepted) ‚Üí Alert if increase > 15%</li>
<li>**Monitoring period**: 2 weeks with daily checks</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> EASY</p>
<p><strong>Time to rollback:</strong> 10 minutes (5 files to revert)</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Identify commit SHA for DEF-102 implementation</li>
<li>`git revert --no-commit <SHA>` (stages revert without committing)</li>
<li>Verify with `git diff --staged` - should show 5 files changed</li>
<li>`git commit -m "rollback(DEF-102): revert contradiction fixes due to <reason>"`</li>
<li>Deploy: `bash scripts/run_app.sh` (automatic restart)</li>
<li>**Verification:** Generate 10 PROCES definitions - should now FAIL validation (strict rules restored)</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>
<ul>
<li>Only validation logic changed, no database schema or data modifications</li>
<li>Existing definitions in database unaffected</li>
<li>Users can continue working during rollback (< 1 min downtime)</li>
</ul>

<p>---</p>

<h3>DEF-103: Categorize 42 Forbidden Patterns</h3>

<p><strong>Effort:</strong> 2 hours | <strong>Risk Level:</strong> LOW | <strong>Week:</strong> 1</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Categorization Misses Patterns</strong></p>
<ul>
<li>**Severity:** 4/10 (Low - doesn't break functionality)</li>
<li>**Probability:** 25% (Subjective categorization)</li>
<li>**Impact Example:** "aspect" categorized as "vague nouns" but acts like "relative clause" in some contexts. Cognitive load reduction goal not achieved.</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Review categorization: each of 42 patterns assigned to exactly 1 category</li>
<li>Test cross-category ambiguity: search definitions for patterns used in multiple ways</li>
<li>User survey (5 users): "Is the grouping clear and helpful?"</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use linguistic framework for categorization (syntax vs semantics vs pragmatics)</li>
<li>Allow 1-2 patterns in multiple categories if genuinely ambiguous</li>
<li>Add clarifying examples per category: "Vague nouns: aspect, element (lack specificity)"</li>
<li>Document rationale in error_prevention_module.py comments</li>
</ol>
<ul>
<li>**Rollback:** Trivial - just remove category headers, keep flat list</li>
</ul>

<p><strong>Risk 2: Cognitive Load NOT Actually Reduced</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - goal not met)</li>
<li>**Probability:** 40% (Subjective improvement, hard to measure)</li>
<li>**Impact Example:** 42 patterns still feel like 42 patterns, just with headers. AI still sees all 42 in prompt, doesn't benefit from grouping.</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Compare prompt token count: before vs after (should be ~same, just +50 tokens for headers)</li>
<li>AI definition quality: no improvement expected (this is UI improvement, not logic)</li>
<li>User feedback: "Does the prompt feel more organized?" (qualitative)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Set realistic expectations: this is DOCUMENTATION improvement, not performance improvement</li>
<li>Consider follow-up: DEF-123 will actually reduce cognitive load (context-aware loading)</li>
<li>Add visual separators in prompt: "\n---\n" between categories</li>
<li>If no perceived benefit after 1 week, consider reverting (low cost to keep, low benefit)</li>
</ol>
<ul>
<li>**Fallback:** Keep implementation but deprioritize - no harm in organized list</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: User Perception - "Nothing Changed"</strong></p>
<ul>
<li>**Severity:** 3/10 (Low - cosmetic issue)</li>
<li>**Probability:** 60% (Users might not notice)</li>
<li>**User Impact:** Users expect performance improvement but see only structural change. Disappointment.</li>
<li>**Detection:** User feedback: "What changed?" vs "Oh, that's clearer now"</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Clear communication: "DEF-103 is a STRUCTURAL improvement (prep for DEF-123)"</li>
<li>Don't announce as major release - bundle with DEF-102</li>
<li>Frame as "foundation for Week 2 improvements"</li>
</ol>
<ul>
<li>**Fallback:** No fallback needed - harmless change</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_forbidden_patterns_categorization.py**: Verify all 42 patterns categorized</li>
<li>**test_category_headers_in_prompt.py**: Check prompt contains category headers</li>
<li>**Visual inspection**: Compare error_prevention_module output before/after</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>No golden reference needed (structure change only)</li>
<li>Validate: Prompt still contains all 42 patterns (none lost during refactor)</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Empty category**: If category has 0 patterns, should not appear in prompt</li>
<li>**Single-item category**: Should still have header (consistency)</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Prompt length ‚Üí Should increase ~50 tokens (category headers), no more</li>
<li>**Metric 2**: Definition quality ‚Üí Should be identical (no logic change)</li>
<li>**Metric 3**: Execution time ‚Üí Should be identical (¬±5ms)</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> TRIVIAL</p>
<p><strong>Time to rollback:</strong> 3 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>`git revert <commit>` (single commit)</li>
<li>Restart app</li>
<li>Verify: error_prevention output is flat list again</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h3>DEF-126: Transform 7 Rule Modules to Instruction Tone</h3>

<p><strong>Effort:</strong> 5 hours | <strong>Risk Level:</strong> MEDIUM | <strong>Week:</strong> 1</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Tone Transformation Weakens Validation</strong></p>
<ul>
<li>**Severity:** 8/10 (Critical - defeats purpose of rules)</li>
<li>**Probability:** 35% (Subjective transformation, easy to go too far)</li>
<li>**Impact Example:**</li>
<li> - Before: "‚ùå Gebruik GEEN koppelwerkwoord aan het begin"</li>
<li> - After (too weak): "Overweeg om niet te starten met een koppelwerkwoord"</li>
<li> - Result: AI ignores instruction, produces invalid definitions</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Compare validation pass rates: before vs after (should be ~same)</li>
<li>Generate 100 definitions - check forbidden pattern frequency:</li>
</ol>
<ul>
<li>    - "start met 'is'": should be < 5% (currently ~3%)</li>
<li>    - "bevat lidwoord": should be < 10% (currently ~7%)</li>
</ul>
<ol>
<li>A/B test: same 10 definitions with old vs new prompts - measure diff</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use IMPERATIVE mood: "Formuleer..." (command) not "Je zou kunnen..." (suggestion)</li>
<li>Keep prohibition strength: Replace "‚ùå GEEN" with "Voorkom" (avoid) not "Overweeg te vermijden" (consider avoiding)</li>
<li>Test each transformation with 5 definitions before committing</li>
<li>Create transformation guidelines doc: "Instruction Tone Principles"</li>
<li>Peer review: have 2nd person validate tone maintains authority</li>
</ol>
<ul>
<li>**Rollback:** Moderate effort - need to revert 7 files with ~200 char changes each</li>
</ul>

<p><strong>Risk 2: Manual Transformation Errors</strong></p>
<ul>
<li>**Severity:** 6/10 (Medium - creates inconsistency)</li>
<li>**Probability:** 45% (Human error across 7 modules √ó ~20 rules = 140 transformations)</li>
<li>**Impact Example:**</li>
<li> - Typos: "Vormuleer" instead of "Formuleer"</li>
<li> - Incomplete transformation: 3 rules in module still use validation tone</li>
<li> - Lost meaning: "Vermijd abstracte containerbegrippen" ‚Üí "Wees specifiek" (too vague)</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Automated check: grep for residual validation keywords ("toetsing", "valideert", "scoort")</li>
<li>Consistency check: all rules in module use same tone pattern</li>
<li>Manual review: read entire prompt output for jarring transitions</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Create transformation script: `scripts/check_instruction_tone.py`</li>
<li>Checklist per module: ‚òê No "toetsing" keywords ‚òê Imperative verbs ‚òê Consistent structure</li>
<li>Transform 1 module ‚Üí test ‚Üí next module (not all at once)</li>
<li>Use find-replace for common patterns: "Toetsvraag:" ‚Üí "Focus:", "valideer" ‚Üí "zorg voor"</li>
<li>Git commit per module (enables granular rollback)</li>
</ol>
<ul>
<li>**Rollback:** Moderate - use git log to find last commit per module, revert individually</li>
</ul>

<p><strong>Risk 3: Inconsistent Phrasing Across Modules</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - UX issue)</li>
<li>**Probability:** 50% (7 different modules, easy to diverge)</li>
<li>**Impact Example:**</li>
<li> - ARAI module: "Formuleer de definitie zo dat..."</li>
<li> - ESS module: "Zorg ervoor dat de definitie..."</li>
<li> - CON module: "Schrijf de definitie zodanig dat..."</li>
<li> - Result: Disjointed prompt, AI confused by style shifts</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Extract instruction verbs from all 7 modules: should use 80% same verbs</li>
<li>Read full prompt output - check for style consistency</li>
<li>Token analysis: measure vocabulary diversity (should not spike)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Define 5 standard instruction verbs: "Formuleer", "Voorkom", "Gebruik", "Vermijd", "Zorg voor"</li>
<li>Template: "[Verb] de definitie [qualifier]: [specific instruction]"</li>
<li>Example: "Formuleer de definitie met een zelfstandig naamwoord aan het begin"</li>
<li>Apply template to all 7 modules before committing</li>
<li>Create `docs/guidelines/INSTRUCTION_TONE_TEMPLATE.md`</li>
</ol>
<ul>
<li>**Rollback:** Easy - revert all 7 modules as batch</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Users Perceive Rules as "Optional"</strong></p>
<ul>
<li>**Severity:** 7/10 (High - undermines system authority)</li>
<li>**Probability:** 30% (If tone is too soft)</li>
<li>**User Impact:** Users think rules are suggestions, ignore validation failures. Quality drops.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>User behavior: rejection rate in Vaststellen phase (should stay ~10%)</li>
<li>Support tickets: "Why did validation fail if rule was just a suggestion?"</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Maintain consequence framing: "Formuleer X, anders [consequence]"</li>
<li>Keep visual severity indicators: ‚ùå, ‚ö†Ô∏è, ‚úÖ (not just text)</li>
<li>Validation report still shows PASS/FAIL (not "considered/ignored")</li>
</ol>
<ul>
<li>**Fallback:** Add disclaimer: "Deze instructies zijn verplicht voor validatie"</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_instruction_tone_consistency.py**: Check all modules use imperative mood</li>
<li>**test_validation_strength_maintained.py**: Generate definitions, compare pass rates</li>
<li>**test_instruction_verb_vocabulary.py**: Verify standard verbs used 80%+</li>
<li>**Manual review**: Read full prompt - check flow and consistency</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: Reuse existing golden definitions (should produce same results)</li>
<li>Sample size: 50 definitions (existing validated definitions)</li>
<li>Pass threshold: ‚â• 95% same results as before (¬±5% acceptable variance)</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Ambiguous instruction**: "Gebruik enkelvoud" - does AI still apply to werkwoorden (infinitief)?</li>
<li>**Tone mismatch**: One harsh "‚ùå VERBODEN" in sea of friendly "Formuleer" - feels jarring</li>
<li>**Lost nuance**: "tenzij" exception clauses - ensure still present after transformation</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Definition quality score ‚Üí Should be identical (¬±3%)</li>
<li>**Metric 2**: Forbidden pattern frequency ‚Üí Should be identical (¬±5%)</li>
<li>**Metric 3**: User acceptance rate ‚Üí Should stay ‚â• 80%</li>
<li>**Metric 4**: Average confidence ‚Üí Should stay ‚â• 0.85</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> MEDIUM (7 files)</p>
<p><strong>Time to rollback:</strong> 15 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Create rollback script: `scripts/rollback_def126.sh`</li>
<li>Script reverts commits for all 7 modules (git revert <SHA1> <SHA2> ... <SHA7>)</li>
<li>Test with 5 definitions - verify validation tone restored</li>
<li>Deploy</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h3>DEF-104: Reorganize Module Execution Order</h3>

<p><strong>Effort:</strong> 3 hours | <strong>Risk Level:</strong> HIGH | <strong>Week:</strong> 2</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Dependency Breaks - Module A Expects Data from Module B</strong></p>
<ul>
<li>**Severity:** 9/10 (CRITICAL - breaks prompt generation)</li>
<li>**Probability:** 60% (15 modules with complex dependencies)</li>
<li>**Impact Example:**</li>
<li> - definition_task_module expects `context.get_shared("ontological_category")` from semantic_categorisation_module</li>
<li> - If execution order puts definition_task BEFORE semantic_categorisation ‚Üí `None` returned ‚Üí prompt generation fails</li>
<li> - Error: "KeyError: 'ontological_category'" or silent failure (empty section)</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Run dependency analyzer: `python -m services.prompts.modules.prompt_orchestrator --analyze-deps`</li>
<li>Check execution order against dependency graph: modules should only reference shared_state set by previous modules</li>
<li>Integration test: generate 20 definitions with new order - check for None/empty sections</li>
<li>Log analysis: check for "Module X validation failed: missing shared state Y"</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**MANDATORY**: Use orchestrator's `resolve_execution_order()` - don't hardcode order</li>
<li>Document dependencies in each module's `get_dependencies()` method</li>
<li>Add assertions in module `execute()`:</li>
<pre><code>     ontological_cat = context.get_shared("ontological_category")
     assert ontological_cat is not None, "semantic_categorisation must run before definition_task"</code></pre>
<li>Create dependency diagram: `docs/architectuur/MODULE_DEPENDENCY_GRAPH.md`</li>
<li>Test suite: `test_execution_order_dependencies.py` - verify no module accesses missing shared_state</li>
</ol>
<ul>
<li>**Rollback:** EASY - revert to `_get_default_module_order()` in prompt_orchestrator.py line 354</li>
</ul>

<p><strong>Risk 2: Metadata No Longer Available When Needed</strong></p>
<ul>
<li>**Severity:** 7/10 (High - affects traceability)</li>
<li>**Probability:** 40% (Metadata usage is scattered)</li>
<li>**Impact Example:**</li>
<li> - definition_task_module (runs LAST) builds metadata section with info from all prior modules</li>
<li> - If module X moves later in order, its metadata isn't available yet</li>
<li> - Result: Incomplete metadata in final prompt (missing rule counts, execution times)</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Check metadata completeness: prompt should contain `execution_metadata` with all module stats</li>
<li>Compare metadata before/after reorder: should have same keys</li>
<li>Test: generate 10 definitions, inspect `get_execution_metadata()` output</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>definition_task_module should ALWAYS be LAST (explicitly documented)</li>
<li>Metadata collection happens AFTER all module execution (already implemented in orchestrator line 193-204)</li>
<li>Add validation: if metadata missing keys, log warning</li>
<li>Test: `test_metadata_completeness.py`</li>
</ol>
<ul>
<li>**Rollback:** EASY - revert order change</li>
</ul>

<p><strong>Risk 3: Prompt Generation Fails Silently</strong></p>
<ul>
<li>**Severity:** 10/10 (CRITICAL - system unusable)</li>
<li>**Probability:** 25% (Orchestrator has error handling, but edge cases exist)</li>
<li>**Impact Example:**</li>
<li> - New order causes ModuleExecutionError in batch 2</li>
<li> - Orchestrator catches exception, returns empty string for that module</li>
<li> - Final prompt is incomplete but doesn't crash - user gets garbage definition</li>
<li> - Users lose trust, system appears broken</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Check logs for "Module execution error" warnings</li>
<li>Prompt length check: before avg 7250 tokens, after should be similar (¬±500)</li>
<li>Test 50 definitions: 0% should have empty sections</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Make orchestrator fail LOUDLY: if ANY module fails, raise exception (don't continue)</li>
<li>Add prompt validation: `assert len(prompt) > 5000, "Prompt suspiciously short"`</li>
<li>UI alert: "Prompt generation failed - contact support"</li>
<li>Implement PromptValidator (DEF-106) to catch incomplete prompts</li>
<li>Staging test: deploy to test environment, generate 100 definitions, check all succeed</li>
</ol>
<ul>
<li>**Rollback:** URGENT - immediate revert if silent failures detected</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Definition Quality Degrades Due to Suboptimal Order</strong></p>
<ul>
<li>**Severity:** 8/10 (High - core product quality)</li>
<li>**Probability:** 35% (Order matters for AI instruction following)</li>
<li>**User Impact:** Definitions become less precise, validation pass rate drops, users frustrated.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>Compare quality metrics: 50 definitions before vs after</li>
<li>Confidence score: should stay ‚â• 0.85 (baseline)</li>
<li>Validation pass rate: should stay ‚â• 90%</li>
<li>User acceptance: should stay ‚â• 80%</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>A/B test new order for 1 week: 50% users old order, 50% new order</li>
<li>Metrics dashboard: track quality metrics in real-time</li>
<li>Rollback trigger: if quality drops >10% for 3 consecutive days, auto-revert</li>
<li>Expert review: sample 20 definitions from new order, manual quality assessment</li>
</ol>
<ul>
<li>**Fallback:** Revert to old order + analyze which modules should be reordered (iterative approach)</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_execution_order_dependencies.py**: Verify no module accesses missing shared_state</li>
<li>**test_module_execution_success_rate.py**: 100 definitions should generate 100% success</li>
<li>**test_prompt_completeness.py**: Check all expected sections present in prompt</li>
<li>**test_metadata_integrity.py**: Verify metadata has all expected keys</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: 50 existing definitions</li>
<li>Expectation: 95%+ produce IDENTICAL results (order change shouldn't affect output)</li>
<li>If results differ: root cause analysis - is difference acceptable?</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Circular dependency**: Module A depends on B, B depends on A (should be caught by orchestrator)</li>
<li>**Missing optional dependency**: Module X optionally uses data from Y - should gracefully handle absence</li>
<li>**Batch execution**: Parallel modules in new order - verify no race conditions</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Module execution success rate ‚Üí 100% (0 failures allowed)</li>
<li>**Metric 2**: Prompt generation time ‚Üí Should be ¬±10% of baseline (no performance regression)</li>
<li>**Metric 3**: Prompt length ‚Üí Should be ¬±5% of baseline (7250 tokens)</li>
<li>**Metric 4**: Definition quality score ‚Üí Should be ‚â• baseline (0.85)</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> EASY</p>
<p><strong>Time to rollback:</strong> 5 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Revert prompt_orchestrator.py `_get_default_module_order()` to original order</li>
<li>Optionally: revert definition_task_module restructure (if that was part of DEF-104)</li>
<li>Restart app</li>
<li>Test: generate 10 definitions - verify prompt format restored</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>
<ul>
<li>Only execution order changed, no database impact</li>
<li>Existing definitions unaffected</li>
</ul>

<p>---</p>

<h3>DEF-106: Create PromptValidator</h3>

<p><strong>Effort:</strong> 2 hours | <strong>Risk Level:</strong> LOW | <strong>Week:</strong> 2</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: False Positives - Blocks Valid Prompts</strong></p>
<ul>
<li>**Severity:** 7/10 (High - breaks functionality)</li>
<li>**Probability:** 30% (Validation rules might be too strict)</li>
<li>**Impact Example:**</li>
<li> - Validator checks "prompt must contain all 7 rule modules"</li>
<li> - Edge case: context-aware loading (DEF-123) legitimately skips CON_rules</li>
<li> - Result: Valid prompt blocked, definition generation fails</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Test with 100 diverse definitions (various contexts, categories)</li>
<li>False positive rate: should be 0% (validator should NEVER block valid prompt)</li>
<li>Log all validation failures for manual review</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Start with LOOSE validation rules: only check critical issues</li>
<li>Critical checks:</li>
</ol>
<ul>
<li>    - Prompt length > 1000 chars</li>
<li>    - Contains "Formuleer de definitie van **{begrip}**"</li>
<li>    - Contains at least 1 validation rule module</li>
<li>    - No duplicate sections (text appears 2x)</li>
</ul>
<ol>
<li>Don't validate module count (allows context-aware loading flexibility)</li>
<li>Use WARN level (not ERROR) for soft failures - log but don't block</li>
<li>Implement flag: `strict_mode=False` by default</li>
</ol>
<ul>
<li>**Rollback:** Disable validator (set `validate_prompt=False` in config)</li>
</ul>

<p><strong>Risk 2: False Negatives - Misses Issues</strong></p>
<ul>
<li>**Severity:** 4/10 (Low - doesn't break, but misses bugs)</li>
<li>**Probability:** 40% (Catching all edge cases is hard)</li>
<li>**Impact Example:**</li>
<li> - Validator checks for duplicate sections</li>
<li> - Doesn't catch: same content with slightly different formatting</li>
<li> - Result: Bloated 9000-token prompt passes validation</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Inject known bad prompts (test cases): empty sections, duplicates, etc.</li>
<li>Validator should catch ‚â• 80% of synthetic issues</li>
<li>Real-world issues: monitor logs for problems validator missed</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Iterative improvement: start simple, add rules as issues discovered</li>
<li>Use fuzzy matching for duplicates: Levenshtein distance > 90% = duplicate</li>
<li>Log all validation warnings (even soft failures) for analysis</li>
<li>Quarterly review: "What issues did validator miss this quarter?"</li>
</ol>
<ul>
<li>**Fallback:** Accept that validator won't catch everything - it's a safety net, not a guarantee</li>
</ul>

<p><strong>Risk 3: Performance Overhead</strong></p>
<ul>
<li>**Severity:** 3/10 (Low - adds latency)</li>
<li>**Probability:** 25% (Validation is cheap, but 100+ definitions = noticeable)</li>
<li>**Impact Example:**</li>
<li> - Validator adds 50ms per prompt</li>
<li> - Bulk generation (100 definitions) adds 5 seconds total</li>
<li> - User perception: "System feels slower"</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Benchmark: time prompt generation with/without validator</li>
<li>Target: < 20ms overhead per prompt</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Optimize checks: use regex compilation, simple string operations</li>
<li>Cache validation results: if prompt unchanged, skip revalidation</li>
<li>Parallel validation: run validator async while AI generates definition</li>
<li>Make validator optional: `enable_prompt_validation=True` in config (can disable if slow)</li>
</ol>
<ul>
<li>**Fallback:** Disable validator in production if overhead > 50ms</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Dev Time vs Value Tradeoff</strong></p>
<ul>
<li>**Severity:** 2/10 (Low - opportunity cost)</li>
<li>**Probability:** 50% (Validator might not catch many real issues)</li>
<li>**User Impact:** 2 hours spent on validator, might catch 1-2 issues per month. Low ROI.</li>
<li>**Detection:** Track validator catches: "Issues prevented" count</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Set success criteria: validator should catch ‚â• 5 issues in first 3 months</li>
<li>If ROI is low, consider removing validator after trial period</li>
<li>Alternatively: expand validator scope to catch more issue types</li>
</ol>
<ul>
<li>**Fallback:** Accept low ROI if validator provides peace of mind (insurance policy)</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_prompt_validator_false_positives.py**: 100 valid prompts ‚Üí 0 should fail</li>
<li>**test_prompt_validator_catches_issues.py**: 20 synthetic bad prompts ‚Üí 16+ should fail (80%)</li>
<li>**test_prompt_validator_performance.py**: Overhead < 20ms per validation</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: Existing 50 definitions</li>
<li>All should PASS validation (100% success rate)</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Empty prompt**: Should FAIL</li>
<li>**Minimal prompt**: Only task, no rules ‚Üí Should WARN</li>
<li>**Duplicate sections**: Same module content 2x ‚Üí Should FAIL</li>
<li>**Context-aware skipped module**: 6 modules instead of 7 ‚Üí Should PASS</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Validation pass rate ‚Üí Should be ‚â• 99% (few failures expected)</li>
<li>**Metric 2**: Validation overhead ‚Üí Should be < 20ms</li>
<li>**Metric 3**: Issues caught ‚Üí Track count (goal: ‚â• 5 in 3 months)</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> TRIVIAL</p>
<p><strong>Time to rollback:</strong> 2 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Set `enable_prompt_validation=False` in config</li>
<li>Restart app (or hot-reload config if supported)</li>
<li>Validator is bypassed, no impact on functionality</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h3>DEF-123: Context-Aware Module Loading</h3>

<p><strong>Effort:</strong> 5 hours | <strong>Risk Level:</strong> HIGH | <strong>Week:</strong> 2</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Wrong Modules Skipped - Missing Critical Content</strong></p>
<ul>
<li>**Severity:** 10/10 (CRITICAL - broken definitions)</li>
<li>**Probability:** 50% (Context relevance is subjective, easy to get wrong)</li>
<li>**Impact Example:**</li>
<li> - Definition for "verhoor" (PROCES) in context "Politie" (NP)</li>
<li> - Context-aware logic skips CON_rules (context rules) because context_score < 0.3</li>
<li> - Result: CON-02 rule not in prompt ‚Üí AI misses context-specific guidance ‚Üí definition too generic</li>
<li> - User rejects definition, trust lost</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Compare definition quality: context-aware vs always-load</li>
<li>Test 50 definitions with strong context (org=NP, jur=Strafrecht) - should reference context appropriately</li>
<li>Check if skipped modules are actually relevant (manual review)</li>
<li>User acceptance rate: should not drop > 10%</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**START CONSERVATIVE**: Only skip modules with 0 context (not < 0.3)</li>
<li>Whitelist "always load" modules: expertise, output_specification, grammar, semantic_categorisation, definition_task (core modules)</li>
<li>Only apply context-aware loading to: CON_rules, [domain_rules if exists], web_lookup_specific</li>
<li>Add override: user can force "load all modules" via checkbox</li>
<li>Log skipped modules: `logger.info(f"Skipped {module_id} due to context_score={score}")`</li>
<li>A/B test: 50% users get context-aware, 50% always-load - compare metrics</li>
</ol>
<ul>
<li>**Rollback:** EASY - set `context_aware_loading=False` in config</li>
</ul>

<p><strong>Risk 2: Context Score Threshold Too High/Low</strong></p>
<ul>
<li>**Severity:** 8/10 (High - affects all definitions)</li>
<li>**Probability:** 60% (Threshold is arbitrary, requires tuning)</li>
<li>**Impact Example:**</li>
<li> - **Too high (0.7)**: Most modules skipped, prompts too short (2000 tokens), definitions too generic</li>
<li> - **Too low (0.1)**: Few modules skipped, no performance benefit, defeats purpose of DEF-123</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Track skip rate: % definitions where ‚â• 1 module skipped (target: 30-50%)</li>
<li>Prompt length distribution: should reduce avg length 10-20% (7250 ‚Üí 6000-6500 tokens)</li>
<li>Quality metrics: should stay ‚â• baseline (no quality loss)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Make threshold configurable: `context_relevance_threshold=0.3` in config</li>
<li>Implement threshold tuning experiment:</li>
</ol>
<ul>
<li>    - Week 1: threshold=0.5 (strict)</li>
<li>    - Week 2: threshold=0.3 (moderate)</li>
<li>    - Week 3: threshold=0.1 (loose)</li>
<li>    - Compare metrics, choose optimal</li>
</ul>
<ol>
<li>Adaptive threshold: if quality drops, automatically raise threshold</li>
<li>Per-module thresholds: CON_rules=0.2 (lower), domain_rules=0.5 (higher)</li>
</ol>
<ul>
<li>**Rollback:** Adjust threshold via config (no code change)</li>
</ul>

<p><strong>Risk 3: Cache Invalidation Bugs</strong></p>
<ul>
<li>**Severity:** 9/10 (CRITICAL - stale prompts)</li>
<li>**Probability:** 35% (Caching context-dependent data is tricky)</li>
<li>**Impact Example:**</li>
<li> - User generates definition for "verhoor" with context NP</li>
<li> - Context-aware loading caches: "Skip CON_rules for NP context"</li>
<li> - User changes context to OM, regenerates</li>
<li> - Cache not invalidated ‚Üí still skips CON_rules ‚Üí wrong prompt for OM context</li>
<li> - Result: Definition for "verhoor@OM" uses cached NP logic</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Test: generate definition with context A, change context to B, regenerate</li>
<li>Verify: module loading decision reflects context B (not cached A)</li>
<li>Check cache keys: should include context hash</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**CRITICAL**: Include context in cache key: `f"modules_{begrip}_{context_hash}"`</li>
<li>Context hash: `hashlib.md5(json.dumps(sorted(context.items()))).hexdigest()`</li>
<li>Cache expiration: 1 hour TTL (context might change)</li>
<li>Add cache invalidation on context change: `cache.clear_pattern(f"modules_{begrip}_*")`</li>
<li>Test suite: `test_context_change_invalidates_cache.py`</li>
</ol>
<ul>
<li>**Rollback:** Disable context-aware caching: `cache_module_decisions=False`</li>
</ul>

<p><strong>Risk 4: Fundamental Architecture Change - Breaks Assumptions</strong></p>
<ul>
<li>**Severity:** 8/10 (High - affects multiple systems)</li>
<li>**Probability:** 40% (Other code might assume all modules always load)</li>
<li>**Impact Example:**</li>
<li> - PromptValidator (DEF-106) checks "prompt must contain 7 rule modules"</li>
<li> - Context-aware loading skips 2 modules ‚Üí validator fails</li>
<li> - Or: Metadata reporting expects 16 modules ‚Üí breaks when only 13 load</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Run full test suite after implementing DEF-123</li>
<li>Check for hardcoded module count assumptions: `grep -r "len(modules) == 16" src/`</li>
<li>Integration test: generate definition with context ‚Üí verify all downstream systems work</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Update PromptValidator: check for "‚â• 5 modules" not "== 16"</li>
<li>Update metadata reporting: use `len(loaded_modules)` not hardcoded count</li>
<li>Add field to metadata: `"context_aware_loading": True, "skipped_modules": ["CON_rules"]`</li>
<li>Document architecture change in `docs/architectuur/MODULE_LOADING_STRATEGY.md`</li>
<li>Update all references to assume variable module count</li>
</ol>
<ul>
<li>**Rollback:** MODERATE complexity - need to revert + fix validator/metadata logic</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Definition Quality Drops Due to Missing Context Rules</strong></p>
<ul>
<li>**Severity:** 9/10 (CRITICAL - breaks core value proposition)</li>
<li>**Probability:** 40% (Context rules are important)</li>
<li>**User Impact:** Definitions become generic, don't reflect organizational specifics. Users lose primary benefit of system.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>User acceptance rate: should stay ‚â• 80%</li>
<li>Expert review: 20 definitions with strong context - should all be context-appropriate</li>
<li>Rejection reasons: track "not specific enough for our org" feedback</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**NEVER skip CON_rules** if any context present (org, jur, or wet)</li>
<li>Implement quality gate: if confidence < 0.7 AND context present, force reload with all modules</li>
<li>User feedback: "Was this definition specific enough for [context]?" (Yes/No)</li>
<li>If "No" > 20%, revert context-aware loading</li>
</ol>
<ul>
<li>**Fallback:** Revert to always-load all modules + implement token reduction via DEF-124 (static caching) instead</li>
</ul>

<p><strong>Risk 2: Performance Benefit NOT Realized</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - wasted effort)</li>
<li>**Probability:** 35% (Token reduction might not translate to speed/cost savings)</li>
<li>**User Impact:** 5 hours dev time spent, but generation time still 4-5 seconds. No user-visible benefit.</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>Measure generation time: before vs after (target: 10-15% reduction)</li>
<li>Measure API costs: tokens sent to GPT-4 (target: 15-20% reduction)</li>
<li>User perception: "Does system feel faster?" (subjective)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Set realistic expectations: 15% token reduction = ~0.5-1 sec faster (not dramatic)</li>
<li>Quantify cost savings: $X saved per 1000 definitions</li>
<li>If no measurable benefit, deprioritize or revert</li>
</ol>
<ul>
<li>**Fallback:** Keep implementation but don't advertise as "performance improvement" (call it "prompt optimization")</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_context_aware_module_loading.py**: Verify modules skipped/loaded based on context_score</li>
<li>**test_context_cache_invalidation.py**: Context change ‚Üí cache invalidated</li>
<li>**test_quality_with_context_aware_loading.py**: 50 definitions with context ‚Üí quality ‚â• baseline</li>
<li>**test_module_loading_edge_cases.py**: 0 context, partial context, full context scenarios</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: 50 definitions (25 with strong context, 25 with weak/no context)</li>
<li>Pass threshold: Quality ‚â• 90% of baseline</li>
<li>Context-appropriate: Expert review 10 strong-context definitions ‚Üí all should reflect context</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**No context**: Should load all modules (fallback)</li>
<li>**Partial context**: org=NP but no jur/wet ‚Üí Should load CON_rules (org is enough)</li>
<li>**Conflicting context**: org=NP + jur=Bestuursrecht (unusual combo) ‚Üí Should load both</li>
<li>**Context change mid-session**: Generate with NP, switch to OM, regenerate ‚Üí Different modules loaded</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Skip rate ‚Üí 30-50% of definitions should skip ‚â• 1 module</li>
<li>**Metric 2**: Prompt length ‚Üí Should reduce 10-20% (7250 ‚Üí 6000-6500 tokens)</li>
<li>**Metric 3**: Generation time ‚Üí Should reduce 10-15% (4.5 ‚Üí 3.8-4.0 sec)</li>
<li>**Metric 4**: Definition quality ‚Üí Should stay ‚â• 0.85 confidence</li>
<li>**Metric 5**: User acceptance ‚Üí Should stay ‚â• 80%</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> MEDIUM</p>
<p><strong>Time to rollback:</strong> 10 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Set `context_aware_loading=False` in prompt_orchestrator config</li>
<li>Restart app</li>
<li>Verify: all modules load for all definitions (check logs)</li>
<li>Test: generate 10 definitions with context ‚Üí should contain all module sections</li>
<li>If PromptValidator was updated, revert validator changes too</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>
<ul>
<li>Only affects prompt generation logic</li>
<li>Existing definitions unaffected</li>
</ul>

<p>---</p>

<h3>DEF-105: Add Visual Priority Badges</h3>

<p><strong>Effort:</strong> 2 hours | <strong>Risk Level:</strong> LOW | <strong>Week:</strong> 3</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Wrong Tier Assignments</strong></p>
<ul>
<li>**Severity:** 4/10 (Low - cosmetic but misleading)</li>
<li>**Probability:** 25% (Tier assignments are subjective)</li>
<li>**Impact Example:**</li>
<li> - ARAI-01 assigned TIER 2 (medium) but is actually critical (TIER 1)</li>
<li> - Users ignore it thinking it's less important</li>
<li> - Result: More ARAI-01 violations in definitions</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Review tier assignments with domain expert</li>
<li>Check validation failure rates per rule - high-failure rules should be TIER 1</li>
<li>User feedback: "Is TIER 1/2/3 labeling accurate?"</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use existing priority field from JSON: `config.get("prioriteit")` ‚Üí map to TIER</li>
<li>Mapping: "hoog" ‚Üí TIER 1, "medium" ‚Üí TIER 2, "laag" ‚Üí TIER 3</li>
<li>Document mapping in `docs/guidelines/RULE_PRIORITY_MAPPING.md`</li>
<li>Validate: all ESS rules should be TIER 1, all VER rules TIER 2-3</li>
<li>If uncertain, default to TIER 1 (safer to over-prioritize)</li>
</ol>
<ul>
<li>**Rollback:** Remove badges, revert to text priority</li>
</ul>

<p><strong>Risk 2: Visual Clutter</strong></p>
<ul>
<li>**Severity:** 3/10 (Low - UX issue)</li>
<li>**Probability:** 40% (Adding badges to 45 rules = 45 new visual elements)</li>
<li>**Impact Example:**</li>
<li> - Prompt becomes visually noisy: ü•á TIER 1 ü•á everywhere</li>
<li> - Users suffer emoji fatigue, start ignoring ALL badges</li>
<li> - Result: Badges lose effectiveness, become decoration</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Review prompt output - does it feel cluttered?</li>
<li>User feedback: "Are badges helpful or distracting?"</li>
<li>Token count: should only add ~100 tokens (45 badges √ó 2 chars)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use SUBTLE badges: [T1], [T2], [T3] instead of ü•áü•àü•â</li>
<li>Only show TIER 1 badges (most important), hide TIER 2-3</li>
<li>Group by tier: "### TIER 1 Rules:" section header (fewer repeated badges)</li>
<li>Make configurable: `show_priority_badges=True` in config (can disable)</li>
</ol>
<ul>
<li>**Fallback:** Simplify to single "CRITICAL" badge for top 10 rules only</li>
</ul>

<p><strong>Risk 3: Test Failures - Output Format Changed</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - breaks CI/CD)</li>
<li>**Probability:** 60% (Tests check exact prompt output)</li>
<li>**Impact Example:**</li>
<li> - Test expects: "üîπ **ESS-02 - Ontologische categorie**"</li>
<li> - Actual output: "ü•á TIER 1 | üîπ **ESS-02 - Ontologische categorie**"</li>
<li> - Result: 20 tests fail due to string mismatch</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Run test suite locally before committing</li>
<li>Check CI/CD pipeline status</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use REGEX in tests: `assert re.search(r"ESS-02.*Ontologische", prompt)`</li>
<li>Don't test exact badge format (test presence, not format)</li>
<li>Update golden reference outputs: regenerate expected strings</li>
<li>Add badge-specific tests: `test_priority_badges_present.py`</li>
</ol>
<ul>
<li>**Rollback:** Update tests to match original format (remove badge assertions)</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Users Don't Use Badges</strong></p>
<ul>
<li>**Severity:** 2/10 (Low - no harm, just wasted effort)</li>
<li>**Probability:** 50% (Users might not find badges useful)</li>
<li>**User Impact:** 2 hours dev time spent on feature nobody uses.</li>
<li>**Detection:** Track if users ask about badge meaning or ignore them</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Add tooltip: "TIER 1 rules are critical for validation"</li>
<li>User onboarding: explain tier system</li>
<li>If no usage after 1 month, consider removing</li>
</ol>
<ul>
<li>**Fallback:** Accept that badges are "nice to have" not critical</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_priority_badges_present.py**: Verify badges appear in prompt output</li>
<li>**test_tier_mapping_correct.py**: Check "hoog" ‚Üí TIER 1, "medium" ‚Üí TIER 2, "laag" ‚Üí TIER 3</li>
<li>**test_prompt_format_unchanged.py**: Ensure badges don't break prompt structure</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: Regenerate expected outputs with badges</li>
<li>Visual review: Check prompt with badges looks good</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Missing priority in JSON**: Should default to TIER 2 or TIER 3 (not crash)</li>
<li>**Invalid priority**: "super-high" not recognized ‚Üí Should default to TIER 2</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Prompt length ‚Üí Should increase ~100 tokens (minimal)</li>
<li>**Metric 2**: User engagement ‚Üí Do users mention badges in feedback?</li>
<li>**Metric 3**: Test pass rate ‚Üí Should be 100% (tests updated)</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> TRIVIAL</p>
<p><strong>Time to rollback:</strong> 5 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Revert badge additions in 7 rule modules</li>
<li>Revert test updates</li>
<li>Restart app</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h3>DEF-124: Static Module Caching</h3>

<p><strong>Effort:</strong> 2 hours | <strong>Risk Level:</strong> MEDIUM | <strong>Week:</strong> 3</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Stale Cache - Static Module Changes Not Reflected</strong></p>
<ul>
<li>**Severity:** 8/10 (High - users see old content)</li>
<li>**Probability:** 50% (Cache invalidation is hard)</li>
<li>**Impact Example:**</li>
<li> - Dev updates expertise_module guidance text</li>
<li> - Module cached from yesterday with old text</li>
<li> - Users generate definitions with outdated instructions</li>
<li> - Definition quality suffers, users confused why changes not visible</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Check module modification time vs cache timestamp</li>
<li>Test: update module file ‚Üí regenerate definition ‚Üí verify new content used</li>
<li>Log cache hits/misses: should see cache miss after module update</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**Cache key includes module file mtime**: `f"{module_id}_{os.path.getmtime(module_file)}"`</li>
<li>Alternatively: cache version number in module: `VERSION = "1.2.3"` ‚Üí include in cache key</li>
<li>Cache TTL: 1 hour (stale cache auto-expires)</li>
<li>Manual cache clear: `cache.clear_pattern("modules_*")` after deploys</li>
<li>Dev mode: disable caching (`enable_module_cache=False` when DEBUG=True)</li>
</ol>
<ul>
<li>**Rollback:** Disable caching: `enable_module_cache=False` in config</li>
</ul>

<p><strong>Risk 2: Cache Size Grows Unbounded</strong></p>
<ul>
<li>**Severity:** 6/10 (Medium - memory leak)</li>
<li>**Probability:** 30% (Depends on cache implementation)</li>
<li>**Impact Example:**</li>
<li> - Each definition caches 16 module outputs √ó 500 chars = 8KB per definition</li>
<li> - 1000 definitions per day √ó 30 days = 240MB cache</li>
<li> - Eventually: out of memory error, app crashes</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Monitor cache size: `cache.get_stats()` ‚Üí check memory usage</li>
<li>Set alert: if cache > 100MB, investigate</li>
<li>Check cache eviction: are old entries being removed?</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Use LRU cache: oldest entries auto-evicted when limit reached</li>
<li>Set max cache size: 10MB or 1000 entries (whichever lower)</li>
<li>Cache only static modules (expertise, grammar, output_spec), not dynamic ones (context_awareness, definition_task)</li>
<li>Cache TTL: 1 hour (prevents long-term accumulation)</li>
<li>Monitoring: log cache size every hour</li>
</ol>
<ul>
<li>**Rollback:** Disable caching or reduce cache size limit</li>
</ul>

<p><strong>Risk 3: Cache Invalidation Bugs - Wrong Content Served</strong></p>
<ul>
<li>**Severity:** 9/10 (CRITICAL - data corruption)</li>
<li>**Probability:** 25% (Cache keys might collide)</li>
<li>**Impact Example:**</li>
<li> - Cache key: `f"module_{module_id}"`</li>
<li> - User A generates definition for "verhoor" ‚Üí caches grammar_module</li>
<li> - User B generates definition for "aanhouding" ‚Üí gets User A's cached grammar_module (if keys collide)</li>
<li> - Result: Definition for "aanhouding" uses grammar rules for "verhoor"</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Test: generate 2 definitions concurrently ‚Üí verify no cross-contamination</li>
<li>Check cache keys: should be unique per (module, context, begrip)</li>
<li>Log cache hits: inspect logs for suspicious patterns</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**CRITICAL**: Cache key MUST include all varying factors:</li>
<pre><code>     cache_key = f"module_{module_id}_{begrip}_{context_hash}_{module_version}"</code></pre>
<li>For truly static modules (expertise, grammar), can omit begrip/context</li>
<li>Test suite: `test_cache_isolation.py` - verify no key collisions</li>
<li>Use namespaced cache: separate cache per user session (if multi-user)</li>
</ol>
<ul>
<li>**Rollback:** Disable caching immediately if data corruption detected</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Minimal Performance Benefit</strong></p>
<ul>
<li>**Severity:** 3/10 (Low - wasted effort)</li>
<li>**Probability:** 40% (Module generation is already fast)</li>
<li>**User Impact:** 2 hours dev time, but generation time only improves 50ms (not noticeable).</li>
<li>**Detection:**</li>
</ul>
<ol>
<li>Benchmark: generation time with/without caching</li>
<li>Target: ‚â• 100ms reduction (noticeable by users)</li>
<li>Cost savings: token reduction ‚Üí API cost (not applicable here - same tokens sent to GPT-4)</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Set realistic expectations: caching saves 50-100ms per definition (not dramatic)</li>
<li>Benefit compounds: 100 definitions = 5-10 seconds saved</li>
<li>If benefit < 50ms, consider removing caching (not worth complexity)</li>
</ol>
<ul>
<li>**Fallback:** Keep caching but don't advertise as "performance improvement"</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**test_module_caching_hit_rate.py**: Generate 10 definitions ‚Üí check cache hit rate ‚â• 50%</li>
<li>**test_cache_invalidation_on_module_update.py**: Update module ‚Üí verify cache miss</li>
<li>**test_cache_isolation.py**: Concurrent definitions ‚Üí no cross-contamination</li>
<li>**test_cache_size_limit.py**: Generate 1000 definitions ‚Üí verify cache size < 10MB</li>
</ol>

<p><strong>Golden Reference Tests:</strong></p>
<ul>
<li>Test suite: 50 definitions with caching enabled</li>
<li>Should produce IDENTICAL results to without caching (100% match)</li>
</ul>

<p><strong>Edge Cases to Test:</strong></p>
<ol>
<li>**Cache cold start**: First definition ‚Üí all cache misses</li>
<li>**Cache warm**: Second definition ‚Üí all cache hits (for static modules)</li>
<li>**Module update mid-session**: Cache invalidated properly?</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Cache hit rate ‚Üí Should be 40-60% (some hits, some misses)</li>
<li>**Metric 2**: Generation time ‚Üí Should reduce 50-100ms</li>
<li>**Metric 3**: Cache size ‚Üí Should stay < 10MB</li>
<li>**Metric 4**: Cache invalidation errors ‚Üí Should be 0 (no stale content)</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> TRIVIAL</p>
<p><strong>Time to rollback:</strong> 2 minutes</p>
<p><strong>Procedure:</strong></p>
<ol>
<li>Set `enable_module_cache=False` in prompt_orchestrator config</li>
<li>Restart app (or hot-reload config)</li>
<li>Verify: no cache hits in logs</li>
</ol>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h3>DEF-107: Documentation & Tests</h3>

<p><strong>Effort:</strong> 4 hours | <strong>Risk Level:</strong> LOW | <strong>Week:</strong> 3</p>

<h4>Technical Risks</h4>

<p><strong>Risk 1: Incomplete Test Coverage</strong></p>
<ul>
<li>**Severity:** 5/10 (Medium - false confidence)</li>
<li>**Probability:** 40% (Easy to miss edge cases)</li>
<li>**Impact Example:**</li>
<li> - Tests cover happy path: ESS-02 exception works for PROCES</li>
<li> - Miss edge case: ESS-02 exception misused for TYPE definitions</li>
<li> - Result: Bug ships to production despite "passing tests"</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Measure test coverage: `pytest --cov` ‚Üí should be ‚â• 80% for modified modules</li>
<li>Review test cases: do they cover edge cases from risk analysis?</li>
<li>Run mutation testing: change code ‚Üí tests should fail</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Create test matrix: each risk identified in this document = ‚â• 1 test case</li>
<li>Test categories:</li>
</ol>
<ul>
<li>    - Unit tests: individual module behavior</li>
<li>    - Integration tests: module interactions (DEF-104 order, DEF-123 loading)</li>
<li>    - End-to-end tests: full definition generation with all changes</li>
</ul>
<ol>
<li>Test edge cases explicitly:</li>
</ol>
<ul>
<li>    - `test_ess02_exception_not_misused_for_type.py`</li>
<li>    - `test_context_aware_loading_with_no_context.py`</li>
</ul>
<ol>
<li>Peer review: have another person review test coverage</li>
</ol>
<ul>
<li>**Rollback:** N/A (documentation/tests don't need rollback)</li>
</ul>

<p><strong>Risk 2: Documentation Out of Sync with Code</strong></p>
<ul>
<li>**Severity:** 4/10 (Low - causes confusion)</li>
<li>**Probability:** 50% (Code evolves faster than docs)</li>
<li>**Impact Example:**</li>
<li> - Documentation says: "ESS-02 exception applies to all process definitions"</li>
<li> - Code actually: only applies if marker == "proces" (stricter)</li>
<li> - Result: Developers/users confused by mismatch</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Code review: verify doc examples match actual behavior</li>
<li>Run doc examples as tests: doctest or code snippets in test suite</li>
<li>Check for outdated references: grep for old module names, deprecated patterns</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Link docs to code: include file paths and line numbers in documentation</li>
<li>Example: "See `src/services/prompts/modules/semantic_categorisation_module.py:182`"</li>
<li>Automated doc tests: extract code examples from docs ‚Üí run as tests</li>
<li>Quarterly doc review: schedule "docs sync" task</li>
<li>Use version control: docs in same repo as code (already done)</li>
</ol>
<ul>
<li>**Rollback:** Update docs to match reverted code</li>
</ul>

<p><strong>Risk 3: Tests Too Brittle</strong></p>
<ul>
<li>**Severity:** 6/10 (Medium - high maintenance cost)</li>
<li>**Probability:** 45% (Testing exact strings is fragile)</li>
<li>**Impact Example:**</li>
<li> - Test checks: `assert "üîπ **ESS-02" in prompt`</li>
<li> - Change badge format: `assert "TIER 1 | üîπ **ESS-02" in prompt`</li>
<li> - Result: Test breaks, requires update despite no functional change</li>
<li>**Detection Method:**</li>
</ul>
<ol>
<li>Track test maintenance: how often do tests break due to non-functional changes?</li>
<li>CI/CD failure rate: should be < 5% false positives</li>
</ol>
<ul>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Test behavior, not implementation:</li>
</ol>
<ul>
<li>    - ‚ùå `assert prompt.startswith("### ‚úÖ Algemene Regels")`</li>
<li>    - ‚úÖ `assert "ESS-02" in prompt and "ontologische" in prompt.lower()`</li>
</ul>
<ol>
<li>Use regex for format-insensitive matching</li>
<li>Test outcomes: definition quality, not prompt format</li>
<li>Snapshot testing: store expected output, flag if changes (but allow approval of intentional changes)</li>
</ol>
<ul>
<li>**Rollback:** Accept some test brittleness as cost of thorough testing</li>
</ul>

<h4>Business Risks</h4>

<p><strong>Risk 1: Documentation Effort vs Utility</strong></p>
<ul>
<li>**Severity:** 2/10 (Low - opportunity cost)</li>
<li>**Probability:** 30% (Docs might not be read)</li>
<li>**User Impact:** 4 hours spent on docs that nobody reads.</li>
<li>**Detection:** Track doc views (if docs are HTML) or ask users "Did you read the docs?"</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Focus on high-value docs: troubleshooting, edge cases, FAQs</li>
<li>Link docs from error messages: "See DEF-102_GUIDE.md for ESS-02 exceptions"</li>
<li>Short, actionable docs (not lengthy tomes)</li>
</ol>
<ul>
<li>**Fallback:** Accept that docs are insurance policy (even if rarely used, valuable when needed)</li>
</ul>

<h4>Testing Strategy</h4>

<p><strong>Pre-Deployment Tests:</strong></p>
<ol>
<li>**Run full test suite**: `pytest --cov=src --cov-report=html`</li>
<li>**Coverage threshold**: ‚â• 80% for modules modified in Plan B</li>
<li>**Manual test run**: Execute all edge cases identified in this risk analysis</li>
<li>**Doc review**: Read all new docs, verify accuracy</li>
</ol>

<p><strong>Test Files to Create:</strong></p>
<ol>
<li>`tests/integration/test_def102_contradictions.py` (20 test cases)</li>
<li>`tests/integration/test_def104_module_order.py` (15 test cases)</li>
<li>`tests/integration/test_def123_context_aware_loading.py` (25 test cases)</li>
<li>`tests/unit/test_prompt_validator.py` (10 test cases)</li>
<li>`tests/smoke/test_plan_b_full_flow.py` (5 end-to-end scenarios)</li>
</ol>

<p><strong>Documentation to Create:</strong></p>
<ol>
<li>`docs/guidelines/DEF-102_ESS02_EXCEPTION_GUIDE.md` (1 page)</li>
<li>`docs/architectuur/MODULE_LOADING_STRATEGY.md` (2 pages)</li>
<li>`docs/guidelines/INSTRUCTION_TONE_TEMPLATE.md` (1 page)</li>
<li>`docs/testing/PLAN_B_TEST_STRATEGY.md` (3 pages)</li>
<li>Update `CLAUDE.md` with Plan B changes</li>
</ol>

<p><strong>Post-Deployment Monitoring:</strong></p>
<ul>
<li>**Metric 1**: Test pass rate ‚Üí Should be 100%</li>
<li>**Metric 2**: Test coverage ‚Üí Should be ‚â• 80% for modified modules</li>
<li>**Metric 3**: Documentation views ‚Üí Track if docs are accessed</li>
</ul>

<h4>Rollback Plan</h4>

<p><strong>Complexity:</strong> N/A (tests/docs don't affect production)</p>
<p><strong>Time to rollback:</strong> 0 minutes</p>
<p><strong>Procedure:</strong> Keep tests and docs even if code is reverted (they document lessons learned)</p>

<p><strong>Data Loss Risk:</strong> NO</p>

<p>---</p>

<h2>üî• Cumulative Risk Analysis</h2>

<h3>Week 1 Compound Risks</h3>

<p><strong>Scenario: DEF-102 Exception Too Broad + DEF-126 Tone Too Weak</strong></p>
<ul>
<li>**Combined Impact:** ESS-02 allows "is een activiteit" broadly + instruction tone sounds optional</li>
<li>**Result:** AI ignores ESS-02 guidance, produces TYPE definitions with PROCES patterns</li>
<li>**Severity:** 9/10 (Critical quality regression)</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Test DEF-102 + DEF-126 together BEFORE merging both</li>
<li>If quality drops > 15%, revert BOTH (not just one)</li>
<li>A/B test: deploy DEF-102 alone for 3 days, then add DEF-126</li>
</ol>

<p><strong>Scenario: DEF-102 Cross-Module Inconsistency + DEF-103 Categorization Confusion</strong></p>
<ul>
<li>**Combined Impact:** 5 modules give conflicting guidance + 42 patterns in confusing categories</li>
<li>**Result:** Prompt is chaotic, AI produces random results</li>
<li>**Severity:** 7/10 (High cognitive load)</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Deploy DEF-103 FIRST (categorization), THEN DEF-102 (logic changes)</li>
<li>PromptValidator checks for contradictory instructions</li>
<li>Manual review: read full prompt after both implemented</li>
</ol>

<h3>Week 2 Compound Risks</h3>

<p><strong>Scenario: DEF-104 Breaks Flow + DEF-123 Skips Wrong Modules</strong></p>
<ul>
<li>**Combined Impact:** New execution order breaks dependencies + context-aware loading skips critical modules</li>
<li>**Result:** Prompt generation fails or produces incomplete prompts</li>
<li>**Severity:** 10/10 (System unusable)</li>
<li>**Probability:** 30% (Two HIGH RISK issues, lots of interaction)</li>
<li>**Recovery Time:** 20-30 minutes (revert both, test, redeploy)</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>**NEVER deploy DEF-104 and DEF-123 same day**</li>
<li>Deploy DEF-104, stabilize for 3 days, THEN deploy DEF-123</li>
<li>If DEF-104 breaks, rollback BEFORE starting DEF-123</li>
<li>Staging environment: test DEF-104 + DEF-123 together for 1 week before production</li>
</ol>

<p><strong>Scenario: DEF-123 Cache Bug + DEF-124 Cache Bug</strong></p>
<ul>
<li>**Combined Impact:** Context-aware caching broken + static module caching broken</li>
<li>**Result:** Stale prompts everywhere, users see wrong content</li>
<li>**Severity:** 9/10 (Critical data integrity issue)</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Implement DEF-123 caching first, test thoroughly</li>
<li>Only start DEF-124 after DEF-123 cache is proven stable</li>
<li>Use different cache namespaces: `cache:context_aware:*` vs `cache:static_modules:*`</li>
<li>If either breaks, disable BOTH caches (safe fallback)</li>
</ol>

<h3>Week 3 Compound Risks</h3>

<p><strong>Week 3 is LOW RISK</strong>: DEF-105, DEF-124, DEF-107 are mostly independent</p>
<ul>
<li>DEF-105 (badges) is cosmetic, doesn't affect logic</li>
<li>DEF-124 (caching) can be disabled without breaking system</li>
<li>DEF-107 (tests/docs) doesn't affect production</li>
</ul>

<h3>Cross-Week Compound Risks</h3>

<p><strong>Point of No Return: After DEF-126 (End of Week 1)</strong></p>
<ul>
<li>**Why:** DEF-126 transforms 7 modules √ó ~20 rules = 140 transformations</li>
<li>**Rollback Complexity:** Need to revert 7 files with manual text changes</li>
<li>**If Week 2 Fails:** Reverting Week 1 becomes painful (7 files √ó 2 weeks of changes)</li>
<li>**Mitigation:**</li>
</ul>
<ol>
<li>Ensure Week 1 is ROCK SOLID before starting Week 2</li>
<li>Freeze Week 1 code: no changes to DEF-102/103/126 during Week 2/3</li>
<li>If Week 2 has major issues, accept Week 1 changes as sunk cost (don't revert)</li>
</ol>

<p>---</p>

<h2>üöß Critical Dependencies</h2>

<h3>Blocking Relationships</h3>

<p><strong>DEF-102 Blocks DEF-126:</strong></p>
<ul>
<li>If DEF-102 exception logic is wrong, DEF-126 tone transformation makes it worse</li>
<li>Example: "Gebruik 'is een activiteit' tenzij..." (bad exception) + "Overweeg..." (weak tone) = disaster</li>
<li>**Solution:** Validate DEF-102 thoroughly before starting DEF-126</li>
</ul>

<p><strong>DEF-104 Blocks DEF-123:</strong></p>
<ul>
<li>If DEF-104 execution order breaks dependencies, DEF-123 context-aware loading will compound issues</li>
<li>Example: Definition task runs before semantic categorisation ‚Üí no ontological_category ‚Üí context-aware loading can't decide which modules to skip</li>
<li>**Solution:** Stabilize DEF-104 for 3-5 days before starting DEF-123</li>
</ul>

<p><strong>DEF-106 Blocks DEF-123:</strong></p>
<ul>
<li>PromptValidator (DEF-106) might assume all 16 modules always load</li>
<li>Context-aware loading (DEF-123) violates this assumption ‚Üí validator fails</li>
<li>**Solution:** Implement DEF-106 AFTER DEF-123, or design DEF-106 to handle variable module count</li>
</ul>

<p><strong>DEF-123 Enables DEF-124:</strong></p>
<ul>
<li>Context-aware loading (DEF-123) determines which modules are static vs dynamic</li>
<li>Static module caching (DEF-124) only caches truly static modules</li>
<li>**Solution:** Implement DEF-124 AFTER DEF-123 is stable</li>
</ul>

<h3>Parallel Execution Risks</h3>

<p><strong>NEVER PARALLELIZE:</strong></p>
<ol>
<li>**DEF-102 + DEF-126**: Both modify error_prevention_module ‚Üí merge conflicts</li>
<li>**DEF-104 + DEF-123**: Both modify prompt_orchestrator ‚Üí conflicts + compounded breakage</li>
<li>**DEF-102 + DEF-104**: DEF-102 changes module content, DEF-104 changes order ‚Üí unpredictable results</li>
</ol>

<p><strong>SAFE TO PARALLELIZE:</strong></p>
<ol>
<li>**DEF-103 + DEF-105**: Both cosmetic changes, different files</li>
<li>**DEF-106 + DEF-107**: Validator creation + tests/docs (different domains)</li>
</ol>

<p><strong>CONDITIONALLY SAFE:</strong></p>
<ol>
<li>**DEF-102 + DEF-103**: If developers coordinate on error_prevention_module edits (DEF-102 changes logic, DEF-103 adds headers)</li>
</ol>

<p>---</p>

<h2>üìä Risk Heatmap</h2>

<p>| Issue | Technical Risk | Business Risk | Rollback Complexity | Overall Risk |</p>
<p>|-------|----------------|---------------|---------------------|--------------|</p>
<p>| <strong>DEF-102</strong> | 7/10 | 6/10 | EASY | <strong>MEDIUM</strong> |</p>
<p>| <strong>DEF-103</strong> | 3/10 | 2/10 | TRIVIAL | <strong>LOW</strong> |</p>
<p>| <strong>DEF-126</strong> | 7/10 | 5/10 | MEDIUM | <strong>MEDIUM</strong> |</p>
<p>| <strong>DEF-104</strong> | 9/10 | 7/10 | EASY | <strong>HIGH</strong> |</p>
<p>| <strong>DEF-106</strong> | 5/10 | 2/10 | TRIVIAL | <strong>LOW</strong> |</p>
<p>| <strong>DEF-123</strong> | 9/10 | 8/10 | MEDIUM | <strong>HIGH</strong> |</p>
<p>| <strong>DEF-105</strong> | 3/10 | 2/10 | TRIVIAL | <strong>LOW</strong> |</p>
<p>| <strong>DEF-124</strong> | 7/10 | 3/10 | TRIVIAL | <strong>MEDIUM</strong> |</p>
<p>| <strong>DEF-107</strong> | 4/10 | 2/10 | N/A | <strong>LOW</strong> |</p>

<h3>Risk Distribution</h3>

<ul>
<li>**HIGH RISK**: 2 issues (DEF-104, DEF-123) - 22% of issues, 36% of effort</li>
<li>**MEDIUM RISK**: 3 issues (DEF-102, DEF-126, DEF-124) - 33% of issues, 36% of effort</li>
<li>**LOW RISK**: 4 issues (DEF-103, DEF-105, DEF-106, DEF-107) - 45% of issues, 28% of effort</li>
</ul>

<h3>Week Risk Profile</h3>

<ul>
<li>**Week 1**: 1 MEDIUM + 2 MEDIUM/LOW = **Moderate Risk**</li>
<li>**Week 2**: 2 HIGH + 1 LOW = **High Risk** (‚ö†Ô∏è Critical week)</li>
<li>**Week 3**: 3 LOW = **Low Risk**</li>
</ul>

<p>---</p>

<h2>üõ°Ô∏è Mitigation Strategy Summary</h2>

<h3>Prevention (Before Implementation)</h3>

<ol>
<li>**Staging Environment**: Deploy to test environment first, run 100 definitions</li>
<li>**A/B Testing**: 50% users get new code, 50% get old code - compare metrics</li>
<li>**Feature Flags**: Ability to disable changes via config (no redeploy needed)</li>
<li>**Code Review**: 2nd developer reviews all HIGH RISK issues</li>
<li>**Dependency Analysis**: Run orchestrator's dependency checker before each deploy</li>
</ol>

<h3>Detection (During Implementation)</h3>

<ol>
<li>**Automated Testing**: 80%+ coverage for modified modules, integration tests for interactions</li>
<li>**Monitoring Dashboard**: Real-time metrics for quality, performance, errors</li>
<li>**Log Analysis**: Daily review of logs for warnings, errors, anomalies</li>
<li>**User Feedback**: Survey 5 power users weekly: "How's the system working?"</li>
<li>**Expert Review**: Sample 20 definitions per week, manual quality assessment</li>
</ol>

<h3>Response (After Issues Detected)</h3>

<ol>
<li>**Rollback Procedures**: Documented step-by-step for each issue (see individual rollback plans)</li>
<li>**Escalation Path**: Define triggers: "If metric X drops >Y%, revert immediately"</li>
<li>**Post-Mortem**: After any rollback, document what went wrong, how to prevent future issues</li>
<li>**Iterative Improvement**: If rollback needed, implement in smaller increments (e.g., DEF-102 one contradiction at a time)</li>
</ol>

<p>---</p>

<h2>üìÖ Recommended Execution Plan</h2>

<h3>Week 1: Foundation (Low-Medium Risk)</h3>

<p><strong>Monday:</strong></p>
<ul>
<li>Deploy DEF-103 (categorization) - 2h</li>
<li>Test with 20 definitions</li>
<li>If stable, proceed</li>
</ul>

<p><strong>Tuesday:</strong></p>
<ul>
<li>Deploy DEF-102 (contradictions) - 3h</li>
<li>Test extensively (40 definitions, 10 per category)</li>
<li>Monitor all day for issues</li>
</ul>

<p><strong>Wednesday-Thursday:</strong></p>
<ul>
<li>Stabilization period for DEF-102</li>
<li>Fix any edge cases discovered</li>
<li>Prepare DEF-126</li>
</ul>

<p><strong>Friday:</strong></p>
<ul>
<li>Deploy DEF-126 (tone transformation) - 5h</li>
<li>Test DEF-102 + DEF-126 together (50 definitions)</li>
<li>Monitor weekend for issues</li>
</ul>

<h3>Week 2: High Risk (Sequential, Not Parallel)</h3>

<p><strong>Monday-Tuesday:</strong></p>
<ul>
<li>Deploy DEF-104 (execution order) - 3h</li>
<li>Test thoroughly (100 definitions)</li>
<li>**DO NOT PROCEED to DEF-123 if any issues**</li>
</ul>

<p><strong>Wednesday:</strong></p>
<ul>
<li>Stabilization day for DEF-104</li>
<li>Fix bugs, monitor metrics</li>
</ul>

<p><strong>Thursday-Friday:</strong></p>
<ul>
<li>Deploy DEF-106 (validator) - 2h (prep for DEF-123)</li>
<li>Deploy DEF-123 (context-aware loading) - 5h</li>
<li>Test extensively (50 definitions with various contexts)</li>
<li>Monitor weekend</li>
</ul>

<h3>Week 3: Cleanup (Low Risk)</h3>

<p><strong>Monday:</strong></p>
<ul>
<li>Deploy DEF-105 (badges) - 2h</li>
<li>Visual review, user feedback</li>
</ul>

<p><strong>Tuesday:</strong></p>
<ul>
<li>Deploy DEF-124 (caching) - 2h</li>
<li>Performance benchmarks</li>
</ul>

<p><strong>Wednesday-Friday:</strong></p>
<ul>
<li>DEF-107 (tests + docs) - 4h</li>
<li>Final integration testing</li>
<li>Prepare handover document</li>
</ul>

<p>---</p>

<h2>üéØ Success Criteria</h2>

<h3>Definition Quality (Non-Negotiable)</h3>
<ul>
<li>‚úÖ Confidence score: ‚â• 0.85 (baseline)</li>
<li>‚úÖ Validation pass rate: ‚â• 90% (baseline)</li>
<li>‚úÖ User acceptance rate: ‚â• 80%</li>
<li>‚úÖ Expert review: ‚â• 80% "actually good" definitions</li>
</ul>

<h3>Performance (Goals)</h3>
<ul>
<li>‚úÖ Generation time: ‚â§ 4.5 seconds (current) or 10% faster</li>
<li>‚úÖ Token usage: 10-20% reduction (DEF-123)</li>
<li>‚úÖ Prompt length: 6000-6500 tokens (from 7250)</li>
</ul>

<h3>System Stability (Non-Negotiable)</h3>
<ul>
<li>‚úÖ Prompt generation success rate: 100% (0 failures)</li>
<li>‚úÖ Module execution errors: 0 per 100 definitions</li>
<li>‚úÖ Rollback-free deployment: Ideally 0 rollbacks, max 1 rollback per week</li>
</ul>

<h3>Testing & Documentation (Quality Gates)</h3>
<ul>
<li>‚úÖ Test coverage: ‚â• 80% for modified modules</li>
<li>‚úÖ Integration tests: ‚â• 90% pass rate</li>
<li>‚úÖ Documentation completeness: All HIGH RISK issues documented</li>
</ul>

<p>---</p>

<h2>üö® Rollback Triggers (Auto-Revert Conditions)</h2>

<h3>Immediate Rollback (No Questions Asked)</h3>
<ol>
<li>**Prompt generation failure rate > 5%** (system broken)</li>
<li>**Data corruption detected** (wrong content served to users)</li>
<li>**Definition quality drops > 20%** (unacceptable regression)</li>
<li>**Validation pass rate < 70%** (too strict or too loose)</li>
</ol>

<h3>Rollback After 24h Observation</h3>
<ol>
<li>**Quality drops 10-20%** (significant but not critical)</li>
<li>**User acceptance < 70%** (users rejecting most definitions)</li>
<li>**Performance regression > 20%** (slower than before)</li>
<li>**Cache errors > 10 per day** (caching is buggy)</li>
</ol>

<h3>Rollback After 1 Week</h3>
<ol>
<li>**No measurable benefit** (effort not justified)</li>
<li>**Unresolved edge cases** (corner cases breaking frequently)</li>
<li>**High maintenance cost** (requires constant fixes)</li>
</ol>

<p>---</p>

<h2>üìù Conclusion</h2>

<p>Plan B is <strong>FEASIBLE</strong> but requires <strong>DISCIPLINED EXECUTION</strong>:</p>

<h3>Key Recommendations</h3>

<ol>
<li>**Sequential, Not Parallel**: Deploy issues one at a time, especially HIGH RISK issues (DEF-104, DEF-123)</li>
<li>**Week 2 is Critical**: DEF-104 + DEF-123 are highest risk - allocate extra time, test thoroughly</li>
<li>**Point of No Return**: After DEF-126 (Week 1 end), rollback becomes painful - ensure Week 1 is solid before proceeding</li>
<li>**Feature Flags**: Implement ability to disable DEF-123 (context-aware loading) and DEF-124 (caching) via config</li>
<li>**Staging First**: Deploy to test environment, run 100-200 definitions before production</li>
<li>**Monitoring**: Set up automated alerts for quality/performance regressions</li>
<li>**Rollback Readiness**: Have rollback procedures documented and tested BEFORE deploying each issue</li>
</ol>

<h3>Risk Mitigation Priority</h3>

<p><strong>Top 3 Risks to Mitigate:</strong></p>
<ol>
<li>**DEF-104 + DEF-123 Compound Failure** (10/10 severity) ‚Üí Deploy 3+ days apart</li>
<li>**DEF-102 Cross-Module Inconsistency** (8/10 severity) ‚Üí Implement PromptValidator first</li>
<li>**DEF-123 Cache Invalidation Bugs** (9/10 severity) ‚Üí Thorough testing with context changes</li>
</ol>

<p><strong>If You Only Have Time for 3 Mitigations:</strong></p>
<ol>
<li>Create dependency diagram for DEF-104 (prevent module execution order bugs)</li>
<li>Implement cache key with context hash for DEF-123 (prevent stale cache bugs)</li>
<li>Add integration test suite for DEF-102 (prevent cross-module contradictions)</li>
</ol>

<p><strong>Confidence Level:</strong> 75% - Plan is solid, but Week 2 has high uncertainty. Success depends on careful testing and willingness to rollback if needed.</p>

<p>---</p>

<p><strong>END OF RISK ANALYSIS</strong></p>

  </div>
</body>
</html>