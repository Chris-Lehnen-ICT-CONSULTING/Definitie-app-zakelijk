<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>UFO Classifier - Concrete Fixes voor Geïdentificeerde Bugs</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">← Terug naar Portal</a>
    <h1>UFO Classifier - Concrete Fixes voor Geïdentificeerde Bugs</h1>

<p><strong>Datum</strong>: 2025-10-07</p>
<p><strong>Bronanalyse</strong>: <code>UFO_CLASSIFIER_DEBUG_ANALYSIS.md</code></p>
<p><strong>Test Suite</strong>: <code>tests/debug/test_ufo_classifier_bugs_reproduction.py</code></p>

<p>---</p>

<h2>Quick Reference</h2>

<p>| Bug ID | Severity | Impact | Fix Time | Status |</p>
<p>|--------|----------|--------|----------|--------|</p>
<p>| BUG-1  | CRITICAL | Tests falen | 1h | TODO |</p>
<p>| BUG-2  | CRITICAL | Silent failures | 30m | TODO |</p>
<p>| BUG-3  | MEDIUM | Inconsistent confidence | 2h | TODO |</p>
<p>| BUG-4  | MEDIUM | Performance hang | 3h | TODO |</p>
<p>| BUG-5  | MEDIUM | Data quality | 2h | TODO |</p>

<p>---</p>

<h2>BUG-1: Input Validatie Fix</h2>

<h3>Probleem</h3>
<pre><code># Current code (line 323-334):
if not term or not definition:
    return UFOClassificationResult(
        term=term or "",
        definition=definition or "",
        primary_category=UFOCategory.UNKNOWN,
        confidence=MIN_CONFIDENCE,
        explanation=["Empty or invalid input"],
    )</code></pre>

<p><strong>Issue</strong>: Tests verwachten <code>ValueError</code>, code retourneert <code>UNKNOWN</code>.</p>

<h3>Fix</h3>

<pre><code>def classify(
    self, term: str, definition: str, context: dict | None = None
) -&gt; UFOClassificationResult:
    """
    Classify a Dutch legal term into UFO category.

    Args:
        term: The term to classify (non-empty string)
        definition: Definition of the term (non-empty string)
        context: Optional context information

    Returns:
        UFOClassificationResult with category and confidence

    Raises:
        ValueError: If term or definition is empty/whitespace after normalization
        TypeError: If term or definition is not a string
    """
    start_time = datetime.now()

    # PHASE 1: Type validation
    if term is None or definition is None:
        raise ValueError("term en definition mogen niet None zijn")

    if not isinstance(term, str):
        raise TypeError(f"term moet een string zijn, kreeg {type(term).__name__}")

    if not isinstance(definition, str):
        raise TypeError(
            f"definition moet een string zijn, kreeg {type(definition).__name__}"
        )

    # PHASE 2: Content validation (after normalization)
    term = self._normalize_text(term)
    definition = self._normalize_text(definition)

    if not term:
        raise ValueError(
            "term mag niet leeg zijn (na verwijderen whitespace en speciale tekens)"
        )

    if not definition:
        raise ValueError(
            "definition mag niet leeg zijn (na verwijderen whitespace en speciale tekens)"
        )

    # PHASE 3: Classification (existing logic)
    try:
        # ... existing feature extraction, disambiguation, etc.
        pass
    except Exception as e:
        logger.error(f"Error classifying '{term}': {e}")
        return UFOClassificationResult(
            term=term,
            definition=definition,
            primary_category=UFOCategory.UNKNOWN,
            confidence=MIN_CONFIDENCE,
            explanation=[f"Classification error: {e!s}"],
        )</code></pre>

<h3>Test Verification</h3>

<pre><code># These should now pass:
def test_empty_term_raises(classifier):
    with pytest.raises(ValueError, match="mag niet leeg"):
        classifier.classify("", "valid definition")

def test_none_term_raises(classifier):
    with pytest.raises(ValueError, match="mogen niet None"):
        classifier.classify(None, "valid definition")

def test_non_string_raises(classifier):
    with pytest.raises(TypeError, match="moet een string zijn"):
        classifier.classify(123, "valid definition")</code></pre>

<p>---</p>

<h2>BUG-2: None Guards Fix</h2>

<h3>Probleem</h3>
<pre><code># _normalize_text() line 200:
if not text or not isinstance(text, str):
    return ""  # Silent failure</code></pre>

<h3>Fix</h3>

<pre><code>def _normalize_text(self, text: str) -&gt; str:
    """
    Normalize text with full Unicode support for Dutch.

    Args:
        text: Input text to normalize (must be string, can be empty)

    Returns:
        Normalized text (can be empty string if input was whitespace-only)

    Raises:
        TypeError: If text is not a string
    """
    # Type check
    if text is None:
        raise TypeError("text cannot be None")

    if not isinstance(text, str):
        raise TypeError(f"text must be str, got {type(text).__name__}")

    # Handle empty string early
    if not text:
        return ""

    # Strip whitespace
    text = text.strip()

    # Remove control characters (except tab, newline, return)
    text = "".join(
        ch
        for ch in text
        if unicodedata.category(ch)[0] != "C" or ch in "\t\n\r"
    )

    # Remove zero-width characters
    zero_width_chars = "\u200b\u200c\u200d\ufeff"
    for zwc in zero_width_chars:
        text = text.replace(zwc, "")

    # Normalize Unicode to NFC (canonical composition)
    text = unicodedata.normalize("NFC", text)

    # Normalize whitespace (collapse multiple spaces)
    text = " ".join(text.split())

    # Limit length with warning
    if len(text) &gt; MAX_TEXT_LENGTH:
        logger.warning(
            f"Text truncated from {len(text)} to {MAX_TEXT_LENGTH} chars "
            f"for input starting with: {text[:50]!r}..."
        )
        text = text[:MAX_TEXT_LENGTH]

    return text</code></pre>

<p>---</p>

<h2>BUG-3: Score Calculation Fix</h2>

<h3>Probleem</h3>
<p>Ambiguity penalty kan leiden tot inconsistente confidence scores.</p>

<h3>Fix</h3>

<pre><code>def _determine_primary_category(
    self, scores: dict[UFOCategory, float]
) -&gt; tuple[UFOCategory, float]:
    """
    Determine primary category from scores with proper guards.

    Args:
        scores: Dictionary mapping categories to raw scores

    Returns:
        Tuple of (primary_category, confidence)
    """
    if not scores:
        return UFOCategory.UNKNOWN, 0.0  # Changed from DEFAULT_CONFIDENCE

    # Sort by score DESC, then category name ASC (for determinism)
    sorted_scores = sorted(
        scores.items(), key=lambda x: (-x[1], x[0].value)
    )

    primary_cat, primary_score = sorted_scores[0]

    # GUARD: Ensure valid score
    primary_score = max(MIN_CONFIDENCE, min(primary_score, MAX_CONFIDENCE))

    # Count ties at top score (within 1% tolerance)
    top_score = primary_score
    num_ties = sum(1 for _, score in sorted_scores if abs(score - top_score) &lt; 0.01)

    if num_ties &gt; 1:
        # Multiple categories tied - very ambiguous
        confidence = primary_score * 0.5
        logger.warning(
            f"Highly ambiguous: {num_ties} categories tied at {top_score:.2f}"
        )
        return primary_cat, max(MIN_CONFIDENCE, confidence)

    # Check second place for ambiguity
    if len(sorted_scores) &gt; 1:
        second_score = sorted_scores[1][1]
        margin = primary_score - second_score

        # Only reduce confidence if:
        # 1. Margin is small (&lt; 0.1)
        # 2. Primary score is significant (&gt; 0.3)
        if margin &lt; 0.1 and primary_score &gt; 0.3:
            # Proportional reduction based on margin
            # Margin 0.1 → no reduction
            # Margin 0.05 → 50% of max reduction
            # Margin 0.0 → max reduction (50%)
            margin_ratio = margin / 0.1  # [0.0, 1.0]
            reduction = 1.0 - (1.0 - margin_ratio) * 0.5  # [0.5, 1.0]

            primary_score *= reduction
            logger.debug(
                f"Ambiguity detected: margin={margin:.2f}, "
                f"reduction factor={reduction:.2f}"
            )

    # FINAL GUARD
    return primary_cat, max(MIN_CONFIDENCE, min(primary_score, MAX_CONFIDENCE))</code></pre>

<p>---</p>

<h2>BUG-4: Regex Performance Fix</h2>

<h3>Probleem</h3>
<p>Geen timeout bij regex matching, kan hangen op pathological input.</p>

<h3>Fix</h3>

<pre><code>import signal
from contextlib import contextmanager

@contextmanager
def time_limit(seconds: int):
    """
    Context manager voor timeout protection.

    Raises:
        TimeoutError: Als operatie te lang duurt
    """

    def signal_handler(signum, frame):
        raise TimeoutError("Operation timed out")

    # Set alarm
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(seconds)

    try:
        yield
    finally:
        # Cancel alarm
        signal.alarm(0)


def _extract_features(self, term: str, definition: str) -&gt; dict[UFOCategory, float]:
    """
    Extract pattern-based features from text with timeout protection.

    Args:
        term: Term to classify
        definition: Definition text

    Returns:
        Dictionary mapping categories to raw scores
    """
    scores = {}

    # Combine and normalize
    combined_text = f"{term} {definition}".lower()

    # Limit text length for regex performance
    # (More aggressive than MAX_TEXT_LENGTH for pattern matching)
    if len(combined_text) &gt; 5000:
        logger.warning(f"Combined text truncated to 5000 chars for pattern matching")
        combined_text = combined_text[:5000]

    # Pattern matching with timeout protection
    try:
        with time_limit(2):  # 2 second timeout
            for category, patterns in self.compiled_patterns.items():
                score = 0.0
                pattern_count = 0

                for pattern in patterns:
                    try:
                        if match := pattern.search(combined_text):
                            # Weight by position (earlier = more important)
                            match_pos = match.start() / max(len(combined_text), 1)
                            match_len = len(match.group(0))

                            # Score: base 0.4, adjusted for position and length
                            position_weight = 1.0 - (match_pos * 0.3)  # [0.7, 1.0]
                            length_weight = min(match_len / 10, 1.0)  # [0.0, 1.0]

                            weight = 0.4 * position_weight * length_weight
                            score += weight
                            pattern_count += 1

                    except Exception as e:
                        logger.warning(f"Pattern matching error: {e}")
                        continue

                # Apply quality penalty for single weak match
                if pattern_count == 1 and score &lt; 0.2:
                    score *= 0.7  # 30% penalty for single weak pattern

                if score &gt; 0:
                    scores[category] = min(score, MAX_CONFIDENCE)

    except TimeoutError:
        logger.error(
            f"Regex timeout for term '{term[:50]}' with {len(combined_text)} chars"
        )
        # Return empty scores - will trigger UNKNOWN classification
        return {}

    return scores</code></pre>

<p>---</p>

<h2>BUG-5: Unicode Normalization Fix</h2>

<p><strong>Zie BUG-2 fix hierboven</strong> - <code>_normalize_text()</code> is volledig herschreven met:</p>
<ul>
<li>Control character removal</li>
<li>Zero-width character removal</li>
<li>NFC normalization</li>
<li>Whitespace collapse</li>
<li>Length limiting met warning</li>
</ul>

<p>---</p>

<h2>EDGE CASE FIXES</h2>

<h3>EDGE-1: All Scores Zero</h3>

<pre><code># In _determine_primary_category():
if not scores:
    return UFOCategory.UNKNOWN, 0.0  # Changed from DEFAULT_CONFIDENCE (0.3)</code></pre>

<p><strong>Rationale</strong>: Geen evidence = geen confidence. 0.3 is misleidend.</p>

<p>---</p>

<h3>EDGE-3: All Scores Equal (Tie-Breaking)</h3>

<pre><code># Deterministic sorting:
sorted_scores = sorted(
    scores.items(),
    key=lambda x: (-x[1], x[0].value)  # Score DESC, category name ASC
)</code></pre>

<p><strong>Rationale</strong>: Bij gelijke scores, sort op category naam voor determinisme.</p>

<p>---</p>

<h3>EDGE-7: Very Long Text Truncation</h3>

<pre><code># In _normalize_text():
if len(text) &gt; MAX_TEXT_LENGTH:
    logger.warning(
        f"Text truncated from {len(text)} to {MAX_TEXT_LENGTH} chars "
        f"for input starting with: {text[:50]!r}..."
    )
    text = text[:MAX_TEXT_LENGTH]</code></pre>

<p><strong>Rationale</strong>: User moet weten dat truncation plaatsvond.</p>

<p>---</p>

<h2>IMPROVED CONFIDENCE FORMULA</h2>

<h3>Current Formula</h3>
<pre><code># Pattern score: 0.4 per match, clamped at 1.0
# Disambiguation: +0.3
# Ambiguity: *0.8 if margin &lt; 0.1</code></pre>

<h3>Proposed Formula</h3>

<pre><code>def _calculate_calibrated_confidence(
    self,
    primary_cat: UFOCategory,
    raw_score: float,
    all_scores: dict[UFOCategory, float],
    num_patterns: int,
    disambiguation_used: bool,
) -&gt; float:
    """
    Calculate calibrated confidence using multiple factors.

    Factors:
    1. Raw score magnitude [0-1]
    2. Pattern count quality [0.5-1.0]
    3. Score distribution (ambiguity) [0.5-1.0]
    4. Disambiguation boost [1.0-1.1]

    Returns:
        Calibrated confidence in [MIN_CONFIDENCE, MAX_CONFIDENCE]
    """

    # Factor 1: Base score
    base = raw_score

    # Factor 2: Quality (more patterns = higher confidence)
    if num_patterns &gt;= 3:
        quality = 1.0
    elif num_patterns == 2:
        quality = 0.9
    elif num_patterns == 1:
        quality = 0.7
    else:
        quality = 0.5

    # Factor 3: Ambiguity (score distribution)
    sorted_scores = sorted(all_scores.values(), reverse=True)
    if len(sorted_scores) &gt;= 2:
        margin = sorted_scores[0] - sorted_scores[1]
        margin_ratio = margin / max(sorted_scores[0], 0.01)
        # High margin_ratio = clear winner = high certainty
        ambiguity = 0.5 + (margin_ratio * 0.5)  # [0.5, 1.0]
    else:
        ambiguity = 1.0  # Only one category

    # Factor 4: Disambiguation
    disambiguation_boost = 1.1 if disambiguation_used else 1.0

    # Combine factors
    confidence = base * quality * ambiguity * disambiguation_boost

    # Clamp
    return max(MIN_CONFIDENCE, min(confidence, MAX_CONFIDENCE))</code></pre>

<p><strong>Usage</strong>:</p>
<pre><code># In classify():
confidence = self._calculate_calibrated_confidence(
    primary_category=primary_cat,
    raw_score=raw_score,
    all_scores=scores,
    num_patterns=len(matched_patterns),
    disambiguation_used=disambiguation_applied,
)</code></pre>

<p>---</p>

<h2>POLICY UPDATES</h2>

<h3>Updated Constants</h3>

<pre><code># Confidence bounds
MIN_CONFIDENCE = 0.05  # Lowered from 0.1
MAX_CONFIDENCE = 1.0
DEFAULT_CONFIDENCE = 0.0  # Changed from 0.3 (no evidence = no confidence)

# Quality gates
CONFIDENCE_HIGH = 0.8  # Auto-approve
CONFIDENCE_MEDIUM = 0.6  # Review recommended
CONFIDENCE_LOW = 0.4  # Review required
CONFIDENCE_INSUFFICIENT = 0.4  # Below this: manual classification

# Secondary category threshold
SECONDARY_THRESHOLD = 0.35  # Raised from 0.2 (reduce noise)</code></pre>

<h3>Result Properties</h3>

<pre><code>@dataclass
class UFOClassificationResult:
    # ... existing fields ...

    @property
    def quality_level(self) -&gt; str:
        """Quality level for approval gates."""
        if self.confidence &gt;= CONFIDENCE_HIGH:
            return "HIGH"
        elif self.confidence &gt;= CONFIDENCE_MEDIUM:
            return "MEDIUM"
        elif self.confidence &gt;= CONFIDENCE_LOW:
            return "LOW"
        else:
            return "INSUFFICIENT"

    @property
    def requires_review(self) -&gt; bool:
        """True if human review recommended."""
        return self.confidence &lt; CONFIDENCE_MEDIUM

    @property
    def requires_manual_classification(self) -&gt; bool:
        """True if automatic classification unreliable."""
        return self.confidence &lt; CONFIDENCE_LOW

    @property
    def can_auto_approve(self) -&gt; bool:
        """True if confidence high enough for auto-approval."""
        return self.confidence &gt;= CONFIDENCE_HIGH</code></pre>

<p>---</p>

<h2>DISAMBIGUATION IMPROVEMENTS</h2>

<h3>Current Issue</h3>
<pre><code># Only first matching pattern gets boost:
for pattern_str, target_category in rules["patterns"].items():
    if re.search(pattern_str, definition_lower):
        scores[target_category] = min(current + 0.3, MAX_CONFIDENCE)
        break  # STOPS HERE!</code></pre>

<h3>Improved Version</h3>

<pre><code>def _apply_disambiguation(
    self, term: str, definition: str, scores: dict[UFOCategory, float]
) -&gt; tuple[dict[UFOCategory, float], bool]:
    """
    Apply disambiguation rules with weighted boosting.

    Returns:
        Tuple of (updated_scores, disambiguation_applied)
    """
    term_lower = term.lower()

    if term_lower not in self.DISAMBIGUATION_RULES:
        return scores, False

    rules = self.DISAMBIGUATION_RULES[term_lower]
    definition_lower = definition.lower()

    # Collect ALL matches with position scores
    matches = []
    for pattern_str, target_category in rules["patterns"].items():
        if match := re.search(pattern_str, definition_lower):
            # Earlier matches are more important
            position = match.start() / max(len(definition_lower), 1)
            position_score = 1.0 - position  # [0.0, 1.0]
            matches.append((target_category, position_score))

    if not matches:
        return scores, False

    # Distribute boost proportionally
    total_position_score = sum(ps for _, ps in matches)

    for target_cat, pos_score in matches:
        weight = pos_score / total_position_score
        boost = 0.3 * weight  # Proportional boost

        current = scores.get(target_cat, 0.0)
        scores[target_cat] = min(current + boost, MAX_CONFIDENCE)

        logger.debug(
            f"Disambiguation: '{term}' → {target_cat.value} "
            f"(boost={boost:.2f}, position_weight={weight:.2f})"
        )

    return scores, True</code></pre>

<p>---</p>

<h2>TEST VERIFICATION CHECKLIST</h2>

<h3>BUG-1 Tests</h3>
<ul>
<li>[ ] `test_empty_term_raises()`</li>
<li>[ ] `test_empty_definition_raises()`</li>
<li>[ ] `test_whitespace_only_raises()`</li>
<li>[ ] `test_none_term_raises()`</li>
<li>[ ] `test_none_definition_raises()`</li>
<li>[ ] `test_non_string_raises()`</li>
</ul>

<h3>BUG-2 Tests</h3>
<ul>
<li>[ ] `test_none_handling()`</li>
<li>[ ] `test_type_checking()`</li>
</ul>

<h3>BUG-3 Tests</h3>
<ul>
<li>[ ] `test_confidence_never_exceeds_one()`</li>
<li>[ ] `test_confidence_bounds()`</li>
</ul>

<h3>BUG-4 Tests</h3>
<ul>
<li>[ ] `test_regex_performance_large_text()`</li>
<li>[ ] `test_redos_protection()`</li>
</ul>

<h3>BUG-5 Tests</h3>
<ul>
<li>[ ] `test_zero_width_removed()`</li>
<li>[ ] `test_bom_removed()`</li>
<li>[ ] `test_control_chars_removed()`</li>
<li>[ ] `test_nfc_nfd_consistency()`</li>
</ul>

<h3>Edge Case Tests</h3>
<ul>
<li>[ ] `test_no_pattern_matches()` - Confidence should be 0.0</li>
<li>[ ] `test_perfect_ambiguity()` - Deterministic tie-breaking</li>
<li>[ ] `test_long_text_warning_logged()`</li>
<li>[ ] `test_emoji_handling()`</li>
<li>[ ] `test_mixed_scripts()`</li>
</ul>

<p>---</p>

<h2>IMPLEMENTATION PLAN</h2>

<h3>Phase 1: Critical Bugs (Priority 1)</h3>
<p><strong>Tijd</strong>: 2 uur</p>

<ol>
<li>**BUG-1**: Input validatie</li>
</ol>
<ul>
<li>  - Update `classify()` met type/content checks</li>
<li>  - Add proper exception raising</li>
<li>  - Test: Run `test_ufo_classifier_edge_cases.py`</li>
</ul>

<ol>
<li>**BUG-2**: None guards</li>
</ol>
<ul>
<li>  - Rewrite `_normalize_text()` met guards</li>
<li>  - Add zero-width/control char removal</li>
<li>  - Test: Run bug reproduction tests</li>
</ul>

<h3>Phase 2: Score & Performance (Priority 2)</h3>
<p><strong>Tijd</strong>: 5 uur</p>

<ol>
<li>**BUG-3**: Score calculation</li>
</ol>
<ul>
<li>  - Update `_determine_primary_category()` met proper guards</li>
<li>  - Implement proportional ambiguity penalty</li>
<li>  - Test: Confidence bounds tests</li>
</ul>

<ol>
<li>**BUG-4**: Regex timeout</li>
</ol>
<ul>
<li>  - Add `time_limit()` context manager</li>
<li>  - Wrap pattern matching in timeout</li>
<li>  - Test: Performance tests</li>
</ul>

<ol>
<li>**EDGE-3**: Tie-breaking</li>
</ol>
<ul>
<li>  - Update sorting for determinism</li>
<li>  - Test: Run disambiguation tests 10x</li>
</ul>

<h3>Phase 3: Confidence & Policy (Priority 3)</h3>
<p><strong>Tijd</strong>: 4 uur</p>

<ol>
<li>**Confidence Formula**: Implement calibrated version</li>
</ol>
<ul>
<li>  - Add `_calculate_calibrated_confidence()`</li>
<li>  - Update result properties</li>
<li>  - Test: Compare old vs new confidence scores</li>
</ul>

<ol>
<li>**Policy Updates**: Update constants and thresholds</li>
</ol>
<ul>
<li>  - DEFAULT_CONFIDENCE = 0.0</li>
<li>  - SECONDARY_THRESHOLD = 0.35</li>
<li>  - Add quality level properties</li>
</ul>

<ol>
<li>**Disambiguation**: Improve weighted boosting</li>
</ol>
<ul>
<li>  - Rewrite `_apply_disambiguation()`</li>
<li>  - Handle multiple matches proportionally</li>
<li>  - Test: Disambiguation tests</li>
</ul>

<h3>Phase 4: Testing & Validation (Priority 4)</h3>
<p><strong>Tijd</strong>: 3 uur</p>

<ol>
<li>Run full test suite</li>
<li>Fix any remaining failures</li>
<li>Update documentation</li>
<li>Code review</li>
</ol>

<p><strong>Total Estimated Time</strong>: ~14 uur (2 dagen)</p>

<p>---</p>

<h2>VERIFICATION COMMANDS</h2>

<pre><code># Run bug reproduction tests
pytest tests/debug/test_ufo_classifier_bugs_reproduction.py -v

# Run edge case tests
pytest tests/services/test_ufo_classifier_edge_cases.py -v

# Run full classifier test suite
pytest tests/services/test_ufo_classifier*.py -v

# Check coverage
pytest tests/services/test_ufo_classifier*.py --cov=src.services.ufo_classifier_service --cov-report=html

# Benchmark performance
pytest tests/debug/test_ufo_performance.py -v</code></pre>

<p>---</p>

<h2>ROLLBACK PLAN</h2>

<p>Als fixes problemen veroorzaken:</p>

<ol>
<li>**Backup current version**:</li>
<pre><code>   cp src/services/ufo_classifier_service.py src/services/ufo_classifier_service.py.backup</code></pre>
</ol>

<ol>
<li>**Git revert**:</li>
<pre><code>   git checkout src/services/ufo_classifier_service.py</code></pre>
</ol>

<ol>
<li>**Cherry-pick safe fixes**:</li>
</ol>
<ul>
<li>  - Start met BUG-1 en BUG-2 (input validatie)</li>
<li>  - Test grondig voor elke fix</li>
<li>  - Commit incrementeel</li>
</ul>

<p>---</p>

<h2>NOTES</h2>

<ul>
<li>Alle fixes zijn **backwards incompatible** maar dit is acceptabel:</li>
<li> - Single-user application</li>
<li> - No production usage yet</li>
<li> - Refactor, geen backwards compatibility (per CLAUDE.md)</li>
</ul>

<ul>
<li>Tests zullen **initieel falen** - dit is verwacht:</li>
<li> - Huidige implementation matcht niet test expectations</li>
<li> - Fixes maken implementation compliant met tests</li>
</ul>

<ul>
<li>Performance impact is **minimal**:</li>
<li> - Type checking: negligible</li>
<li> - Timeout wrapper: only for edge cases</li>
<li> - Unicode normalization: already present, just improved</li>
</ul>

<p>---</p>

<p><strong>End of Fixes Document</strong></p>

  </div>
</body>
</html>