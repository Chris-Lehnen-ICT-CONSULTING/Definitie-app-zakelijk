<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>üìã Overdracht Document - Episch Verhaal 3: Web Lookup Modernization</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>üìã Overdracht Document - Episch Verhaal 3: Web Lookup Modernization</h1>

<h2>üéØ Executive Summary</h2>

<p>Dit document bevat alle informatie voor het implementeren van Episch Verhaal 3: Web Lookup Modernization. Het doel is OM de web lookup service te moderniseren voor betere definitie-generatie via LLM context verrijking, met volledige bronverantwoording en provenance tracking.</p>

<p><strong>Geschatte doorlooptijd:</strong> 4-7 dagen totaal (MVP in 2 dagen)</p>
<p><strong>Status:</strong> Ready for implementation</p>
<p><strong>Document datum:</strong> 09-01-2025</p>

<h2>üìä Current Situation</h2>

<h3>Wat We Hebben</h3>
<ul>
<li>**5 legacy implementaties** verspreid over meerdere files (633+ regels code)</li>
<li>**ModernWebLookupService** partially ge√Ømplementeerd (alleen Wikipedia/SRU)</li>
<li>**HybridContextEngine** kan web context gebruiken</li>
<li>**Geen bronverantwoording** in UI of database</li>
<li>**Encoding problemen** in legacy code (UTF-8 issues)</li>
<li>**Geen caching** in moderne implementatie</li>
</ul>

<h3>Kritieke Legacy Files</h3>
<pre><code>src/web_lookup/
‚îú‚îÄ‚îÄ lookup.py (475 lines) - 7 bronnen implementaties
‚îú‚îÄ‚îÄ bron_lookup.py (633 lines) - Validatie &amp; scoring
‚îú‚îÄ‚îÄ definitie_lookup.py (717 lines) - Duplicate detection
‚îú‚îÄ‚îÄ juridische_lookup.py (89 lines) - Juridische regex patterns
‚îî‚îÄ‚îÄ [various _broken/_encoding_issue variants]</code></pre>

<h3>Probleem Statement</h3>
<ol>
<li>**Geen unified contract** - Elke provider eigen format</li>
<li>**Geen provenance tracking** - Bronnen worden niet opgeslagen</li>
<li>**Prestaties issues** - Geen caching, sequenti√´le calls</li>
<li>**UI geeft geen bronnen weer** - Gebruiker ziet niet wat gebruikt is</li>
<li>**Export mist bronnen** - Geen bronverantwoording in exports</li>
</ol>

<h2>üöÄ Implementatie Roadmap</h2>

<h3>Fase 0: Contract & Specification (0.5 dag) ‚≠ê START HIER</h3>

<p><strong>Doel:</strong> Volledig gespecificeerd contract voordat code geschreven wordt</p>

<p><strong>Deliverables:</strong></p>
<ol>
<li>Create `docs/technical/web-lookup-contract-v1.0.md`</li>
<li>Create `src/services/web_lookup/contracts.py`:</li>
</ol>

<pre><code>from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
from enum import Enum

class LookupErrorType(Enum):
    TIMEOUT = "Provider timeout exceeded"
    NETWORK = "Network connection failed"
    PARSE = "Response parsing failed"
    RATE_LIMIT = "Rate limit exceeded"
    AUTH = "Authentication failed"
    INVALID_RESPONSE = "Invalid/empty response"

@dataclass
class WebLookupResult:
    # Core Fields (REQUIRED)
    provider: str           # "wikipedia", "sru_overheid"
    source_label: str       # "Wikipedia NL", "Overheid.nl"
    title: str             # Article title
    url: str               # Absolute URL (validated)
    snippet: str           # Sanitized, max 500 chars
    score: float           # 0.0-1.0 normalized

    # Usage Tracking (REQUIRED)
    used_in_prompt: bool
    position_in_prompt: int

    # Metadata (REQUIRED)
    retrieved_at: datetime
    content_hash: str      # SHA256 for dedup
    error: Optional[str]   # Error from LookupErrorType

    # Juridisch (Nederlandse Overheid)
    legal_refs: List[str] = None
    is_authoritative: bool = False
    legal_weight: float = 0.0

    # Linguistic (Legacy preservation)
    is_plurale_tantum: bool = False

    # Caching
    cache_key: str = ""
    ttl_seconds: int = 3600</code></pre>

<ol>
<li>Create `config/web_lookup_defaults.yaml` (deze wordt door de app standaard gebruikt; optioneel kun je met `WEB_LOOKUP_CONFIG=/pad/naar/config.yaml` overschrijven):</li>
</ol>

<pre><code>web_lookup:
  enabled: true

  cache:
    strategy: "stale-while-revalidate"
    grace_period: 300
    default_ttl: 3600
    max_entries: 1000

  sanitization:
    strip_tags: [script, style, iframe, object, embed, form]
    block_protocols: [javascript, data, vbscript]
    max_snippet_length: 500

  providers:
    wikipedia:
      enabled: true
      weight: 0.7
      timeout: 5
      cache_ttl: 7200
      min_score: 0.3

    sru_overheid:
      enabled: true
      weight: 1.0  # Highest for juridisch
      timeout: 5
      cache_ttl: 3600
      min_score: 0.4

  context_mappings:
    DJI: ["Pbw", "WvSr"]
    OM: ["WvSv"]
    Rechtspraak: ["Rv"]</code></pre>

<p><strong>‚úÖ Definition of Done:</strong></p>
<ul>
<li>[ ] Contract reviewed by tech lead</li>
<li>[ ] No ambiguities in specification</li>
<li>[ ] Config file created and validated</li>
</ul>

<h3>Fase 1: Core Service Implementatie (1-2 dagen)</h3>

<p><strong>Dag 1: Wikipedia Adapter</strong></p>

<ol>
<li>**Refactor WikipediaService** (`src/services/web_lookup/wikipedia_service.py`):</li>
<pre><code>class WikipediaAdapter:
    async def lookup(self, query: str, context: dict) -&gt; List[WebLookupResult]:
        # 1. Call Wikipedia API
        # 2. Normalize to WebLookupResult
        # 3. Apply sanitization
        # 4. Calculate score
        # 5. Add to cache
        return results</code></pre>
</ol>

<ol>
<li>**Implement Basic Cache**:</li>
<pre><code>class SimpleCache:
    def __init__(self, max_entries=1000):
        self._cache = {}  # key -&gt; (value, expires_at)
        self._max_entries = max_entries

    def get(self, key: str) -&gt; Optional[Any]:
        # Check TTL, return if valid

    def set(self, key: str, value: Any, ttl: int):
        # Store with expiration</code></pre>
</ol>

<p><strong>Dag 2: SRU/Overheid.nl Adapter</strong></p>

<ol>
<li>**Refactor SRUService** (`src/services/web_lookup/sru_service.py`)</li>
<li>**Add Rate Limiting**:</li>
<pre><code>class RateLimiter:
    def __init__(self, max_per_minute=10):
        self._calls = []
        self._max = max_per_minute

    async def acquire(self):
        # Wait if limit reached</code></pre>
</ol>

<ol>
<li>**Juridische Ref Extraction**:</li>
<pre><code>JURIDISCHE_PATTERNS = [
    r"artikel\s+(\d+[a-z]?)\s+(?:van\s+)?(?:de\s+)?([\w\s]+wet)",
    r"art\.\s*(\d+[a-z]?(?::\d+)?)\s+(\w+)",
    # etc.
]</code></pre>
</ol>

<h3>Fase 1.5: Quick Win Provenance (0.5 dag)</h3>

<p><strong>No Database Changes!</strong> Store in metadata:</p>

<ol>
<li>**Update Definition Storage**:</li>
<pre><code># In DefinitionRepository.save()
definition.metadata = {
    "sources": [
        {
            "provider": result.provider,
            "title": result.title,
            "url": result.url,
            "snippet": result.snippet,
            "score": result.score,
            "used_in_prompt": result.used_in_prompt,
            "retrieved_at": result.retrieved_at.isoformat()
        }
        for result in lookup_results
    ],
    # Other metadata...
}</code></pre>
</ol>

<ol>
<li>**Basic UI Display** (`src/ui/components/definition_generator_tab.py`):</li>
<pre><code># After definition generation
if generation_result.get("metadata", {}).get("sources"):
    st.markdown("### üìö Gebruikte Bronnen")

    sources = generation_result["metadata"]["sources"]
    for source in sources:
        if source.get("used_in_prompt"):
            with st.expander(f"{source['provider']}: {source['title'][:50]}..."):
                st.write(f"**URL:** {source['url']}")
                st.write(f"**Fragment:** {source['snippet'][:200]}...")
                if os.getenv("DEV_MODE"):
                    st.caption(f"Score: {source['score']:.2f}")</code></pre>
</ol>

<ol>
<li>**Update Export** (`src/services/export_service.py`):</li>
<pre><code># In export_to_json()
export_data["bronnen"] = definition.metadata.get("sources", [])</code></pre>
</ol>

<h3>Fase 2: Database Migration (1 dag) - OPTIONAL</h3>

<p><strong>Only after MVP is working!</strong></p>

<ol>
<li>**Create Migration** (`migrations/004_add_definitie_bronnen.sql`):</li>
<pre><code>CREATE TABLE IF NOT EXISTS definitie_bronnen (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    definitie_id INTEGER REFERENCES definities(id),
    source_type VARCHAR(50) NOT NULL,
    url TEXT NOT NULL,
    title VARCHAR(500),
    snippet TEXT,
    score DECIMAL(3,2),
    used_in_prompt BOOLEAN DEFAULT FALSE,
    metadata JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_definitie_id (definitie_id)
);</code></pre>
</ol>

<ol>
<li>**Data Migration Script**:</li>
<pre><code># Migrate from metadata to DB
for definition in all_definitions:
    if definition.metadata.get("sources"):
        for source in definition.metadata["sources"]:
            insert_into_definitie_bronnen(definition.id, source)</code></pre>
</ol>

<h3>Fase 3: Full Implementatie (1-2 dagen)</h3>

<p>Add remaining 5 providers:</p>
<ul>
<li>Wiktionary</li>
<li>Ensie.nl</li>
<li>Wetten.nl</li>
<li>Strafrechtketen.nl</li>
<li>Kamerstukken.nl</li>
</ul>

<h3>Fase 4: Testen & Quality (0.5-1 dag)</h3>

<p>Create offline tests with mocked responses.</p>

<h2>üîß Technical Implementatie Details</h2>

<h3>Ranking Algorithm</h3>
<pre><code>def rank_results(results: List[WebLookupResult], context: dict) -&gt; List[WebLookupResult]:
    # 1. Apply provider weights
    for result in results:
        provider_weight = config["providers"][result.provider]["weight"]
        result.final_score = provider_weight * result.score

        # 2. Boost juridisch for legal context
        if context.get("juridisch") and result.is_authoritative:
            result.final_score *= 1.5

    # 3. Sort with tiebreakers
    return sorted(results, key=lambda r: (
        -r.final_score,      # Higher score first
        -r.is_authoritative, # Juridisch first
        r.title,            # Alphabetical
        r.url               # URL as final tiebreaker
    ))</code></pre>

<h3>Deduplication</h3>
<pre><code>def deduplicate(results: List[WebLookupResult]) -&gt; List[WebLookupResult]:
    seen_urls = {}
    seen_hashes = {}
    deduped = []

    for result in results:
        canonical_url = normalize_url(result.url)

        if canonical_url in seen_urls:
            # Keep highest score
            if result.final_score &gt; seen_urls[canonical_url].final_score:
                seen_urls[canonical_url] = result
        elif result.content_hash in seen_hashes:
            # Same content, different URL
            if result.final_score &gt; seen_hashes[result.content_hash].final_score:
                seen_hashes[result.content_hash] = result
        else:
            seen_urls[canonical_url] = result
            seen_hashes[result.content_hash] = result
            deduped.append(result)

    return deduped</code></pre>

<h3>Context Pack Building</h3>
<pre><code>def build_context_pack(
    results: List[WebLookupResult],
    max_tokens: int = 1000
) -&gt; Tuple[str, List[WebLookupResult]]:
    context_parts = []
    used_results = []
    token_count = 0

    for i, result in enumerate(results):
        snippet_tokens = estimate_tokens(result.snippet)
        if token_count + snippet_tokens &gt; max_tokens:
            break

        context_parts.append(
            f"[{result.source_label}] {result.title}: {result.snippet}"
        )
        result.used_in_prompt = True
        result.position_in_prompt = i
        used_results.append(result)
        token_count += snippet_tokens

    return "\n\n".join(context_parts), used_results</code></pre>

<h2>‚ö†Ô∏è Critical Implementatie Notities</h2>

<h3>1. Start Small</h3>
<ul>
<li>**Begin with Wikipedia only**</li>
<li>Get full flow working end-to-end</li>
<li>Dan add SRU/Overheid.nl</li>
<li>Other providers last</li>
</ul>

<h3>2. No Breaking Changes</h3>
<ul>
<li>Keep existing interfaces intact</li>
<li>ModernWebLookupService should still work</li>
<li>Don't modify existing database schema (use metadata)</li>
</ul>

<h3>3. Testen Strategy</h3>
<pre><code># Use mocked responses for all tests
@patch('httpx.AsyncClient.get')
async def test_wikipedia_lookup(mock_get):
    mock_get.return_value = Mock(
        status_code=200,
        json=lambda: {"extract": "Test content"}
    )

    result = await adapter.lookup("test")
    assert result[0].provider == "wikipedia"
    assert result[0].snippet == "Test content"</code></pre>

<h3>4. Feature Flags</h3>
<pre><code># Check if web lookup is enabled
if os.getenv("WEB_LOOKUP_ENABLED", "false").lower() == "true":
    results = await lookup_service.lookup(query)
else:
    results = []  # Graceful degradation</code></pre>

<h3>5. Error Handling</h3>
<pre><code>try:
    results = await provider.lookup(query)
except asyncio.TimeoutError:
    logger.warning(f"Provider {provider} timed out")
    return WebLookupResult(
        provider=provider,
        error=LookupErrorType.TIMEOUT,
        # ... minimal fields
    )</code></pre>

<h2>üìä Success Metrics</h2>

<h3>MVP Success (Fase 0-1.5)</h3>
<ul>
<li>[ ] Contract defined and approved</li>
<li>[ ] Wikipedia lookup returns WebLookupResult</li>
<li>[ ] Results stored in metadata.sources</li>
<li>[ ] UI shows at least 1 source</li>
<li>[ ] Export contains sources</li>
<li>[ ] Zero database changes</li>
</ul>

<h3>Full Success (All Phases)</h3>
<ul>
<li>[ ] All 7 providers working</li>
<li>[ ] Cache hit rate >60%</li>
<li>[ ] P95 latency <500ms (cached)</li>
<li>[ ] Sources shown in UI for >80% definitions</li>
<li>[ ] Duplicate detection working</li>
<li>[ ] No encoding issues</li>
</ul>

<h2>üö® Common Pitfalls to Avoid</h2>

<ol>
<li>**Don't start with database** - Use metadata first</li>
<li>**Don't implement all providers at once** - Wikipedia first</li>
<li>**Don't forget determinism** - Same input = same output</li>
<li>**Don't skip sanitization** - XSS risk is real</li>
<li>**Don't forget legacy patterns** - Preserve juridische regex</li>
<li>**Don't break existing code** - Keep interfaces intact</li>
</ol>

<h2>üìû Resources & Support</h2>

<h3>Key Files to Review</h3>
<ul>
<li>`src/services/modern_web_lookup_service.py` - Current implementation</li>
<li>`src/web_lookup/lookup.py` - Legacy with 7 providers</li>
<li>`src/hybrid_context/hybrid_context_engine.py` - How context is used</li>
<li>`docs/backlog/stories/epic-3-web-lookup-modernization.md` - Full epic</li>
</ul>

<h3>Test Commands</h3>
<pre><code># Test Wikipedia adapter
python -m pytest tests/services/web_lookup/test_wikipedia_adapter.py -v

# Test with DEV_MODE
DEV_MODE=true WEB_LOOKUP_ENABLED=true streamlit run src/main.py

# Check metadata storage
sqlite3 data/definities.db "SELECT metadata FROM definities WHERE id=X"</code></pre>

<h3>Quick Validation Checklist</h3>

<p><strong>After Fase 0:</strong></p>
<ul>
<li>[ ] Contract file exists</li>
<li>[ ] Config file valid YAML</li>
<li>[ ] Types importable</li>
</ul>

<p><strong>After Fase 1:</strong></p>
<ul>
<li>[ ] Wikipedia returns results</li>
<li>[ ] Cache reduces API calls</li>
<li>[ ] Sanitization removes HTML</li>
</ul>

<p><strong>After Fase 1.5:</strong></p>
<ul>
<li>[ ] Sources in metadata</li>
<li>[ ] UI shows sources</li>
<li>[ ] Export has sources</li>
</ul>

<p><strong>After Fase 2:</strong></p>
<ul>
<li>[ ] DB table created</li>
<li>[ ] Data migrated</li>
<li>[ ] Queries work</li>
</ul>

<h2>üí° Tips for Success</h2>

<ol>
<li>**Use the contract strictly** - Don't deviate from WebLookupResult</li>
<li>**Test offline first** - Mock all external calls</li>
<li>**Log everything** - Especially provider errors</li>
<li>**Cache aggressively** - APIs are slow</li>
<li>**Keep legacy tests** - They document business rules</li>
<li>**Ask questions early** - Contract ambiguities = bugs later</li>
</ol>

<h2>üéØ Day 1 Checklist</h2>

<p>Morning:</p>
<ul>
<li>[ ] Read this document completely</li>
<li>[ ] Review Episch Verhaal 3 document</li>
<li>[ ] Check out feature branch</li>
<li>[ ] Create contract file</li>
<li>[ ] Get contract reviewed</li>
</ul>

<p>Afternoon:</p>
<ul>
<li>[ ] Implement WebLookupResult class</li>
<li>[ ] Create Wikipedia adapter skeleton</li>
<li>[ ] Write first unit test</li>
<li>[ ] Implement normalize_to_result()</li>
<li>[ ] Test with real Wikipedia API</li>
</ul>

<p>End of Day:</p>
<ul>
<li>[ ] Wikipedia returns WebLookupResult</li>
<li>[ ] Basic caching works</li>
<li>[ ] Unit tests passing</li>
<li>[ ] Commit and push</li>
</ul>

<p>---</p>

<p><em>Generated: 09-01-2025</em></p>
<p><em>For: Next Developer</em></p>
<p><em>By: Development Team</em></p>
<p><em>Questions: Check Episch Verhaal 3 document or ask in team chat</em></p>

<p><strong>Remember: MVP in 2 dagen is achievable if you follow this plan!</strong> üöÄ</p>

  </div>
</body>
</html>