<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>DEF-151: Generation Prompt Storage - Implementation Specification</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>DEF-151: Generation Prompt Storage - Implementation Specification</h1>
<p><strong>Version:</strong> 1.0</p>
<p><strong>Date:</strong> 2025-11-11</p>
<p><strong>Author:</strong> BMad Master (Claude Code)</p>
<p><strong>Status:</strong> READY FOR IMPLEMENTATION</p>
<p><strong>Linear Issue:</strong> https://linear.app/definitie-app/issue/DEF-151</p>

<p>---</p>

<h2>üìã TABLE OF CONTENTS</h2>

<ol>
<li>[Executive Summary](#executive-summary)</li>
<li>[Architecture Overview](#architecture-overview)</li>
<li>[Database Design](#database-design)</li>
<li>[Data Model](#data-model)</li>
<li>[Service Layer Changes](#service-layer-changes)</li>
<li>[Repository Layer](#repository-layer)</li>
<li>[Integration Points](#integration-points)</li>
<li>[Migration Strategy](#migration-strategy)</li>
<li>[Testing Strategy](#testing-strategy)</li>
<li>[Performance Considerations](#performance-considerations)</li>
<li>[Security & Privacy](#security--privacy)</li>
<li>[Rollout Plan](#rollout-plan)</li>
</ol>

<p>---</p>

<h2>üéØ EXECUTIVE SUMMARY</h2>

<h3>Problem</h3>
<p>90 gegenereerde definities hebben GEEN audit trail van gebruikte prompts, wat leidt tot:</p>
<ul>
<li>üö´ Geen reproducibility (kan definitie niet opnieuw genereren)</li>
<li>üö´ Geen debugging capability (waarom deze output?)</li>
<li>üö´ Geen A/B testing (kan prompts niet vergelijken)</li>
<li>üö´ Compliance risk (geen GDPR-compliant audit trail)</li>
</ul>

<h3>Solution</h3>
<p>Implementeer <code>generation_logs</code> tabel die voor elke gegenereerde definitie opslaat:</p>
<ul>
<li>Complete prompt tekst (10KB+)</li>
<li>Model parameters (temperature, max_tokens, etc.)</li>
<li>Token usage & duration</li>
<li>Response metadata</li>
</ul>

<h3>Success Criteria</h3>
<ul>
<li>‚úÖ Alle nieuwe definities hebben generation log</li>
<li>‚úÖ <100ms overhead per generatie</li>
<li>‚úÖ >95% test coverage</li>
<li>‚úÖ Zero data loss (ACID compliance)</li>
</ul>

<p>---</p>

<h2>üèóÔ∏è ARCHITECTURE OVERVIEW</h2>

<h3>Current Flow (WITHOUT Logging)</h3>
<pre><code>User Input
    ‚Üì
UnifiedDefinitionGenerator.generate()
    ‚Üì
PromptOrchestrator.build_prompt()
    ‚Üì
AIServiceV2.generate()
    ‚Üì
DefinitionRepository.save()
    ‚Üì
Database (definities table)</code></pre>

<h3>New Flow (WITH Logging)</h3>
<pre><code>User Input
    ‚Üì
UnifiedDefinitionGenerator.generate()
    ‚Üì
PromptOrchestrator.build_prompt()
    ‚Üì [CAPTURE: prompt_full_text + params]
    ‚Üì
GenerationLogRepository.create_pending_log()  ‚Üê NEW
    ‚Üì
AIServiceV2.generate()
    ‚Üì [CAPTURE: tokens + duration + response_id]
    ‚Üì
GenerationLogRepository.update_with_response()  ‚Üê NEW
    ‚Üì
DefinitionRepository.save()
    ‚Üì
Database (definities + generation_logs tables)</code></pre>

<h3>Key Design Decisions</h3>

<p><strong>Decision 1: Separate Table (Not Columns)</strong></p>
<ul>
<li>‚úÖ CHOSEN: `generation_logs` table (normalized)</li>
<li>‚ùå REJECTED: Add columns to `definities` (denormalized)</li>
<li>**Rationale:** Future-proof, supports multiple generation attempts, keeps main table lean</li>
</ul>

<p><strong>Decision 2: Log BEFORE API Call</strong></p>
<ul>
<li>‚úÖ CHOSEN: Create pending log ‚Üí update after response</li>
<li>‚ùå REJECTED: Create log only after successful response</li>
<li>**Rationale:** Captures failures, enables retry analysis, better debugging</li>
</ul>

<p><strong>Decision 3: Store Full Prompt (Not Template ID)</strong></p>
<ul>
<li>‚úÖ CHOSEN: Store complete prompt text (10KB+)</li>
<li>‚ùå REJECTED: Store only template ID + variables</li>
<li>**Rationale:** True reproducibility requires exact prompt, templates change over time</li>
</ul>

<p>---</p>

<h2>üóÉÔ∏è DATABASE DESIGN</h2>

<h3>Schema Definition</h3>

<p><strong>File:</strong> <code>src/database/migrations/20251111_add_generation_logs.sql</code></p>

<pre><code>-- ================================================================
-- Migration: Add generation_logs table for prompt audit trail
-- Date: 2025-11-11
-- Author: DEF-151
-- ================================================================

CREATE TABLE generation_logs (
    -- Primary Key
    id INTEGER PRIMARY KEY AUTOINCREMENT,

    -- Link to Definitie (UNIQUE: 1 log per definitie)
    definitie_id INTEGER NOT NULL UNIQUE,

    -- ============================================================
    -- PROMPT CONTEXT (captured BEFORE API call)
    -- ============================================================

    -- Full prompt text (10KB+ typical)
    prompt_full_text TEXT NOT NULL,

    -- Template versioning
    prompt_template_version VARCHAR(50), -- e.g., "2.0", "2.1-beta"
    prompt_template_name VARCHAR(100),   -- e.g., "unified_definition_v2"

    -- Prompt modules included
    prompt_modules_used TEXT,            -- JSON array: ["base", "context", "examples", ...]

    -- ============================================================
    -- MODEL PARAMETERS (from OpenAI API request)
    -- ============================================================

    model_name VARCHAR(100) NOT NULL,    -- e.g., "gpt-4-turbo-2024-04-09"
    model_temperature DECIMAL(3,2),      -- e.g., 0.70
    model_max_tokens INTEGER,            -- e.g., 2000
    model_top_p DECIMAL(3,2),            -- e.g., 1.00
    model_frequency_penalty DECIMAL(3,2), -- e.g., 0.00
    model_presence_penalty DECIMAL(3,2),  -- e.g., 0.00

    -- ============================================================
    -- GENERATION RESULTS (captured AFTER API response)
    -- ============================================================

    -- Token usage (from response.usage)
    tokens_prompt INTEGER,               -- Input tokens
    tokens_completion INTEGER,           -- Output tokens
    tokens_total INTEGER,                -- Total tokens

    -- Performance metrics
    duration_ms INTEGER,                 -- Generation time in milliseconds

    -- Response metadata (from OpenAI response)
    response_finish_reason VARCHAR(50),  -- "stop", "length", "content_filter", etc.
    response_id VARCHAR(255),            -- OpenAI response ID (e.g., "chatcmpl-...")
    response_created_at INTEGER,         -- Unix timestamp from OpenAI

    -- Generation status
    generation_status VARCHAR(20) NOT NULL DEFAULT 'pending',
    -- Values: 'pending' (before API call), 'success', 'failed', 'timeout'

    -- Error tracking (if generation failed)
    error_message TEXT,
    error_type VARCHAR(50),              -- "api_error", "timeout", "validation_error", etc.
    error_traceback TEXT,

    -- ============================================================
    -- EXTENSIBILITY
    -- ============================================================

    -- JSON field for future metadata (system info, feature flags, etc.)
    additional_metadata TEXT,

    -- ============================================================
    -- AUDIT TRAIL
    -- ============================================================

    logged_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,

    -- ============================================================
    -- CONSTRAINTS
    -- ============================================================

    FOREIGN KEY(definitie_id) REFERENCES definities(id) ON DELETE CASCADE,

    CHECK (generation_status IN ('pending', 'success', 'failed', 'timeout')),
    CHECK (model_temperature &gt;= 0.0 AND model_temperature &lt;= 2.0),
    CHECK (tokens_prompt &gt;= 0),
    CHECK (tokens_completion &gt;= 0),
    CHECK (duration_ms &gt;= 0)
);

-- ============================================================
-- INDEXES FOR PERFORMANCE
-- ============================================================

-- Primary lookup: Get log for a definitie
CREATE INDEX idx_generation_logs_definitie
    ON generation_logs(definitie_id);

-- Analytics: Query by timestamp
CREATE INDEX idx_generation_logs_timestamp
    ON generation_logs(logged_at);

-- Analytics: Query by model
CREATE INDEX idx_generation_logs_model
    ON generation_logs(model_name);

-- Analytics: Query by template version
CREATE INDEX idx_generation_logs_template
    ON generation_logs(prompt_template_version);

-- Debug: Find failed generations
CREATE INDEX idx_generation_logs_status
    ON generation_logs(generation_status);

-- ============================================================
-- TRIGGERS FOR UPDATED_AT
-- ============================================================

CREATE TRIGGER generation_logs_updated_at
AFTER UPDATE ON generation_logs
FOR EACH ROW
BEGIN
    UPDATE generation_logs
    SET updated_at = CURRENT_TIMESTAMP
    WHERE id = NEW.id;
END;

-- ============================================================
-- VIEWS FOR COMMON QUERIES
-- ============================================================

-- View: Definities with generation context
CREATE VIEW definities_with_generation AS
SELECT
    d.*,
    gl.prompt_template_version,
    gl.model_name,
    gl.tokens_total,
    gl.duration_ms,
    gl.generation_status,
    gl.logged_at as generated_at
FROM definities d
LEFT JOIN generation_logs gl ON d.id = gl.definitie_id
WHERE d.source_type = 'generated';

-- View: Failed generations for debugging
CREATE VIEW failed_generations AS
SELECT
    d.begrip,
    gl.error_message,
    gl.error_type,
    gl.logged_at,
    gl.prompt_full_text
FROM generation_logs gl
JOIN definities d ON gl.definitie_id = d.id
WHERE gl.generation_status = 'failed';

-- ============================================================
-- COMMENTS
-- ============================================================

-- Table purpose
COMMENT ON TABLE generation_logs IS
'Audit trail for AI-generated definitions. Stores complete prompt context,
model parameters, and response metadata for reproducibility and compliance.';

-- Column comments
COMMENT ON COLUMN generation_logs.prompt_full_text IS
'Complete prompt sent to GPT-4 including all modules. Typically 5KB-15KB.';

COMMENT ON COLUMN generation_logs.generation_status IS
'pending: Log created before API call
success: Definition generated successfully
failed: API error or validation failure
timeout: Request timed out';</code></pre>

<p>---</p>

<h2>üì¶ DATA MODEL</h2>

<h3>Python Models</h3>

<p><strong>File:</strong> <code>src/models/generation_context.py</code> (NEW)</p>

<pre><code>from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from datetime import datetime
from decimal import Decimal

@dataclass
class ModelParameters:
    """OpenAI model configuration."""
    name: str                           # "gpt-4-turbo-2024-04-09"
    temperature: Decimal = Decimal("0.7")
    max_tokens: int = 2000
    top_p: Decimal = Decimal("1.0")
    frequency_penalty: Decimal = Decimal("0.0")
    presence_penalty: Decimal = Decimal("0.0")

@dataclass
class PromptContext:
    """Complete prompt context before API call."""
    full_text: str                      # Complete prompt (10KB+)
    template_version: str               # "2.0"
    template_name: str                  # "unified_definition_v2"
    modules_used: List[str]             # ["base", "context", "examples"]
    model_params: ModelParameters

@dataclass
class ResponseMetadata:
    """OpenAI API response metadata."""
    tokens_prompt: int
    tokens_completion: int
    tokens_total: int
    finish_reason: str                  # "stop", "length", etc.
    response_id: str                    # "chatcmpl-..."
    created_at: int                     # Unix timestamp
    duration_ms: int

@dataclass
class ErrorContext:
    """Error information if generation failed."""
    message: str
    error_type: str                     # "api_error", "timeout", etc.
    traceback: Optional[str] = None

@dataclass
class GenerationContext:
    """
    Complete generation context for audit trail.

    Lifecycle:
    1. Created with PromptContext (status='pending')
    2. Updated with ResponseMetadata (status='success') OR
       Updated with ErrorContext (status='failed')
    """
    id: Optional[int] = None
    definitie_id: Optional[int] = None

    # Prompt context (set at creation)
    prompt: PromptContext = None

    # Response metadata (set after API call)
    response: Optional[ResponseMetadata] = None

    # Error context (set if failed)
    error: Optional[ErrorContext] = None

    # Status tracking
    status: str = "pending"  # pending, success, failed, timeout

    # Additional metadata (extensibility)
    metadata: Dict[str, Any] = field(default_factory=dict)

    # Timestamps
    logged_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)

    def mark_success(self, response: ResponseMetadata):
        """Update with successful response."""
        self.response = response
        self.status = "success"
        self.updated_at = datetime.utcnow()

    def mark_failed(self, error: ErrorContext):
        """Update with failure information."""
        self.error = error
        self.status = "failed"
        self.updated_at = datetime.utcnow()

    def to_dict(self) -&gt; dict:
        """Convert to dictionary for JSON storage."""
        return {
            'id': self.id,
            'definitie_id': self.definitie_id,
            'prompt': {
                'full_text': self.prompt.full_text,
                'template_version': self.prompt.template_version,
                'template_name': self.prompt.template_name,
                'modules_used': self.prompt.modules_used,
                'model_params': {
                    'name': self.prompt.model_params.name,
                    'temperature': float(self.prompt.model_params.temperature),
                    'max_tokens': self.prompt.model_params.max_tokens,
                    'top_p': float(self.prompt.model_params.top_p),
                    'frequency_penalty': float(self.prompt.model_params.frequency_penalty),
                    'presence_penalty': float(self.prompt.model_params.presence_penalty),
                }
            },
            'response': {
                'tokens_prompt': self.response.tokens_prompt,
                'tokens_completion': self.response.tokens_completion,
                'tokens_total': self.response.tokens_total,
                'finish_reason': self.response.finish_reason,
                'response_id': self.response.response_id,
                'created_at': self.response.created_at,
                'duration_ms': self.response.duration_ms,
            } if self.response else None,
            'error': {
                'message': self.error.message,
                'error_type': self.error.error_type,
                'traceback': self.error.traceback,
            } if self.error else None,
            'status': self.status,
            'metadata': self.metadata,
            'logged_at': self.logged_at.isoformat(),
            'updated_at': self.updated_at.isoformat(),
        }</code></pre>

<p>---</p>

<h2>üîß SERVICE LAYER CHANGES</h2>

<h3>1. UnifiedDefinitionGenerator</h3>

<p><strong>File:</strong> <code>src/services/generation/unified_definition_generator.py</code></p>

<p><strong>Changes:</strong></p>

<pre><code>from models.generation_context import (
    GenerationContext, PromptContext, ModelParameters,
    ResponseMetadata, ErrorContext
)
from repositories.generation_log_repository import GenerationLogRepository

class UnifiedDefinitionGenerator:
    def __init__(
        self,
        ai_service: AIServiceV2,
        prompt_orchestrator: PromptOrchestrator,
        generation_log_repo: GenerationLogRepository  # NEW dependency
    ):
        self.ai_service = ai_service
        self.prompt_orchestrator = prompt_orchestrator
        self.generation_log_repo = generation_log_repo  # NEW

    def generate_definition(self, input_data: dict) -&gt; dict:
        """
        Generate definition with complete audit trail.

        Returns:
            {
                'definitie': str,
                'generation_log_id': int,
                'tokens_used': int,
                'duration_ms': int
            }
        """
        # Step 1: Build complete prompt
        prompt_result = self.prompt_orchestrator.build_prompt(input_data)
        prompt_text = prompt_result['complete_prompt']
        modules_used = prompt_result['modules_used']

        # Step 2: Prepare model parameters
        model_params = ModelParameters(
            name=self.ai_service.model_name,
            temperature=Decimal(str(self.ai_service.temperature)),
            max_tokens=self.ai_service.max_tokens,
            # ... other params
        )

        # Step 3: Create generation context
        prompt_context = PromptContext(
            full_text=prompt_text,
            template_version="2.0",  # From config
            template_name="unified_definition_v2",
            modules_used=modules_used,
            model_params=model_params
        )

        generation_context = GenerationContext(
            prompt=prompt_context,
            status="pending"
        )

        # Step 4: Create pending log BEFORE API call
        log_id = self.generation_log_repo.create_pending_log(generation_context)
        generation_context.id = log_id

        # Step 5: Call GPT-4
        start_time = datetime.utcnow()

        try:
            response = self.ai_service.generate(
                prompt=prompt_text,
                temperature=float(model_params.temperature),
                max_tokens=model_params.max_tokens
            )

            # Step 6: Calculate duration
            duration_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)

            # Step 7: Create response metadata
            response_meta = ResponseMetadata(
                tokens_prompt=response.usage.prompt_tokens,
                tokens_completion=response.usage.completion_tokens,
                tokens_total=response.usage.total_tokens,
                finish_reason=response.finish_reason,
                response_id=response.id,
                created_at=response.created,
                duration_ms=duration_ms
            )

            # Step 8: Update generation log with success
            generation_context.mark_success(response_meta)
            self.generation_log_repo.update_with_response(
                log_id,
                generation_context
            )

            # Step 9: Return result
            return {
                'definitie': response.choices[0].message.content,
                'generation_log_id': log_id,
                'tokens_used': response_meta.tokens_total,
                'duration_ms': duration_ms
            }

        except Exception as e:
            # Step 10: Log failure
            error_ctx = ErrorContext(
                message=str(e),
                error_type=type(e).__name__,
                traceback=traceback.format_exc()
            )

            generation_context.mark_failed(error_ctx)
            self.generation_log_repo.update_with_error(
                log_id,
                generation_context
            )

            # Re-raise for caller to handle
            raise</code></pre>

<p>---</p>

<h2>üìÇ REPOSITORY LAYER</h2>

<h3>GenerationLogRepository</h3>

<p><strong>File:</strong> <code>src/repositories/generation_log_repository.py</code> (NEW)</p>

<pre><code>import json
from typing import Optional, List
from datetime import datetime
from decimal import Decimal

from models.generation_context import GenerationContext, ErrorContext
from database.connection import get_connection

class GenerationLogRepository:
    """Repository for generation_logs table."""

    def create_pending_log(self, context: GenerationContext) -&gt; int:
        """
        Create pending generation log BEFORE API call.

        Args:
            context: GenerationContext with PromptContext filled

        Returns:
            generation_log_id
        """
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO generation_logs (
                prompt_full_text,
                prompt_template_version,
                prompt_template_name,
                prompt_modules_used,
                model_name,
                model_temperature,
                model_max_tokens,
                model_top_p,
                model_frequency_penalty,
                model_presence_penalty,
                generation_status,
                logged_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'pending', ?)
        """, (
            context.prompt.full_text,
            context.prompt.template_version,
            context.prompt.template_name,
            json.dumps(context.prompt.modules_used),
            context.prompt.model_params.name,
            float(context.prompt.model_params.temperature),
            context.prompt.model_params.max_tokens,
            float(context.prompt.model_params.top_p),
            float(context.prompt.model_params.frequency_penalty),
            float(context.prompt.model_params.presence_penalty),
            context.logged_at.isoformat()
        ))

        log_id = cursor.lastrowid
        conn.commit()

        return log_id

    def update_with_response(
        self,
        log_id: int,
        context: GenerationContext
    ) -&gt; None:
        """
        Update generation log with successful response metadata.

        Args:
            log_id: Generation log ID
            context: GenerationContext with ResponseMetadata filled
        """
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE generation_logs SET
                tokens_prompt = ?,
                tokens_completion = ?,
                tokens_total = ?,
                duration_ms = ?,
                response_finish_reason = ?,
                response_id = ?,
                response_created_at = ?,
                generation_status = 'success',
                updated_at = ?
            WHERE id = ?
        """, (
            context.response.tokens_prompt,
            context.response.tokens_completion,
            context.response.tokens_total,
            context.response.duration_ms,
            context.response.finish_reason,
            context.response.response_id,
            context.response.created_at,
            context.updated_at.isoformat(),
            log_id
        ))

        conn.commit()

    def update_with_error(
        self,
        log_id: int,
        context: GenerationContext
    ) -&gt; None:
        """
        Update generation log with error information.

        Args:
            log_id: Generation log ID
            context: GenerationContext with ErrorContext filled
        """
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE generation_logs SET
                error_message = ?,
                error_type = ?,
                error_traceback = ?,
                generation_status = 'failed',
                updated_at = ?
            WHERE id = ?
        """, (
            context.error.message,
            context.error.error_type,
            context.error.traceback,
            context.updated_at.isoformat(),
            log_id
        ))

        conn.commit()

    def link_to_definitie(self, log_id: int, definitie_id: int) -&gt; None:
        """
        Link generation log to definitie after definitie is saved.

        Args:
            log_id: Generation log ID
            definitie_id: Definitie ID
        """
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("""
            UPDATE generation_logs
            SET definitie_id = ?
            WHERE id = ?
        """, (definitie_id, log_id))

        conn.commit()

    def get_by_definitie_id(self, definitie_id: int) -&gt; Optional[GenerationContext]:
        """
        Retrieve generation log for a definitie.

        Args:
            definitie_id: Definitie ID

        Returns:
            GenerationContext or None if not found
        """
        conn = get_connection()
        cursor = conn.cursor()

        cursor.execute("""
            SELECT * FROM generation_logs
            WHERE definitie_id = ?
        """, (definitie_id,))

        row = cursor.fetchone()

        if not row:
            return None

        # Convert row to GenerationContext
        return self._row_to_context(row)

    def get_prompt_history(
        self,
        template_version: Optional[str] = None,
        from_date: Optional[datetime] = None,
        to_date: Optional[datetime] = None,
        status: Optional[str] = None,
        limit: int = 100
    ) -&gt; List[GenerationContext]:
        """
        Query generation logs for analysis.

        Args:
            template_version: Filter by template version
            from_date: Filter by date range (start)
            to_date: Filter by date range (end)
            status: Filter by generation status
            limit: Maximum results

        Returns:
            List of GenerationContext
        """
        conn = get_connection()
        cursor = conn.cursor()

        query = "SELECT * FROM generation_logs WHERE 1=1"
        params = []

        if template_version:
            query += " AND prompt_template_version = ?"
            params.append(template_version)

        if from_date:
            query += " AND logged_at &gt;= ?"
            params.append(from_date.isoformat())

        if to_date:
            query += " AND logged_at &lt;= ?"
            params.append(to_date.isoformat())

        if status:
            query += " AND generation_status = ?"
            params.append(status)

        query += " ORDER BY logged_at DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        rows = cursor.fetchall()

        return [self._row_to_context(row) for row in rows]

    def _row_to_context(self, row: tuple) -&gt; GenerationContext:
        """Convert database row to GenerationContext."""
        # Implementation: Map row columns to GenerationContext fields
        # ... (omitted for brevity)
        pass</code></pre>

<p>---</p>

<h2>üîó INTEGRATION POINTS</h2>

<h3>1. Definition Creation Flow</h3>

<p><strong>File:</strong> <code>src/ui/tabs/creation_tab.py</code></p>

<pre><code>def on_generate_button_click():
    """User clicked 'Genereer Definitie' button."""

    # Step 1: Collect input
    input_data = collect_user_input()

    # Step 2: Generate definitie (with logging)
    result = generator.generate_definition(input_data)

    # Step 3: Save definitie to database
    definitie_id = repository.save_definitie({
        'begrip': input_data['begrip'],
        'definitie': result['definitie'],
        'source_type': 'generated',
        # ... other fields
    })

    # Step 4: Link generation log to definitie
    generation_log_repo.link_to_definitie(
        log_id=result['generation_log_id'],
        definitie_id=definitie_id
    )

    # Step 5: Show success message
    st.success(f"Definitie gegenereerd in {result['duration_ms']}ms")
    st.info(f"Tokens gebruikt: {result['tokens_used']}")</code></pre>

<p>---</p>

<h3>2. Definitie Detail View (Optional Phase 2)</h3>

<p><strong>File:</strong> <code>src/ui/components/definitie_detail.py</code></p>

<pre><code>def show_generation_details(definitie_id: int):
    """Show generation metadata in expander."""

    # Fetch generation log
    log = generation_log_repo.get_by_definitie_id(definitie_id)

    if not log:
        st.info("Geen generatie metadata beschikbaar (ge√Ømporteerde of legacy definitie)")
        return

    with st.expander("üìä Generatie Details"):
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Model", log.prompt.model_params.name)
            st.metric("Temperature", float(log.prompt.model_params.temperature))

        with col2:
            st.metric("Tokens Gebruikt", log.response.tokens_total)
            st.metric("Prompt Tokens", log.response.tokens_prompt)
            st.metric("Completion Tokens", log.response.tokens_completion)

        with col3:
            st.metric("Generatietijd", f"{log.response.duration_ms}ms")
            st.metric("Template Versie", log.prompt.template_version)

        # Show full prompt (collapsible)
        if st.checkbox("Toon Volledige Prompt"):
            st.code(log.prompt.full_text, language="markdown")</code></pre>

<p>---</p>

<h2>üöÄ MIGRATION STRATEGY</h2>

<h3>Phase 1: Database Migration (Week 1, Day 1)</h3>

<pre><code># Run migration
python src/database/migrate_database.py

# Verify schema
sqlite3 data/definities.db "PRAGMA table_info(generation_logs);"

# Check indexes
sqlite3 data/definities.db ".indexes generation_logs"</code></pre>

<h3>Phase 2: Code Deployment (Week 1, Day 2-4)</h3>

<p><strong>Step 1:</strong> Deploy repository layer</p>
<ul>
<li>Create `GenerationLogRepository`</li>
<li>Add tests</li>
</ul>

<p><strong>Step 2:</strong> Deploy service layer</p>
<ul>
<li>Update `UnifiedDefinitionGenerator`</li>
<li>Add generation context capture</li>
</ul>

<p><strong>Step 3:</strong> Deploy UI layer</p>
<ul>
<li>Update creation tab to link logs</li>
<li>Add generation details view (optional)</li>
</ul>

<h3>Phase 3: Validation (Week 1, Day 5)</h3>

<p><strong>Test in production:</strong></p>
<ul>
<li>Generate 10 test definitions</li>
<li>Verify all have generation_logs entries</li>
<li>Check prompt_full_text is complete</li>
<li>Verify token counts match</li>
</ul>

<p>---</p>

<h2>üß™ TESTING STRATEGY</h2>

<h3>Unit Tests</h3>

<p><strong>File:</strong> <code>tests/repositories/test_generation_log_repository.py</code></p>

<pre><code>def test_create_pending_log():
    """Verify pending log created with correct fields."""
    pass

def test_update_with_response():
    """Verify log updated with response metadata."""
    pass

def test_update_with_error():
    """Verify log updated with error information."""
    pass

def test_link_to_definitie():
    """Verify definitie_id linked correctly."""
    pass

def test_get_by_definitie_id():
    """Verify log retrieved by definitie_id."""
    pass

def test_get_prompt_history_filters():
    """Verify query filters work correctly."""
    pass</code></pre>

<h3>Integration Tests</h3>

<p><strong>File:</strong> <code>tests/integration/test_generation_with_logging.py</code></p>

<pre><code>def test_end_to_end_generation():
    """
    Full generation flow with logging:
    1. Generate definitie
    2. Verify definitie saved
    3. Verify generation log created
    4. Verify link exists
    5. Verify prompt is complete
    """
    pass

def test_generation_failure_logged():
    """Verify failures are logged with error details."""
    pass

def test_token_count_accuracy():
    """Verify logged token counts match actual usage."""
    pass</code></pre>

<p>---</p>

<h2>‚ö° PERFORMANCE CONSIDERATIONS</h2>

<h3>Expected Overhead</h3>

<p><strong>Per Generation:</strong></p>
<ul>
<li>2x database INSERT: ~10ms each = 20ms</li>
<li>1x database UPDATE: ~10ms</li>
<li>JSON serialization: ~5ms</li>
<li>**Total overhead: ~35ms** (<100ms target ‚úÖ)</li>
</ul>

<h3>Prompt Storage Size</h3>

<p><strong>Typical prompt:</strong></p>
<ul>
<li>Base: 2KB</li>
<li>Context: 1KB</li>
<li>Examples: 3KB</li>
<li>Rules: 4KB</li>
<li>**Total: ~10KB per log**</li>
</ul>

<p><strong>Storage calculation (1 year):</strong></p>
<ul>
<li>1,000 definitions/year √ó 10KB = 10MB/year ‚úÖ</li>
<li>Negligible storage impact</li>
</ul>

<h3>Database Size Impact</h3>

<p><strong>After 1 year:</strong></p>
<ul>
<li>`definities` table: ~1MB (existing)</li>
<li>`generation_logs` table: ~10MB (new)</li>
<li>**Total: ~11MB** (very manageable ‚úÖ)</li>
</ul>

<p>---</p>

<h2>üîí SECURITY & PRIVACY</h2>

<h3>Sensitive Data in Prompts</h3>

<p><strong>Risk:</strong> Prompts may contain PII from context fields</p>

<p><strong>Mitigation:</strong></p>
<ol>
<li>‚úÖ Same access controls as `definities` table</li>
<li>‚úÖ Cascade delete: Delete definitie ‚Üí delete log</li>
<li>‚ö†Ô∏è Consider: Redact PII from prompts before storage (Phase 2)</li>
</ol>

<h3>GDPR Compliance</h3>

<p><strong>Article 22: Right to Explanation</strong></p>
<ul>
<li>‚úÖ Generation logs provide explanation for AI decisions</li>
<li>‚úÖ User can request: "Show me prompt used for definitie X"</li>
</ul>

<p><strong>Article 17: Right to Erasure</strong></p>
<ul>
<li>‚úÖ ON DELETE CASCADE ensures log deleted with definitie</li>
</ul>

<p>---</p>

<h2>üìÖ ROLLOUT PLAN</h2>

<h3>Week 1: Implementation</h3>

<p><strong>Day 1:</strong> Database migration</p>
<ul>
<li>[ ] Create migration script</li>
<li>[ ] Test on dev database</li>
<li>[ ] Deploy to production</li>
</ul>

<p><strong>Day 2-3:</strong> Repository + Service Layer</p>
<ul>
<li>[ ] Implement `GenerationLogRepository`</li>
<li>[ ] Update `UnifiedDefinitionGenerator`</li>
<li>[ ] Write unit tests</li>
</ul>

<p><strong>Day 4:</strong> Integration</p>
<ul>
<li>[ ] Update creation tab</li>
<li>[ ] Link logs to definities</li>
<li>[ ] Write integration tests</li>
</ul>

<p><strong>Day 5:</strong> Validation</p>
<ul>
<li>[ ] Generate 10 test definitions</li>
<li>[ ] Verify logs complete</li>
<li>[ ] Performance testing</li>
</ul>

<h3>Week 2: Monitoring</h3>

<p><strong>Day 1-5:</strong> Production monitoring</p>
<ul>
<li>[ ] Check all new definitions have logs</li>
<li>[ ] Monitor database size</li>
<li>[ ] Monitor generation time overhead</li>
<li>[ ] Fix any issues</li>
</ul>

<h3>Week 3: Enhancement (Optional)</h3>

<p><strong>UI enhancements:</strong></p>
<ul>
<li>[ ] Add generation details to definitie view</li>
<li>[ ] Add admin dashboard: token usage trends</li>
<li>[ ] Add export: generation logs to CSV</li>
</ul>

<p>---</p>

<h2>‚úÖ ACCEPTANCE CRITERIA</h2>

<h3>Must Have (Week 1)</h3>
<ul>
<li>[ ] `generation_logs` table created</li>
<li>[ ] All new definitions have generation log</li>
<li>[ ] Prompt full text stored (not truncated)</li>
<li>[ ] Token usage & duration captured</li>
<li>[ ] Model parameters captured</li>
<li>[ ] Tests passing (>95% coverage)</li>
<li>[ ] Performance overhead <100ms</li>
</ul>

<h3>Should Have (Week 2)</h3>
<ul>
<li>[ ] UI shows generation metadata</li>
<li>[ ] Admin can query logs</li>
<li>[ ] Error logging works</li>
</ul>

<h3>Nice to Have (Week 3)</h3>
<ul>
<li>[ ] Export logs to CSV</li>
<li>[ ] Compare prompts (diff view)</li>
<li>[ ] Token usage dashboard</li>
</ul>

<p>---</p>

<h2>üìö APPENDIX</h2>

<h3>A. Example Generation Log</h3>

<pre><code>{
  "id": 123,
  "definitie_id": 456,
  "prompt_full_text": "# DEFINITIE GENERATIE\n\nGenereer een definitie voor...",
  "prompt_template_version": "2.0",
  "prompt_template_name": "unified_definition_v2",
  "prompt_modules_used": ["base", "context", "examples", "validation"],
  "model_name": "gpt-4-turbo-2024-04-09",
  "model_temperature": 0.7,
  "model_max_tokens": 2000,
  "tokens_prompt": 1234,
  "tokens_completion": 567,
  "tokens_total": 1801,
  "duration_ms": 2345,
  "response_finish_reason": "stop",
  "response_id": "chatcmpl-AbCdEf123456",
  "generation_status": "success",
  "logged_at": "2025-11-11T14:23:45.123Z",
  "updated_at": "2025-11-11T14:23:47.456Z"
}</code></pre>

<h3>B. SQL Queries for Analysis</h3>

<p><strong>Token usage over time:</strong></p>
<pre><code>SELECT
    DATE(logged_at) as date,
    AVG(tokens_total) as avg_tokens,
    MAX(tokens_total) as max_tokens,
    COUNT(*) as generations
FROM generation_logs
WHERE generation_status = 'success'
GROUP BY DATE(logged_at)
ORDER BY date DESC;</code></pre>

<p><strong>Most expensive prompts:</strong></p>
<pre><code>SELECT
    d.begrip,
    gl.tokens_total,
    gl.duration_ms,
    gl.logged_at
FROM generation_logs gl
JOIN definities d ON gl.definitie_id = d.id
WHERE gl.generation_status = 'success'
ORDER BY gl.tokens_total DESC
LIMIT 10;</code></pre>

<p><strong>Failure rate by template:</strong></p>
<pre><code>SELECT
    prompt_template_version,
    COUNT(*) as total,
    SUM(CASE WHEN generation_status = 'failed' THEN 1 ELSE 0 END) as failures,
    ROUND(100.0 * SUM(CASE WHEN generation_status = 'failed' THEN 1 ELSE 0 END) / COUNT(*), 2) as failure_rate
FROM generation_logs
GROUP BY prompt_template_version;</code></pre>

<p>---</p>

<h2>üèÅ CONCLUSION</h2>

<p><strong>Implementation Status:</strong> ‚úÖ READY FOR DEVELOPMENT</p>

<p><strong>Key Benefits:</strong></p>
<ul>
<li>‚úÖ Complete audit trail (GDPR compliant)</li>
<li>‚úÖ Reproducibility (exact prompts stored)</li>
<li>‚úÖ Debugging capability (see what GPT-4 received)</li>
<li>‚úÖ A/B testing framework (compare prompts)</li>
<li>‚úÖ Low overhead (<100ms per generation)</li>
</ul>

<p><strong>Next Steps:</strong></p>
<ol>
<li>Review this specification</li>
<li>Approve database schema</li>
<li>Begin Week 1 implementation</li>
<li>Deploy to production</li>
<li>Monitor for 1 week</li>
</ol>

<p>---</p>

<p><strong>Document Version:</strong> 1.0</p>
<p><strong>Last Updated:</strong> 2025-11-11</p>
<p><strong>Approved By:</strong> [PENDING REVIEW]</p>
<p><strong>Implementation Target:</strong> Week 1 (6-8 hours)</p>


  </div>
</body>
</html>