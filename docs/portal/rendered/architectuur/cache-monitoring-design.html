<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Cache Monitoring System - Architectural Design</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">â† Terug naar Portal</a>
    <p>---</p>
<p>title: Cache Monitoring System - Architectural Design</p>
<p>epic: EPIC-026</p>
<p>related_stories: [US-201, US-202, US-203]</p>
<p>status: design</p>
<p>created: 2025-10-07</p>
<p>last_verified: 2025-10-07</p>
<p>owner: development-team</p>
<p>applies_to: definitie-app@current</p>
<p>tags: [architecture, monitoring, performance, caching]</p>
<p>---</p>

<h1>Cache Monitoring System - Architectural Design</h1>

<h2>Executive Summary</h2>

<p>This document describes the architectural design for a comprehensive cache monitoring system that tracks RuleCache, ServiceContainer, and other caching mechanisms in the DefinitieAgent application. The system provides visibility into cache effectiveness, memory usage, and performance metrics with minimal overhead (<5ms).</p>

<h2>1. Current Cache Implementations</h2>

<h3>1.1 RuleCache (`src/toetsregels/rule_cache.py`)</h3>
<p><strong>Type</strong>: Custom decorator-based cache with TTL</p>
<p><strong>Backend</strong>: <code>utils.cache.cached</code> decorator (TTL: 3600s)</p>
<p><strong>Scope</strong>: Process-local, singleton pattern</p>
<p><strong>Current Metrics</strong>:</p>
<ul>
<li>Basic call counters (`get_all_calls`, `get_single_calls`)</li>
<li>Total rules cached</li>
<li>Cache directory info</li>
</ul>

<p><strong>Strengths</strong>:</p>
<ul>
<li>âœ… Already has singleton pattern</li>
<li>âœ… Basic stats tracking in place</li>
<li>âœ… Uses global cache facade</li>
</ul>

<p><strong>Gaps</strong>:</p>
<ul>
<li>âŒ No hit/miss tracking (always reports "cache hits")</li>
<li>âŒ No disk vs memory distinction</li>
<li>âŒ No timing metrics</li>
<li>âŒ No memory usage tracking</li>
</ul>

<h3>1.2 ServiceContainer (`src/utils/container_manager.py`)</h3>
<p><strong>Type</strong>: Python <code>@lru_cache(maxsize=1)</code> decorator</p>
<p><strong>Scope</strong>: Module-level singleton</p>
<p><strong>Current Metrics</strong>:</p>
<ul>
<li>Built-in `cache_info()` available: hits, misses, maxsize, currsize</li>
<li>`get_container_stats()`: service count, service names, config</li>
</ul>

<p><strong>Strengths</strong>:</p>
<ul>
<li>âœ… Uses standard library `lru_cache` with built-in metrics</li>
<li>âœ… Already has stats function</li>
<li>âœ… Initialization count tracking</li>
</ul>

<p><strong>Gaps</strong>:</p>
<ul>
<li>âŒ `cache_info()` not exposed to monitoring</li>
<li>âŒ No timing for service initialization</li>
<li>âŒ No memory usage per service</li>
</ul>

<h3>1.3 General Cache (`src/utils/cache.py`)</h3>
<p><strong>Type</strong>: Dual system</p>
<ol>
<li>**FileCache**: File-based pickle cache with metadata</li>
<li>**CacheManager**: In-memory OrderedDict with LRU + file persistence</li>
<li>**@cached decorator**: Wrapper around both</li>
</ol>

<p><strong>Current Metrics</strong>:</p>
<ul>
<li>Global stats: hits, misses, hit_rate, evictions</li>
<li>FileCache: entries, total_size_bytes, oldest/newest entry</li>
<li>CacheManager: hits, misses, hit_rate, evictions, entries</li>
</ul>

<p><strong>Strengths</strong>:</p>
<ul>
<li>âœ… Comprehensive stats already tracked</li>
<li>âœ… Thread-safe implementation</li>
<li>âœ… Both memory and disk tracking</li>
</ul>

<p><strong>Gaps</strong>:</p>
<ul>
<li>âŒ No per-function breakdown</li>
<li>âŒ No timing per operation</li>
<li>âŒ Stats are global, hard to attribute</li>
</ul>

<h2>2. Proposed Monitoring Architecture</h2>

<h3>2.1 Design Principles</h3>

<ol>
<li>**Non-invasive**: <5ms overhead per cache operation</li>
<li>**Pluggable**: Can be disabled in production via config</li>
<li>**Structured**: Metrics follow consistent schema</li>
<li>**Actionable**: Expose data for decision-making</li>
<li>**Compatible**: Works with existing cache implementations</li>
</ol>

<h3>2.2 Architecture Overview</h3>

<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cache Monitoring Layer                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   RuleCache  â”‚  â”‚  Container   â”‚  â”‚   General    â”‚      â”‚
â”‚  â”‚   Monitor    â”‚  â”‚   Monitor    â”‚  â”‚   Cache      â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚   Monitor    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                  â”‚                  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                            â”‚                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                    â”‚  Metrics       â”‚                        â”‚
â”‚                    â”‚  Aggregator    â”‚                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                            â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Logger    â”‚  â”‚   JSON File    â”‚  â”‚   API       â”‚     â”‚
â”‚  â”‚   Backend   â”‚  â”‚   Backend      â”‚  â”‚   Endpoint  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

<h3>2.3 Core Components</h3>

<h4>2.3.1 CacheMetrics (Data Model)</h4>
<pre><code>@dataclass
class CacheOperation:
    """Single cache operation metrics."""
    cache_name: str           # "RuleCache", "ServiceContainer", "FileCache"
    operation: str            # "get", "set", "delete", "clear"
    timestamp: float          # Unix timestamp
    duration_ms: float        # Operation duration
    result: str               # "hit", "miss", "store", "evict"
    key: str | None = None   # Cache key (optional, for debugging)
    size_bytes: int | None = None  # Size of cached value
    source: str | None = None # "disk", "memory", "fresh"

@dataclass
class CacheSnapshot:
    """Point-in-time cache state."""
    cache_name: str
    timestamp: float
    total_entries: int
    memory_usage_bytes: int
    hit_rate: float
    avg_operation_ms: float
    hits: int
    misses: int
    evictions: int
    custom_metrics: Dict[str, Any]  # Cache-specific metrics</code></pre>

<h4>2.3.2 CacheMonitor (Base Class)</h4>
<pre><code>class CacheMonitor:
    """Base monitoring class for all cache types."""

    def __init__(self, cache_name: str, enabled: bool = True):
        self.cache_name = cache_name
        self.enabled = enabled
        self.operations: List[CacheOperation] = []
        self._lock = threading.Lock()

    @contextmanager
    def track_operation(
        self,
        operation: str,
        key: str | None = None
    ) -&gt; Generator[Dict, None, None]:
        """Context manager to track cache operation timing."""
        if not self.enabled:
            yield {}
            return

        start = time.perf_counter()
        result_data = {}

        try:
            yield result_data
        finally:
            duration = (time.perf_counter() - start) * 1000

            with self._lock:
                self.operations.append(CacheOperation(
                    cache_name=self.cache_name,
                    operation=operation,
                    timestamp=time.time(),
                    duration_ms=duration,
                    result=result_data.get("result", "unknown"),
                    key=key,
                    size_bytes=result_data.get("size"),
                    source=result_data.get("source")
                ))

                # Keep last 10000 operations only (memory limit)
                if len(self.operations) &gt; 10000:
                    self.operations = self.operations[-5000:]

    def get_snapshot(self) -&gt; CacheSnapshot:
        """Get current cache statistics."""
        raise NotImplementedError

    def get_recent_operations(self, limit: int = 100) -&gt; List[CacheOperation]:
        """Get recent operations for debugging."""
        with self._lock:
            return self.operations[-limit:]</code></pre>

<h4>2.3.3 RuleCacheMonitor</h4>
<pre><code>class RuleCacheMonitor(CacheMonitor):
    """Monitor for RuleCache operations."""

    def __init__(self, rule_cache: RuleCache, enabled: bool = True):
        super().__init__("RuleCache", enabled)
        self.cache = rule_cache

    def get_snapshot(self) -&gt; CacheSnapshot:
        """Generate snapshot of RuleCache state."""
        stats = self.cache.get_stats()
        all_rules = self.cache.get_all_rules()

        # Calculate memory usage (approximate)
        memory_bytes = sum(
            len(json.dumps(rule, default=str).encode())
            for rule in all_rules.values()
        )

        # Calculate hit rate from operations
        recent_ops = self.get_recent_operations(limit=1000)
        hits = sum(1 for op in recent_ops if op.result == "hit")
        misses = sum(1 for op in recent_ops if op.result == "miss")
        total = hits + misses
        hit_rate = hits / total if total &gt; 0 else 0.0

        # Average operation time
        avg_time = (
            sum(op.duration_ms for op in recent_ops) / len(recent_ops)
            if recent_ops else 0.0
        )

        return CacheSnapshot(
            cache_name="RuleCache",
            timestamp=time.time(),
            total_entries=len(all_rules),
            memory_usage_bytes=memory_bytes,
            hit_rate=hit_rate,
            avg_operation_ms=avg_time,
            hits=hits,
            misses=misses,
            evictions=0,  # RuleCache doesn't evict
            custom_metrics={
                "get_all_calls": stats["get_all_calls"],
                "get_single_calls": stats["get_single_calls"],
                "cache_dir": stats["cache_dir"]
            }
        )</code></pre>

<h4>2.3.4 ContainerCacheMonitor</h4>
<pre><code>class ContainerCacheMonitor(CacheMonitor):
    """Monitor for ServiceContainer caching."""

    def __init__(self, enabled: bool = True):
        super().__init__("ServiceContainer", enabled)

    def get_snapshot(self) -&gt; CacheSnapshot:
        """Generate snapshot of container cache state."""
        # Get lru_cache stats
        cache_info = get_cached_container.cache_info()

        # Get container stats
        container_stats = get_container_stats()

        # Estimate memory usage (rough)
        service_count = container_stats.get("service_count", 0)
        memory_bytes = service_count * 50000  # ~50KB per service (estimate)

        # Calculate metrics
        total = cache_info.hits + cache_info.misses
        hit_rate = cache_info.hits / total if total &gt; 0 else 0.0

        recent_ops = self.get_recent_operations(limit=100)
        avg_time = (
            sum(op.duration_ms for op in recent_ops) / len(recent_ops)
            if recent_ops else 0.0
        )

        return CacheSnapshot(
            cache_name="ServiceContainer",
            timestamp=time.time(),
            total_entries=cache_info.currsize,
            memory_usage_bytes=memory_bytes,
            hit_rate=hit_rate,
            avg_operation_ms=avg_time,
            hits=cache_info.hits,
            misses=cache_info.misses,
            evictions=0,  # maxsize=1, no evictions
            custom_metrics={
                "maxsize": cache_info.maxsize,
                "service_count": service_count,
                "services": container_stats.get("services", []),
                "initialization_count": getattr(
                    get_cached_container(),
                    "_initialization_count",
                    0
                )
            }
        )</code></pre>

<h4>2.3.5 MetricsAggregator</h4>
<pre><code>class MetricsAggregator:
    """Central aggregator for all cache metrics."""

    def __init__(self):
        self.monitors: Dict[str, CacheMonitor] = {}
        self._enabled = True

    def register_monitor(self, monitor: CacheMonitor):
        """Register a cache monitor."""
        self.monitors[monitor.cache_name] = monitor

    def get_all_snapshots(self) -&gt; List[CacheSnapshot]:
        """Get snapshots from all monitors."""
        return [
            monitor.get_snapshot()
            for monitor in self.monitors.values()
        ]

    def get_summary(self) -&gt; Dict[str, Any]:
        """Get aggregated summary of all caches."""
        snapshots = self.get_all_snapshots()

        total_memory = sum(s.memory_usage_bytes for s in snapshots)
        total_entries = sum(s.total_entries for s in snapshots)
        avg_hit_rate = (
            sum(s.hit_rate for s in snapshots) / len(snapshots)
            if snapshots else 0.0
        )

        return {
            "timestamp": time.time(),
            "total_caches": len(snapshots),
            "total_memory_bytes": total_memory,
            "total_memory_mb": total_memory / (1024 * 1024),
            "total_entries": total_entries,
            "average_hit_rate": round(avg_hit_rate, 2),
            "caches": {s.cache_name: s for s in snapshots}
        }</code></pre>

<h2>3. Integration Points</h2>

<h3>3.1 RuleCache Integration</h3>
<p><strong>File</strong>: <code>src/toetsregels/rule_cache.py</code></p>

<pre><code># Add to RuleCache.__init__
self.monitor = None  # Will be set by monitoring system

# Modify get_all_rules()
def get_all_rules(self) -&gt; dict[str, dict[str, Any]]:
    if self.monitor and self.monitor.enabled:
        with self.monitor.track_operation("get_all", "all_rules") as result:
            cached_data = _load_all_rules_cached(str(self.regels_dir))
            result["result"] = "hit" if cached_data else "miss"
            result["size"] = len(cached_data)
            result["source"] = "cache" if cached_data else "disk"
            return cached_data
    else:
        # Original behavior
        self.stats["get_all_calls"] += 1
        return _load_all_rules_cached(str(self.regels_dir))</code></pre>

<p><strong>Impact</strong>: +2 lines per method, <1ms overhead</p>

<h3>3.2 ServiceContainer Integration</h3>
<p><strong>File</strong>: <code>src/utils/container_manager.py</code></p>

<pre><code># Add module-level monitor
_container_monitor: ContainerCacheMonitor | None = None

def get_cached_container() -&gt; ServiceContainer:
    global _container_monitor

    if _container_monitor and _container_monitor.enabled:
        with _container_monitor.track_operation("get", "singleton") as result:
            # Check if cached
            info = get_cached_container.cache_info()
            was_cached = info.currsize &gt; 0

            # Original logic here (moved to inner function)
            container = _get_cached_container_impl()

            result["result"] = "hit" if was_cached else "miss"
            result["source"] = "memory" if was_cached else "fresh"
            return container
    else:
        # Original implementation
        return _get_cached_container_impl()</code></pre>

<p><strong>Impact</strong>: +5 lines, <1ms overhead</p>

<h3>3.3 General Cache Integration</h3>
<p><strong>File</strong>: <code>src/utils/cache.py</code></p>

<p>The <code>@cached</code> decorator already tracks hits/misses globally. Enhance it:</p>

<pre><code># Add optional monitor parameter to cached decorator
def cached(
    ttl: int | None = None,
    cache_key_func: Callable | None = None,
    cache_manager: Optional["CacheManager"] = None,
    monitor: Optional[CacheMonitor] = None,  # NEW
):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_key = # ... generate key

            # Track operation if monitor enabled
            if monitor and monitor.enabled:
                with monitor.track_operation("get", cache_key) as result:
                    cached_result = backend_get(cache_key)
                    if cached_result is not None:
                        result["result"] = "hit"
                        result["source"] = "cache"
                        _stats["hits"] += 1
                        return cached_result

                    # Cache miss
                    result["result"] = "miss"
                    _stats["misses"] += 1
                    value = func(*args, **kwargs)
                    backend_set(cache_key, value, ttl)
                    return value
            else:
                # Original behavior
                # ... existing code</code></pre>

<p><strong>Impact</strong>: +3 lines, <1ms overhead</p>

<h2>4. Metrics Data Model</h2>

<h3>4.1 Core Metrics Schema</h3>

<pre><code>cache_operation:
  cache_name: string          # "RuleCache", "ServiceContainer", etc.
  operation: enum             # get, set, delete, clear
  timestamp: float            # Unix timestamp
  duration_ms: float          # Operation duration
  result: enum                # hit, miss, store, evict
  key: string?                # Optional cache key
  size_bytes: int?            # Size of value
  source: enum?               # disk, memory, fresh

cache_snapshot:
  cache_name: string
  timestamp: float
  total_entries: int
  memory_usage_bytes: int
  hit_rate: float             # 0.0 - 1.0
  avg_operation_ms: float
  hits: int
  misses: int
  evictions: int
  custom_metrics: object      # Cache-specific data</code></pre>

<h3>4.2 Metrics Collection Points</h3>

<p>| Cache | Get | Set | Delete | Clear | Snapshot |</p>
<p>|-------|-----|-----|--------|-------|----------|</p>
<p>| RuleCache | âœ… | âŒ | âŒ | âœ… | âœ… |</p>
<p>| ServiceContainer | âœ… | âŒ | âŒ | âœ… | âœ… |</p>
<p>| FileCache | âœ… | âœ… | âœ… | âœ… | âœ… |</p>
<p>| CacheManager | âœ… | âœ… | âœ… | âœ… | âœ… |</p>

<h2>5. Metrics Exposure</h2>

<h3>5.1 Logging Backend</h3>
<p><strong>File</strong>: <code>src/monitoring/cache_logger.py</code></p>

<pre><code>class CacheMetricsLogger:
    """Log cache metrics to structured logs."""

    def __init__(self, log_dir: Path = Path("logs")):
        self.log_dir = log_dir
        self.logger = logging.getLogger("cache_metrics")

        # Create rotating file handler
        handler = RotatingFileHandler(
            log_dir / "cache_metrics.log",
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        handler.setFormatter(
            logging.Formatter('%(asctime)s - %(message)s')
        )
        self.logger.addHandler(handler)

    def log_operation(self, operation: CacheOperation):
        """Log single cache operation."""
        self.logger.info(json.dumps(asdict(operation)))

    def log_snapshot(self, snapshot: CacheSnapshot):
        """Log cache snapshot."""
        self.logger.info(json.dumps(asdict(snapshot)))</code></pre>

<p><strong>Output Format</strong>:</p>
<pre><code>{"cache_name": "RuleCache", "operation": "get", "timestamp": 1696789200.123, "duration_ms": 0.45, "result": "hit", "source": "cache"}
{"cache_name": "ServiceContainer", "operation": "get", "timestamp": 1696789201.456, "duration_ms": 245.2, "result": "miss", "source": "fresh"}</code></pre>

<h3>5.2 JSON File Backend</h3>
<p><strong>File</strong>: <code>src/monitoring/cache_json_backend.py</code></p>

<pre><code>class CacheMetricsJSONBackend:
    """Persist cache metrics to JSON files."""

    def __init__(self, data_dir: Path = Path("data/metrics")):
        self.data_dir = data_dir
        data_dir.mkdir(parents=True, exist_ok=True)

    def save_snapshot(self, snapshot: CacheSnapshot):
        """Save snapshot to JSON file."""
        filename = (
            f"cache_snapshot_{snapshot.cache_name}_"
            f"{int(snapshot.timestamp)}.json"
        )

        with open(self.data_dir / filename, 'w') as f:
            json.dump(asdict(snapshot), f, indent=2)

    def get_snapshots_since(
        self,
        cache_name: str,
        since: float
    ) -&gt; List[CacheSnapshot]:
        """Load snapshots for analysis."""
        pattern = f"cache_snapshot_{cache_name}_*.json"
        snapshots = []

        for file in sorted(self.data_dir.glob(pattern)):
            timestamp = int(file.stem.split('_')[-1])
            if timestamp &gt;= since:
                with open(file) as f:
                    data = json.load(f)
                    snapshots.append(CacheSnapshot(**data))

        return snapshots</code></pre>

<h3>5.3 API Endpoint</h3>
<p><strong>File</strong>: <code>src/api/cache_metrics_api.py</code> (new)</p>

<pre><code># Simple Flask/FastAPI endpoint for metrics
@app.get("/api/metrics/cache/summary")
def get_cache_summary():
    """Get aggregated cache metrics."""
    aggregator = get_metrics_aggregator()
    return aggregator.get_summary()

@app.get("/api/metrics/cache/{cache_name}/snapshot")
def get_cache_snapshot(cache_name: str):
    """Get specific cache snapshot."""
    aggregator = get_metrics_aggregator()
    monitor = aggregator.monitors.get(cache_name)

    if not monitor:
        return {"error": "Cache not found"}, 404

    return monitor.get_snapshot()

@app.get("/api/metrics/cache/{cache_name}/operations")
def get_recent_operations(cache_name: str, limit: int = 100):
    """Get recent operations for debugging."""
    aggregator = get_metrics_aggregator()
    monitor = aggregator.monitors.get(cache_name)

    if not monitor:
        return {"error": "Cache not found"}, 404

    operations = monitor.get_recent_operations(limit)
    return [asdict(op) for op in operations]</code></pre>

<p><strong>Note</strong>: This requires adding a minimal API server. Can start with logging/JSON only.</p>

<h3>5.4 Streamlit UI Dashboard (Optional)</h3>
<p><strong>File</strong>: <code>src/ui/components/cache_metrics_dashboard.py</code></p>

<pre><code>def render_cache_dashboard():
    """Render cache metrics in Streamlit sidebar."""
    st.sidebar.markdown("### ğŸ“Š Cache Metrics")

    aggregator = get_metrics_aggregator()
    summary = aggregator.get_summary()

    # Overall stats
    st.sidebar.metric(
        "Total Memory",
        f"{summary['total_memory_mb']:.1f} MB"
    )
    st.sidebar.metric(
        "Avg Hit Rate",
        f"{summary['average_hit_rate']*100:.1f}%"
    )

    # Per-cache expander
    with st.sidebar.expander("Cache Details"):
        for cache_name, snapshot in summary['caches'].items():
            st.markdown(f"**{cache_name}**")
            cols = st.columns(2)
            cols[0].metric("Entries", snapshot.total_entries)
            cols[1].metric("Hit Rate", f"{snapshot.hit_rate*100:.0f}%")
            st.metric("Avg Time", f"{snapshot.avg_operation_ms:.2f}ms")</code></pre>

<h2>6. Configuration</h2>

<h3>6.1 Config File</h3>
<p><strong>File</strong>: <code>config/monitoring.yaml</code></p>

<pre><code>cache_monitoring:
  enabled: true

  # Backends
  backends:
    - type: logger
      enabled: true
      log_level: INFO
      log_file: logs/cache_metrics.log

    - type: json
      enabled: false  # Disabled by default (large files)
      data_dir: data/metrics
      snapshot_interval_seconds: 300  # Every 5 minutes

    - type: api
      enabled: false  # Requires API server

  # Performance
  max_operations_history: 10000  # Keep last N operations
  track_keys: false  # Don't log cache keys (privacy)

  # Caches to monitor
  caches:
    RuleCache:
      enabled: true
      track_timing: true
      track_memory: true

    ServiceContainer:
      enabled: true
      track_timing: true
      track_memory: true

    FileCache:
      enabled: false  # Too noisy

    CacheManager:
      enabled: false</code></pre>

<h3>6.2 Environment Variables</h3>
<pre><code>CACHE_MONITORING_ENABLED=true
CACHE_MONITORING_LOG_LEVEL=INFO
CACHE_MONITORING_BACKENDS=logger,json</code></pre>

<h2>7. Performance Impact</h2>

<h3>7.1 Overhead Analysis</h3>

<p>| Component | Operation | Overhead | Frequency | Total Impact |</p>
<p>|-----------|-----------|----------|-----------|--------------|</p>
<p>| Context Manager Setup | per operation | ~0.1ms | High | Low |</p>
<p>| Time Measurement | per operation | ~0.05ms | High | Low |</p>
<p>| Dict Creation | per operation | ~0.1ms | High | Low |</p>
<p>| List Append (locked) | per operation | ~0.2ms | High | Low |</p>
<p>| Snapshot Generation | per snapshot | ~5-10ms | Low (on-demand) | Negligible |</p>
<p>| <strong>Total per operation</strong> | | <strong>~0.45ms</strong> | | <strong><1ms</strong> âœ… |</p>

<h3>7.2 Memory Impact</h3>

<p>| Component | Memory | Justification |</p>
<p>|-----------|--------|---------------|</p>
<p>| CacheOperation (10K) | ~5MB | Circular buffer, auto-trim |</p>
<p>| Monitors (3 instances) | ~100KB | Minimal overhead |</p>
<p>| Aggregator | ~50KB | Single instance |</p>
<p>| <strong>Total</strong> | <strong>~5.2MB</strong> | Acceptable âœ… |</p>

<h3>7.3 Disable in Production</h3>
<p>If needed, monitoring can be fully disabled:</p>
<ul>
<li>Set `CACHE_MONITORING_ENABLED=false`</li>
<li>Overhead becomes zero (early return in context manager)</li>
<li>No memory allocated for operations list</li>
</ul>

<h2>8. Implementation Phases</h2>

<h3>Phase 1: Foundation (Week 1, 2-3 days)</h3>
<p><strong>Complexity</strong>: SIMPLE</p>
<ul>
<li>[ ] Create `src/monitoring/cache_monitoring.py` with base classes</li>
<li>[ ] Define `CacheOperation` and `CacheSnapshot` dataclasses</li>
<li>[ ] Implement `CacheMonitor` base class</li>
<li>[ ] Add configuration loading</li>
<li>[ ] Write unit tests</li>
</ul>

<p><strong>Deliverables</strong>:</p>
<ul>
<li>Base monitoring framework</li>
<li>Configuration system</li>
<li>80% test coverage</li>
</ul>

<h3>Phase 2: Integration (Week 1, 2 days)</h3>
<p><strong>Complexity</strong>: SIMPLE-MEDIUM</p>
<ul>
<li>[ ] Integrate with RuleCache</li>
<li>[ ] Integrate with ServiceContainer</li>
<li>[ ] Create `MetricsAggregator`</li>
<li>[ ] Add logging backend</li>
<li>[ ] Test end-to-end</li>
</ul>

<p><strong>Deliverables</strong>:</p>
<ul>
<li>Working monitoring for 2 main caches</li>
<li>Structured logs with metrics</li>
<li>Integration tests</li>
</ul>

<h3>Phase 3: Exposure (Week 2, 2-3 days)</h3>
<p><strong>Complexity</strong>: MEDIUM</p>
<ul>
<li>[ ] JSON persistence backend</li>
<li>[ ] Streamlit dashboard component</li>
<li>[ ] Performance benchmarking</li>
<li>[ ] Documentation</li>
</ul>

<p><strong>Deliverables</strong>:</p>
<ul>
<li>Multiple backends working</li>
<li>UI dashboard (optional)</li>
<li>Performance report</li>
<li>User documentation</li>
</ul>

<h3>Phase 4: Polish (Week 2, 1 day)</h3>
<p><strong>Complexity</strong>: SIMPLE</p>
<ul>
<li>[ ] Add remaining cache types (FileCache, CacheManager)</li>
<li>[ ] Performance optimization</li>
<li>[ ] Production testing</li>
<li>[ ] Monitoring alerts (if needed)</li>
</ul>

<p><strong>Deliverables</strong>:</p>
<ul>
<li>Complete system</li>
<li>Production-ready</li>
<li>Monitoring documentation</li>
</ul>

<p><strong>Total Estimate</strong>: 8-10 days for full implementation</p>

<h2>9. Files to Create/Modify</h2>

<h3>New Files</h3>
<pre><code>src/
  monitoring/
    __init__.py
    cache_monitoring.py          # Core monitoring classes (300 lines)
    cache_logger.py              # Logging backend (100 lines)
    cache_json_backend.py        # JSON persistence (150 lines)
    metrics_aggregator.py        # Aggregation logic (100 lines)
  api/
    cache_metrics_api.py         # Optional API endpoints (100 lines)
  ui/
    components/
      cache_metrics_dashboard.py # Optional UI (50 lines)

config/
  monitoring.yaml                # Configuration (50 lines)

tests/
  monitoring/
    test_cache_monitoring.py     # Unit tests (300 lines)
    test_integration.py          # Integration tests (200 lines)</code></pre>

<h3>Modified Files</h3>
<pre><code>src/toetsregels/rule_cache.py      # +20 lines (monitor integration)
src/utils/container_manager.py     # +30 lines (monitor integration)
src/utils/cache.py                  # +10 lines (monitor parameter)
src/services/container.py           # +5 lines (monitor init)</code></pre>

<p><strong>Total</strong>: ~1400 lines of new code, ~65 lines of modifications</p>

<h2>10. Testing Strategy</h2>

<h3>10.1 Unit Tests</h3>
<pre><code>def test_cache_monitor_tracks_operations():
    """Test that operations are tracked correctly."""
    monitor = CacheMonitor("TestCache")

    with monitor.track_operation("get", "key1") as result:
        result["result"] = "hit"
        result["size"] = 1024

    ops = monitor.get_recent_operations(limit=1)
    assert len(ops) == 1
    assert ops[0].operation == "get"
    assert ops[0].result == "hit"
    assert ops[0].duration_ms &gt; 0

def test_monitor_respects_disabled_flag():
    """Test that disabled monitor has zero overhead."""
    monitor = CacheMonitor("TestCache", enabled=False)

    with monitor.track_operation("get", "key1") as result:
        result["result"] = "hit"

    # Should not track when disabled
    assert len(monitor.operations) == 0</code></pre>

<h3>10.2 Integration Tests</h3>
<pre><code>def test_rule_cache_monitoring_end_to_end():
    """Test RuleCache monitoring integration."""
    # Setup
    cache = get_rule_cache()
    monitor = RuleCacheMonitor(cache, enabled=True)
    cache.monitor = monitor

    # Exercise
    rules = cache.get_all_rules()
    rule = cache.get_rule("CON-01")

    # Verify
    snapshot = monitor.get_snapshot()
    assert snapshot.total_entries &gt; 0
    assert snapshot.hits &gt;= 0

    ops = monitor.get_recent_operations(limit=10)
    assert len(ops) &gt;= 2  # At least get_all and get_rule</code></pre>

<h3>10.3 Performance Tests</h3>
<pre><code>def test_monitoring_overhead_under_5ms():
    """Verify monitoring overhead is &lt;5ms per operation."""
    monitor = CacheMonitor("TestCache", enabled=True)

    # Measure without monitoring
    start = time.perf_counter()
    for _ in range(1000):
        cache.get("test_key")
    baseline = time.perf_counter() - start

    # Measure with monitoring
    start = time.perf_counter()
    for _ in range(1000):
        with monitor.track_operation("get", "test_key") as result:
            cache.get("test_key")
            result["result"] = "hit"
    monitored = time.perf_counter() - start

    overhead_per_op = (monitored - baseline) / 1000 * 1000  # ms
    assert overhead_per_op &lt; 5.0, f"Overhead {overhead_per_op}ms exceeds 5ms"</code></pre>

<h2>11. Success Metrics</h2>

<h3>11.1 Functional Success</h3>
<ul>
<li>âœ… Can track hit/miss rates for all caches</li>
<li>âœ… Can distinguish disk vs memory access</li>
<li>âœ… Can measure operation timing</li>
<li>âœ… Can estimate memory usage</li>
<li>âœ… Can export metrics for analysis</li>
</ul>

<h3>11.2 Performance Success</h3>
<ul>
<li>âœ… Overhead <5ms per cache operation</li>
<li>âœ… Memory footprint <10MB</li>
<li>âœ… No impact on cache effectiveness</li>
<li>âœ… Can be disabled in production with zero overhead</li>
</ul>

<h3>11.3 Usability Success</h3>
<ul>
<li>âœ… Metrics accessible via logs</li>
<li>âœ… Metrics accessible via API (optional)</li>
<li>âœ… Metrics visible in UI (optional)</li>
<li>âœ… Easy to understand and act on</li>
</ul>

<h2>12. Future Enhancements</h2>

<h3>12.1 Phase 2 Features (Post-MVP)</h3>
<ul>
<li>**Alerting**: Send alerts when hit rate drops below threshold</li>
<li>**Trends**: Track metrics over time, detect anomalies</li>
<li>**Comparison**: Compare cache effectiveness across deployments</li>
<li>**Optimization**: Suggest TTL/size adjustments based on metrics</li>
</ul>

<h3>12.2 Advanced Features</h3>
<ul>
<li>**Distributed Tracing**: Track cache operations across services</li>
<li>**Heatmaps**: Visualize cache access patterns</li>
<li>**Cost Tracking**: Estimate cost savings from caching</li>
<li>**A/B Testing**: Compare different cache configurations</li>
</ul>

<h2>13. Dependencies</h2>

<h3>Required</h3>
<ul>
<li>Python 3.11+ (already available)</li>
<li>`threading` (stdlib)</li>
<li>`time` (stdlib)</li>
<li>`dataclasses` (stdlib)</li>
<li>`json` (stdlib)</li>
</ul>

<h3>Optional</h3>
<ul>
<li>`tiktoken` (for token counting - not yet available, see US-203)</li>
<li>Flask/FastAPI (for API endpoints - not required for MVP)</li>
<li>Streamlit (already available for UI dashboard)</li>
</ul>

<h2>14. Risks & Mitigations</h2>

<p>| Risk | Impact | Probability | Mitigation |</p>
<p>|------|--------|-------------|------------|</p>
<p>| Performance overhead >5ms | HIGH | LOW | Thorough benchmarking, ability to disable |</p>
<p>| Memory leak from history | MEDIUM | MEDIUM | Circular buffer with max size |</p>
<p>| Thread safety issues | HIGH | LOW | Use threading.Lock for shared state |</p>
<p>| Integration complexity | MEDIUM | LOW | Start with 1-2 caches, expand gradually |</p>
<p>| Config complexity | LOW | LOW | Sensible defaults, simple on/off flag |</p>

<h2>15. Related Work</h2>

<h3>15.1 Existing Infrastructure</h3>
<ul>
<li>`src/utils/cache.py`: Already has stats tracking</li>
<li>`src/services/monitoring.py`: Minimal stub (can be extended)</li>
<li>`logs/`: Directory exists for log output</li>
</ul>

<h3>15.2 Related Stories</h3>
<ul>
<li>**US-201**: ServiceContainer optimization (completed)</li>
<li>**US-202**: RuleCache optimization (completed)</li>
<li>**US-203**: Prompt token optimization (open, needs monitoring)</li>
</ul>

<h2>16. Decision Log</h2>

<h3>Why NOT use external monitoring tools (Prometheus, Grafana)?</h3>
<p><strong>Decision</strong>: Build custom monitoring</p>
<p><strong>Rationale</strong>:</p>
<ul>
<li>Simple single-user application, not distributed system</li>
<li>Want tight integration with existing code</li>
<li>No operational overhead of running external services</li>
<li>Faster to implement custom solution</li>
<li>Can evolve as needs change</li>
</ul>

<h3>Why track operations in memory instead of just aggregates?</h3>
<p><strong>Decision</strong>: Keep last 10K operations in memory</p>
<p><strong>Rationale</strong>:</p>
<ul>
<li>Enables debugging of specific cache misses</li>
<li>Allows calculation of windowed metrics (last hour, etc.)</li>
<li>Memory impact is small (~5MB)</li>
<li>Can be disabled if not needed</li>
<li>Provides operational visibility</li>
</ul>

<h3>Why use context manager for timing?</h3>
<p><strong>Decision</strong>: <code>with monitor.track_operation()</code> pattern</p>
<p><strong>Rationale</strong>:</p>
<ul>
<li>Ensures timing even if exception occurs</li>
<li>Clean syntax, readable</li>
<li>Separates monitoring from business logic</li>
<li>Easy to disable (early return)</li>
<li>Standard Python pattern</li>
</ul>

<h2>17. Conclusion</h2>

<p>This architectural design provides a comprehensive, low-overhead cache monitoring system that:</p>

<ol>
<li>**Tracks all major caches**: RuleCache, ServiceContainer, FileCache, CacheManager</li>
<li>**Provides actionable metrics**: hit rates, timing, memory, source tracking</li>
<li>**Has minimal impact**: <5ms overhead, <10MB memory</li>
<li>**Is configurable**: Can disable entirely in production</li>
<li>**Is extensible**: Easy to add new caches or backends</li>
<li>**Is testable**: Clear interfaces for unit and integration tests</li>
</ol>

<p><strong>Implementation Complexity</strong>: MEDIUM (8-10 days)</p>
<p><strong>Performance Impact</strong>: <5ms per operation, <10MB memory</p>
<p><strong>Maintenance Burden</strong>: LOW (well-abstracted, simple interfaces)</p>

<p><strong>Recommendation</strong>: Proceed with Phase 1 (foundation) to prove out the design, then iterate based on real-world usage patterns.</p>

<p>---</p>

<h2>Appendix A: Example Metrics Output</h2>

<h3>Logging Output</h3>
<pre><code>2025-10-07 14:32:01 - {"cache_name": "RuleCache", "operation": "get_all", "timestamp": 1696689121.234, "duration_ms": 0.45, "result": "hit", "source": "cache"}
2025-10-07 14:32:01 - {"cache_name": "RuleCache", "operation": "get", "timestamp": 1696689121.567, "duration_ms": 0.12, "result": "hit", "key": "CON-01", "source": "cache"}
2025-10-07 14:32:05 - {"cache_name": "ServiceContainer", "operation": "get", "timestamp": 1696689125.890, "duration_ms": 245.3, "result": "miss", "source": "fresh"}</code></pre>

<h3>JSON Snapshot</h3>
<pre><code>{
  "cache_name": "RuleCache",
  "timestamp": 1696689200.0,
  "total_entries": 45,
  "memory_usage_bytes": 125000,
  "hit_rate": 0.98,
  "avg_operation_ms": 0.35,
  "hits": 245,
  "misses": 5,
  "evictions": 0,
  "custom_metrics": {
    "get_all_calls": 12,
    "get_single_calls": 238,
    "cache_dir": "/Users/chrislehnen/Projecten/Definitie-app/src/toetsregels/regels"
  }
}</code></pre>

<h3>API Response</h3>
<pre><code>{
  "timestamp": 1696689300.0,
  "total_caches": 2,
  "total_memory_bytes": 2625000,
  "total_memory_mb": 2.5,
  "total_entries": 46,
  "average_hit_rate": 0.97,
  "caches": {
    "RuleCache": { /* snapshot */ },
    "ServiceContainer": { /* snapshot */ }
  }
}</code></pre>

<h2>Appendix B: Quick Start Guide</h2>

<h3>For Developers</h3>
<pre><code># 1. Enable monitoring in config
# config/monitoring.yaml
cache_monitoring:
  enabled: true
  backends: [logger]

# 2. Initialize monitoring in main.py
from monitoring.cache_monitoring import setup_monitoring
setup_monitoring()

# 3. View metrics in logs
tail -f logs/cache_metrics.log

# 4. Get programmatic access
from monitoring.metrics_aggregator import get_metrics_aggregator
aggregator = get_metrics_aggregator()
summary = aggregator.get_summary()
print(f"Total cache memory: {summary['total_memory_mb']:.1f} MB")</code></pre>

<h3>For Operations</h3>
<pre><code># Check current cache status
python scripts/check_cache_health.py

# View metrics dashboard (if UI enabled)
streamlit run src/main.py  # Check sidebar

# Export metrics for analysis
python scripts/export_cache_metrics.py --since 2025-10-01 --output metrics.json</code></pre>

  </div>
</body>
</html>