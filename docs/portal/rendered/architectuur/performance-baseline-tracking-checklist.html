<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Performance Baseline Tracking - Implementation Checklist</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>Performance Baseline Tracking - Implementation Checklist</h1>

<p><strong>Status:</strong> Ready for Implementation</p>
<p><strong>Estimated Effort:</strong> 4-5 days (core), 7-8 days (with UI)</p>
<p><strong>Related:</strong> <a href="./performance-baseline-tracking-design.md" target="_blank">Design</a> | <a href="./performance-baseline-tracking-summary.md" target="_blank">Summary</a> | <a href="./performance-baseline-tracking-architecture.md" target="_blank">Architecture</a></p>

<p>---</p>

<h2>Pre-Implementation</h2>

<ul>
<li>[ ] **Review Design Documents**</li>
<li> - [ ] Read full design document</li>
<li> - [ ] Read architecture diagrams</li>
<li> - [ ] Understand 3-tier regression detection</li>
<li> - [ ] Understand baseline calculation algorithm</li>
</ul>

<ul>
<li>[ ] **Create User Story**</li>
<li> - [ ] Add US-203 to backlog</li>
<li> - [ ] Link to design documents</li>
<li> - [ ] Estimate story points (13 for core, 21 with UI)</li>
</ul>

<ul>
<li>[ ] **Branch Setup**</li>
<li> - [ ] Create feature branch: `git checkout -b feature/US-203-performance-tracking`</li>
<li> - [ ] Verify branch tracks remote: `git push -u origin feature/US-203-performance-tracking`</li>
</ul>

<ul>
<li>[ ] **Backup Critical Files**</li>
<pre><code>  mkdir -p backups/US-203
  cp src/database/schema.sql backups/US-203/
  cp data/definities.db backups/US-203/
  cp src/main.py backups/US-203/</code></pre>
</ul>

<p>---</p>

<h2>Phase 1: Foundation (4 hours) ‚úÖ SIMPLE</h2>

<h3>1.1 Database Schema</h3>

<ul>
<li>[ ] **Create Migration Script**</li>
<li> - [ ] Create file: `src/database/migrations/010_add_performance_tracking.sql`</li>
<li> - [ ] Add `performance_baselines` table (see design doc section 2.1)</li>
<li> - [ ] Add `performance_alerts` table</li>
<li> - [ ] Add `performance_targets` table</li>
<li> - [ ] Add indexes for common queries</li>
<li> - [ ] Seed performance targets from CLAUDE.md</li>
</ul>

<ul>
<li>[ ] **Test Migration**</li>
<pre><code>  # Backup database
  cp data/definities.db data/definities.db.backup

  # Run migration
  python src/database/migrate_database.py

  # Verify tables exist
  sqlite3 data/definities.db ".schema performance_baselines"
  sqlite3 data/definities.db "SELECT COUNT(*) FROM performance_targets"
  # Should see 8 targets

  # Rollback if needed
  cp data/definities.db.backup data/definities.db</code></pre>
</ul>

<h3>1.2 PerformanceTracker Core Class</h3>

<ul>
<li>[ ] **Create Core Module**</li>
<li> - [ ] Create file: `src/monitoring/performance_tracker.py`</li>
<li> - [ ] Implement `PerformanceTracker` class (singleton pattern)</li>
<li> - [ ] Implement `start_operation(name)` method</li>
<li> - [ ] Implement `stop_operation(name)` method</li>
<li> - [ ] Implement `record_metric(id, value)` method</li>
<li> - [ ] Implement `record_baseline_snapshot()` method</li>
<li> - [ ] Add `_generate_session_id()` helper</li>
<li> - [ ] Add `_get_git_commit()` helper</li>
<li> - [ ] Add `_insert_baseline()` helper</li>
</ul>

<ul>
<li>[ ] **Test Core Functionality**</li>
<pre><code>  # Test script: scripts/monitoring/test_tracker.py
  from monitoring.performance_tracker import PerformanceTracker

  tracker = PerformanceTracker.get_instance()
  tracker.start_operation('test_op')
  # ... simulate work ...
  duration = tracker.stop_operation('test_op')
  assert duration &gt; 0

  tracker.record_metric('test_metric', 123)
  baseline_id = tracker.record_baseline_snapshot()
  assert baseline_id &gt; 0

  print("‚úÖ PerformanceTracker works!")</code></pre>
</ul>

<h3>1.3 Basic Instrumentation</h3>

<ul>
<li>[ ] **Instrument main.py**</li>
<pre><code>  # src/main.py (add at top)
  from monitoring.performance_tracker import PerformanceTracker

  # In main() function
  def main():
      tracker = PerformanceTracker.get_instance()
      tracker.start_operation('app_startup')

      try:
          SessionStateManager.initialize_session_state()
          interface = TabbedInterface()
          interface.render()

          tracker.stop_operation('app_startup')
          tracker.record_baseline_snapshot()
      except Exception as e:
          tracker.stop_operation('app_startup', error=e)
          raise</code></pre>
</ul>

<ul>
<li>[ ] **Test End-to-End**</li>
<pre><code>  # Run app and check logs
  streamlit run src/main.py

  # Verify baseline recorded
  sqlite3 data/definities.db "SELECT * FROM performance_baselines ORDER BY timestamp DESC LIMIT 1"
  # Should show recent baseline with startup_time_ms populated

  # Check session logs
  tail logs/*.log
  # Should see "Operation app_startup completed in XXXms"</code></pre>
</ul>

<h3>1.4 CLI Status Command</h3>

<ul>
<li>[ ] **Create CLI Module**</li>
<li> - [ ] Create file: `scripts/monitoring/performance_cli.py`</li>
<li> - [ ] Implement `status` command (shows last baseline)</li>
<li> - [ ] Add argument parsing with `argparse`</li>
<li> - [ ] Make executable: `chmod +x scripts/monitoring/performance_cli.py`</li>
</ul>

<ul>
<li>[ ] **Test CLI**</li>
<pre><code>  python -m scripts.monitoring.performance_cli status

  # Expected output:
  # üìä Latest Performance Baseline
  # Timestamp: 2025-10-07 14:32:15
  # Session: sess_abc123
  # Startup Time: 387ms ‚úÖ (target: 500ms)
  # ...</code></pre>
</ul>

<h3>1.5 Phase 1 Verification</h3>

<ul>
<li>[ ] **Run Full Test Suite**</li>
<pre><code>  pytest -q tests/
  # All tests should pass</code></pre>
</ul>

<ul>
<li>[ ] **Measure Overhead**</li>
<pre><code>  import time
  start = time.perf_counter()
  tracker = PerformanceTracker.get_instance()
  tracker.start_operation('test')
  tracker.stop_operation('test')
  tracker.record_baseline_snapshot()
  overhead_ms = (time.perf_counter() - start) * 1000
  assert overhead_ms &lt; 50, f"Overhead too high: {overhead_ms}ms"
  print(f"‚úÖ Overhead: {overhead_ms:.1f}ms")</code></pre>
</ul>

<ul>
<li>[ ] **Verify Database Growth**</li>
<pre><code>  ls -lh data/definities.db
  # Should be ~1KB larger per baseline</code></pre>
</ul>

<ul>
<li>[ ] **Commit Phase 1**</li>
<pre><code>  git add src/database/migrations/010_add_performance_tracking.sql
  git add src/monitoring/performance_tracker.py
  git add src/main.py
  git add scripts/monitoring/performance_cli.py
  git commit -m "feat(US-203): Phase 1 - Basic performance tracking

  - Add database tables for baselines, alerts, targets
  - Implement PerformanceTracker singleton class
  - Instrument main.py for startup timing
  - Add CLI status command

  Overhead: &lt;50ms per app start
  Coverage: APP-001 (startup time) tracked"</code></pre>
</ul>

<p>---</p>

<h2>Phase 2: Comprehensive Metrics (2 days) üìä MEDIUM</h2>

<h3>2.1 ServiceContainer Instrumentation</h3>

<ul>
<li>[ ] **Update container.py**</li>
<pre><code>  # src/services/container.py
  from monitoring.performance_tracker import PerformanceTracker

  class ServiceContainer:
      _init_count = 0

      def __init__(self, config=None):
          ServiceContainer._init_count += 1

          tracker = PerformanceTracker.get_instance()
          tracker.record_metric('container_init_count', ServiceContainer._init_count)
          tracker.start_operation('container_init')

          try:
              # Existing init code
              self._initialize_services(config)
              tracker.stop_operation('container_init')
          except Exception as e:
              tracker.stop_operation('container_init', error=e)
              raise</code></pre>
</ul>

<ul>
<li>[ ] **Test Container Tracking**</li>
<pre><code>  # Verify container init count is recorded
  container = ServiceContainer()
  # Check database for container_init_count metric</code></pre>
</ul>

<h3>2.2 Validation Instrumentation</h3>

<ul>
<li>[ ] **Update ValidationOrchestratorV2**</li>
<pre><code>  # src/services/validation/validation_orchestrator_v2.py
  from monitoring.decorators import measure_performance

  class ValidationOrchestratorV2:
      @measure_performance('validation', 'VAL-001')
      async def validate(self, definition: str) -&gt; dict:
          # Existing validation code
          result = await self._perform_validation(definition)
          return result</code></pre>
</ul>

<ul>
<li>[ ] **Create Decorator Module**</li>
<li> - [ ] Create file: `src/monitoring/decorators.py`</li>
<li> - [ ] Implement `@measure_performance` decorator (see design appendix A)</li>
<li> - [ ] Handle both sync and async functions</li>
</ul>

<ul>
<li>[ ] **Test Validation Tracking**</li>
<pre><code>  # Run validation and verify timing recorded
  orchestrator = ValidationOrchestratorV2(...)
  await orchestrator.validate("test definition")
  # Check database for validation_time_ms</code></pre>
</ul>

<h3>2.3 Definition Generation Instrumentation</h3>

<ul>
<li>[ ] **Update DefinitionOrchestratorV2**</li>
<pre><code>  # src/services/orchestrators/definition_orchestrator_v2.py
  class DefinitionOrchestratorV2:
      @measure_performance('generation', 'GEN-001')
      async def generate_definition(self, term: str) -&gt; dict:
          result = await self._generate(term)

          # Record API metrics
          tracker = PerformanceTracker.get_instance()
          tracker.record_metric('prompt_token_count', result['token_count'])
          tracker.record_metric('api_cost_cents', result['cost'] * 100)

          return result</code></pre>
</ul>

<h3>2.4 Rule Cache Instrumentation</h3>

<ul>
<li>[ ] **Update RuleCache**</li>
<pre><code>  # src/toetsregels/rule_cache.py
  class RuleCache:
      @measure_performance('rule_loading', 'VAL-002')
      def get_cached_toetsregel_manager(self):
          result = self._get_or_load()

          # Record cache stats
          tracker = PerformanceTracker.get_instance()
          stats = self.get_stats()
          tracker.record_metric('rule_cache_hit_rate', stats['hit_rate'])

          return result</code></pre>
</ul>

<h3>2.5 Memory & Cache Integration</h3>

<ul>
<li>[ ] **Integrate with Existing Monitors**</li>
<pre><code>  # In PerformanceTracker.record_baseline_snapshot()

  # Pull from APIMonitor
  from monitoring.api_monitor import get_metrics_collector
  collector = get_metrics_collector()
  api_metrics = collector.get_realtime_metrics()
  self.record_metric('api_call_time_ms', api_metrics['avg_response_time'] * 1000)

  # Pull from RuleCache
  from toetsregels.rule_cache import RuleCache
  cache = RuleCache.get_instance()
  cache_stats = cache.get_stats()
  self.record_metric('rule_cache_hit_rate', cache_stats['hit_rate'])

  # Memory usage (already in code)
  import psutil
  process = psutil.Process()
  memory_mb = process.memory_info().rss // (1024 * 1024)
  self.record_metric('peak_memory_mb', memory_mb)</code></pre>
</ul>

<h3>2.6 Baseline Calculation Logic</h3>

<ul>
<li>[ ] **Implement BaselineRecorder**</li>
<li> - [ ] Create file: `src/monitoring/baseline_recorder.py`</li>
<li> - [ ] Implement `calculate_baseline(metric_id, window=20)` method</li>
<li> - [ ] Implement outlier removal (>3 std devs)</li>
<li> - [ ] Implement confidence calculation</li>
<li> - [ ] Return dict with median, p5, p95, std_dev, confidence</li>
</ul>

<ul>
<li>[ ] **Test Baseline Calculation**</li>
<pre><code>  # Test with synthetic data
  from monitoring.baseline_recorder import BaselineRecorder

  recorder = BaselineRecorder()
  baseline = recorder.calculate_baseline('APP-001', window=20)

  assert 'median' in baseline
  assert 'confidence' in baseline
  assert 0 &lt;= baseline['confidence'] &lt;= 1
  print(f"‚úÖ Baseline: {baseline}")</code></pre>
</ul>

<h3>2.7 Cold Start Detection</h3>

<ul>
<li>[ ] **Add Cold Start Logic**</li>
<pre><code>  # In PerformanceTracker.__init__()
  self.is_cold_start = self._detect_cold_start()

  def _detect_cold_start(self) -&gt; bool:
      """Detect if this is a cold start (first 3 runs in last hour)."""
      recent_count = self.db.execute("""
          SELECT COUNT(*) FROM performance_baselines
          WHERE timestamp &gt; datetime('now', '-1 hour')
      """).fetchone()[0]

      return recent_count &lt; 3

  # In record_baseline_snapshot()
  baseline['is_cold_start'] = self.is_cold_start</code></pre>
</ul>

<h3>2.8 Phase 2 Verification</h3>

<ul>
<li>[ ] **Verify All Metrics Tracked**</li>
<pre><code>  SELECT * FROM performance_baselines ORDER BY timestamp DESC LIMIT 1;
  -- Check all 13 core metrics are populated</code></pre>
</ul>

<ul>
<li>[ ] **Run Performance Tests**</li>
<pre><code>  pytest tests/performance/ -v
  # All performance tests should pass</code></pre>
</ul>

<ul>
<li>[ ] **Measure Baseline Confidence**</li>
<pre><code>  # After 20 runs
  baseline = recorder.calculate_baseline('APP-001')
  assert baseline['confidence'] &gt; 0.7, "Need more samples"</code></pre>
</ul>

<ul>
<li>[ ] **Commit Phase 2**</li>
<pre><code>  git add src/monitoring/decorators.py
  git add src/monitoring/baseline_recorder.py
  git add src/services/container.py
  git add src/services/validation/validation_orchestrator_v2.py
  git add src/services/orchestrators/definition_orchestrator_v2.py
  git add src/toetsregels/rule_cache.py
  git commit -m "feat(US-203): Phase 2 - Comprehensive metrics tracking

  - Add @measure_performance decorator for clean instrumentation
  - Track all 13 core metrics (APP, SVC, VAL, GEN, API, EXP, MEM)
  - Implement baseline calculation with outlier removal
  - Add cold start detection
  - Integrate with existing monitors (APIMonitor, RuleCache)

  Coverage: All core metrics tracked with high confidence baselines"</code></pre>
</ul>

<p>---</p>

<h2>Phase 3: Regression Detection (2 days) üö® MEDIUM</h2>

<h3>3.1 RegressionDetector Core</h3>

<ul>
<li>[ ] **Create Detector Module**</li>
<li> - [ ] Create file: `src/monitoring/regression_detector.py`</li>
<li> - [ ] Implement `RegressionDetector` class</li>
<li> - [ ] Implement `evaluate_metric(metric_id, current_value)` method</li>
<li> - [ ] Returns dict with severity, deviation_percent, details</li>
</ul>

<h3>3.2 Tier 1: Threshold Detection</h3>

<ul>
<li>[ ] **Implement Threshold Check**</li>
<pre><code>  # In RegressionDetector
  def check_threshold_breach(self, current, target, thresholds):
      """Compare current value against target with thresholds."""
      deviation_pct = ((current - target['value']) / target['value']) * 100

      if deviation_pct &gt; thresholds['error']:
          return 'critical' if deviation_pct &gt; thresholds['error'] * 1.5 else 'error'
      elif deviation_pct &gt; thresholds['warning']:
          return 'warning'
      else:
          return 'ok'</code></pre>
</ul>

<ul>
<li>[ ] **Test Threshold Detection**</li>
<pre><code>  detector = RegressionDetector(db)

  # Test warning threshold
  result = detector.check_threshold_breach(
      current=550,  # 10% over
      target={'value': 500},
      thresholds={'warning': 10, 'error': 20}
  )
  assert result == 'warning'

  # Test error threshold
  result = detector.check_threshold_breach(
      current=650,  # 30% over
      target={'value': 500},
      thresholds={'warning': 10, 'error': 20}
  )
  assert result == 'error'</code></pre>
</ul>

<h3>3.3 Tier 2: Statistical Detection</h3>

<ul>
<li>[ ] **Implement Statistical Check**</li>
<pre><code>  def check_regression(self, current, baseline):
      """Compare current value against baseline statistics."""
      z_score = (current - baseline['median']) / baseline['std_dev']

      if z_score &gt; 3:
          severity = 'critical'
      elif z_score &gt; 2:
          severity = 'error'
      elif z_score &gt; 1:
          severity = 'warning'
      else:
          severity = 'ok'

      return {
          'is_regression': z_score &gt; 1,
          'severity': severity,
          'z_score': z_score,
          'percentile': self._calculate_percentile(current, baseline)
      }</code></pre>
</ul>

<ul>
<li>[ ] **Test Statistical Detection**</li>
<pre><code>  baseline = {
      'median': 400,
      'std_dev': 20,
      'p5': 380,
      'p95': 420
  }

  # Test normal value
  result = detector.check_regression(current=405, baseline=baseline)
  assert result['severity'] == 'ok'

  # Test outlier
  result = detector.check_regression(current=460, baseline=baseline)
  assert result['severity'] in ['warning', 'error']
  assert result['z_score'] &gt; 1</code></pre>
</ul>

<h3>3.4 Tier 3: Trend Analysis</h3>

<ul>
<li>[ ] **Implement Trend Detection**</li>
<pre><code>  def detect_trend_regression(self, metric_id, window_hours=24):
      """Detect if metric is trending worse over time."""
      import numpy as np

      # Get timeseries data
      rows = self.db.execute("""
          SELECT timestamp, {metric_id} as value
          FROM performance_baselines
          WHERE timestamp &gt; datetime('now', '-{hours} hours')
          ORDER BY timestamp
      """.format(metric_id=metric_id, hours=window_hours)).fetchall()

      if len(rows) &lt; 5:
          return None  # Not enough data

      # Linear regression
      times = [(row['timestamp'] - rows[0]['timestamp']).total_seconds() / 3600
               for row in rows]
      values = [row['value'] for row in rows]

      slope, intercept, r_value = np.polyfit(times, values, 1)

      # Positive slope = getting worse (for time metrics)
      if slope &gt; 0.05 and r_value ** 2 &gt; 0.6:
          severity = 'error' if r_value ** 2 &gt; 0.8 else 'warning'
      else:
          severity = 'ok'

      return {
          'has_trend_regression': slope &gt; 0.05 and r_value ** 2 &gt; 0.6,
          'severity': severity,
          'slope': slope,
          'r_squared': r_value ** 2
      }</code></pre>
</ul>

<h3>3.5 Combined Evaluation Logic</h3>

<ul>
<li>[ ] **Implement Evaluate Metric**</li>
<pre><code>  def evaluate_metric(self, metric_id, current_value):
      """
      Combine all three tiers into final decision.
      Returns dict with severity and all tier results.
      """
      # Get target and baseline
      target = self._get_target(metric_id)
      baseline = self._calculate_baseline(metric_id)

      # Tier 1: Threshold
      threshold_result = self.check_threshold_breach(current_value, target, target)

      # Tier 2: Statistical
      regression_result = self.check_regression(current_value, baseline)

      # Tier 3: Trend
      trend_result = self.detect_trend_regression(metric_id)

      # Combine (worst severity wins)
      severities = ['ok', 'warning', 'error', 'critical']
      final_severity = max(
          threshold_result,
          regression_result['severity'],
          trend_result['severity'] if trend_result else 'ok',
          key=lambda s: severities.index(s)
      )

      return {
          'metric_id': metric_id,
          'metric_name': target['name'],
          'severity': final_severity,
          'current_value': current_value,
          'expected_value': baseline['median'],
          'target_value': target['value'],
          'deviation_percent': ((current_value - baseline['median']) / baseline['median']) * 100,
          'threshold_breached': threshold_result != 'ok',
          'details': {
              'threshold': threshold_result,
              'regression': regression_result,
              'trend': trend_result
          }
      }</code></pre>
</ul>

<h3>3.6 Alert Creation</h3>

<ul>
<li>[ ] **Integrate with PerformanceTracker**</li>
<pre><code>  # In PerformanceTracker.record_baseline_snapshot()
  def _check_regressions(self, baseline_id: int) -&gt; list:
      """Check for regressions and create alerts."""
      from monitoring.regression_detector import RegressionDetector

      detector = RegressionDetector(self.db)
      alerts = []

      for metric_id, current_value in self.metrics.items():
          if current_value is None:
              continue

          result = detector.evaluate_metric(metric_id, current_value)

          if result['severity'] != 'ok':
              alert = {
                  'baseline_id': baseline_id,
                  'alert_type': 'regression',
                  'severity': result['severity'],
                  'metric_id': metric_id,
                  'metric_name': result['metric_name'],
                  'current_value': current_value,
                  'expected_value': result['expected_value'],
                  'deviation_percent': result['deviation_percent'],
                  'threshold_breached': result['threshold_breached'],
                  'context_json': json.dumps(result['details'])
              }

              alert_id = self._insert_alert(alert)
              alerts.append(alert)

      return alerts</code></pre>
</ul>

<h3>3.7 Terminal Output</h3>

<ul>
<li>[ ] **Enhance Log Summary**</li>
<pre><code>  # In PerformanceTracker._log_summary()
  def _log_summary(self, baseline: dict, alerts: list):
      """Log performance summary with alerts to console."""
      print("\nüöÄ DefinitieAgent Startup Performance")
      print("‚îÄ" * 60)

      # Key metrics with status indicators
      startup = baseline.get('startup_time_ms')
      if startup:
          status = "‚úÖ" if startup &lt; 500 else "‚ö†Ô∏è" if startup &lt; 600 else "‚ùå"
          print(f"‚è±Ô∏è  Startup Time:        {startup:.0f}ms  {status} (target: 500ms)")

      # ... more metrics ...

      # Warnings section
      if alerts:
          print(f"\n‚ö†Ô∏è  WARNINGS:")
          for alert in alerts[:3]:  # Top 3 alerts
              emoji = {'warning': '‚ö†Ô∏è', 'error': '‚ùå', 'critical': 'üö®'}[alert['severity']]
              print(f"  {emoji} {alert['metric_name']}: {alert['current_value']:.0f} "
                    f"(expected: {alert['expected_value']:.0f}) "
                    f"{alert['severity'].upper()}")

      # Trend indicator
      trend = self._calculate_trend()
      if trend &gt; 0:
          print(f"\nüìà Trend: Performance improving (+{trend:.1f}% vs 7-day avg)")
      elif trend &lt; 0:
          print(f"\nüìâ Trend: Performance degrading ({trend:.1f}% vs 7-day avg)")

      print("‚îÄ" * 60)
      print()</code></pre>
</ul>

<h3>3.8 Structured Logging</h3>

<ul>
<li>[ ] **Add JSON Log Output**</li>
<pre><code>  # In PerformanceTracker.record_baseline_snapshot()

  import logging
  logger = logging.getLogger(__name__)

  # Log structured JSON to file
  log_entry = {
      'timestamp': datetime.now().isoformat(),
      'session_id': self.session_id,
      'git_commit': self._get_git_commit(),
      'metrics': {k: v for k, v in self.metrics.items() if v is not None},
      'alerts': [
          {
              'metric_id': a['metric_id'],
              'severity': a['severity'],
              'message': f"{a['metric_name']} {a['deviation_percent']:.1f}% above expected"
          }
          for a in alerts
      ],
      'performance_index': self.metrics.get('performance_index', 0)
  }

  logger.info(f"Performance baseline recorded: {json.dumps(log_entry)}")

  # Also write to dedicated performance log
  perf_log_path = Path('logs/performance') / f"performance_{datetime.now().strftime('%Y%m%d')}.log"
  perf_log_path.parent.mkdir(exist_ok=True)
  with open(perf_log_path, 'a') as f:
      f.write(json.dumps(log_entry) + '\n')</code></pre>
</ul>

<h3>3.9 CLI Commands Extension</h3>

<ul>
<li>[ ] **Add Report Command**</li>
<pre><code>  # In scripts/monitoring/performance_cli.py

  def command_report(args):
      """Generate detailed performance report."""
      db = get_database_connection()

      # Query baselines in date range
      baselines = db.execute("""
          SELECT * FROM performance_baselines
          WHERE timestamp &gt; datetime('now', '-{days} days')
          ORDER BY timestamp DESC
      """.format(days=args.days)).fetchall()

      # Generate report
      print(f"üìä Performance Report (last {args.days} days)")
      print("=" * 60)

      # Summary stats
      print(f"\nTotal baselines: {len(baselines)}")

      # Metrics summary
      for metric_id in ['APP-001', 'VAL-001', 'GEN-001', 'API-002']:
          values = [b[metric_id] for b in baselines if b.get(metric_id)]
          if values:
              print(f"\n{metric_id}:")
              print(f"  Min: {min(values):.1f}")
              print(f"  Median: {sorted(values)[len(values)//2]:.1f}")
              print(f"  Max: {max(values):.1f}")

      # Alerts summary
      alerts = db.execute("""
          SELECT severity, COUNT(*) as count
          FROM performance_alerts
          WHERE triggered_at &gt; datetime('now', '-{days} days')
          GROUP BY severity
      """.format(days=args.days)).fetchall()

      print(f"\nüìã Alerts:")
      for row in alerts:
          print(f"  {row['severity']}: {row['count']}")</code></pre>
</ul>

<ul>
<li>[ ] **Add Alerts Command**</li>
<pre><code>  # List unacknowledged alerts
  python -m scripts.monitoring.performance_cli alerts --unacknowledged

  # Acknowledge alert
  python -m scripts.monitoring.performance_cli alerts ack --id 42 --notes "Fixed by PR #123"</code></pre>
</ul>

<h3>3.10 Phase 3 Verification</h3>

<ul>
<li>[ ] **Test Regression Detection**</li>
<pre><code>  # Create synthetic regression
  tracker = PerformanceTracker.get_instance()
  tracker.record_metric('startup_time_ms', 700)  # Way over target
  tracker.record_baseline_snapshot()

  # Verify alert created
  alert = db.execute("""
      SELECT * FROM performance_alerts
      ORDER BY triggered_at DESC LIMIT 1
  """).fetchone()

  assert alert['severity'] in ['error', 'critical']
  assert alert['metric_id'] == 'startup_time_ms'
  print("‚úÖ Regression detection works!")</code></pre>
</ul>

<ul>
<li>[ ] **Test Terminal Output**</li>
<pre><code>  # Run app with intentional regression
  # Should see warning in terminal output
  streamlit run src/main.py
  # Expected:
  # ‚ö†Ô∏è  WARNINGS:
  #   ‚ùå Startup Time: 700ms (expected: 400ms) ERROR</code></pre>
</ul>

<ul>
<li>[ ] **Test CLI Report**</li>
<pre><code>  python -m scripts.monitoring.performance_cli report --days 7
  # Should show summary of last 7 days</code></pre>
</ul>

<ul>
<li>[ ] **Commit Phase 3**</li>
<pre><code>  git add src/monitoring/regression_detector.py
  git add src/monitoring/performance_tracker.py  # Updated
  git add scripts/monitoring/performance_cli.py  # Extended
  git commit -m "feat(US-203): Phase 3 - Regression detection and alerting

  - Implement 3-tier regression detection (threshold + statistical + trend)
  - Add alert creation and storage
  - Enhance terminal output with warnings
  - Add structured JSON logging
  - Extend CLI with report and alerts commands

  Detection: &gt;10% = warning, &gt;20% = error, &gt;1 std dev = anomaly
  Alerting: Terminal + logs + database
  Coverage: Automated regression detection for all metrics"</code></pre>
</ul>

<p>---</p>

<h2>Phase 4: UI Dashboard (3 days) üé® COMPLEX</h2>

<h3>4.1 Performance Tab Structure</h3>

<ul>
<li>[ ] **Create Performance Tab**</li>
<li> - [ ] Create file: `src/ui/tabs/performance_tab.py`</li>
<li> - [ ] Implement `PerformanceTab` class</li>
<li> - [ ] Implement `render()` method</li>
<li> - [ ] Add to `TabbedInterface` tab list</li>
</ul>

<h3>4.2 Metric Cards</h3>

<ul>
<li>[ ] **Implement Metric Cards**</li>
<pre><code>  def _render_metric_card(self, metric_id: str, name: str, unit: str):
      """Render a metric card with current value, target, and trend."""
      # Get current value
      current = self._get_latest_metric(metric_id)
      target = self._get_target(metric_id)
      baseline = self._get_baseline(metric_id)

      # Calculate trend (vs last 7 days)
      trend = self._calculate_trend(metric_id, days=7)

      # Status indicator
      status = "‚úÖ" if current &lt;= target else "‚ö†Ô∏è"

      # Render card
      st.metric(
          label=f"{status} {name}",
          value=f"{current:.0f}{unit}",
          delta=f"{trend:+.1f}%" if trend else None,
          delta_color="inverse"  # Red for increase (bad for time metrics)
      )

      # Comparison bar
      st.progress(min(1.0, current / target))
      st.caption(f"Target: {target:.0f}{unit}")</code></pre>
</ul>

<h3>4.3 Trend Charts</h3>

<ul>
<li>[ ] **Implement Trend Chart**</li>
<pre><code>  def _render_trend_chart(self, metric_id: str, time_range: str):
      """Render time-series chart for metric."""
      import plotly.graph_objects as go

      # Get data
      days = {'Last 24 Hours': 1, 'Last 7 Days': 7, 'Last 30 Days': 30}[time_range]
      data = self._get_metric_timeseries(metric_id, days=days)

      # Get baseline for reference line
      baseline = self._get_baseline(metric_id)

      # Create chart
      fig = go.Figure()

      # Actual values
      fig.add_trace(go.Scatter(
          x=[row['timestamp'] for row in data],
          y=[row['value'] for row in data],
          mode='lines+markers',
          name='Actual',
          line=dict(color='#1f77b4', width=2)
      ))

      # Baseline (median)
      fig.add_trace(go.Scatter(
          x=[data[0]['timestamp'], data[-1]['timestamp']],
          y=[baseline['median'], baseline['median']],
          mode='lines',
          name='Baseline (median)',
          line=dict(color='green', dash='dash')
      ))

      # Baseline band (p5-p95)
      fig.add_trace(go.Scatter(
          x=[row['timestamp'] for row in data],
          y=[baseline['p95']] * len(data),
          mode='lines',
          name='95th percentile',
          line=dict(color='orange', dash='dot'),
          fill=None
      ))

      fig.add_trace(go.Scatter(
          x=[row['timestamp'] for row in data],
          y=[baseline['p5']] * len(data),
          mode='lines',
          name='5th percentile',
          line=dict(color='orange', dash='dot'),
          fill='tonexty',
          fillcolor='rgba(255, 165, 0, 0.1)'
      ))

      # Layout
      fig.update_layout(
          title=f"{metric_id} Trend",
          xaxis_title="Time",
          yaxis_title="Value",
          hovermode='x unified'
      )

      st.plotly_chart(fig, use_container_width=True)</code></pre>
</ul>

<h3>4.4 Alert Table</h3>

<ul>
<li>[ ] **Implement Alert Table**</li>
<pre><code>  def _render_alert_table(self):
      """Render table of recent alerts."""
      alerts = self._get_recent_alerts(days=7, acknowledged=False)

      if not alerts:
          st.info("No recent alerts")
          return

      # Convert to DataFrame
      import pandas as pd
      df = pd.DataFrame(alerts)

      # Format columns
      df['triggered_at'] = pd.to_datetime(df['triggered_at']).dt.strftime('%Y-%m-%d %H:%M')
      df['severity'] = df['severity'].apply(lambda s: {
          'warning': '‚ö†Ô∏è Warning',
          'error': '‚ùå Error',
          'critical': 'üö® Critical'
      }[s])

      # Display table
      st.dataframe(
          df[['triggered_at', 'severity', 'metric_name', 'current_value', 'expected_value', 'deviation_percent']],
          use_container_width=True
      )

      # Acknowledge buttons
      for idx, alert in enumerate(alerts):
          col1, col2 = st.columns([3, 1])
          with col2:
              if st.button(f"Acknowledge #{alert['id']}", key=f"ack_{idx}"):
                  self._acknowledge_alert(alert['id'], notes=st.session_state.get(f'notes_{idx}', ''))
                  st.rerun()</code></pre>
</ul>

<h3>4.5 Export Functionality</h3>

<ul>
<li>[ ] **Add Export Button**</li>
<pre><code>  if st.button("üì• Export Performance Report"):
      # Generate report
      report_path = self._export_report(format='csv', days=30)

      # Provide download link
      with open(report_path, 'rb') as f:
          st.download_button(
              label="Download CSV",
              data=f,
              file_name=Path(report_path).name,
              mime='text/csv'
          )</code></pre>
</ul>

<h3>4.6 Time Range Filtering</h3>

<ul>
<li>[ ] **Add Time Range Selector**</li>
<pre><code>  time_range = st.selectbox(
      "Time Range",
      ["Last 24 Hours", "Last 7 Days", "Last 30 Days"],
      index=1  # Default to 7 days
  )

  # Use time_range in all queries
  days = {'Last 24 Hours': 1, 'Last 7 Days': 7, 'Last 30 Days': 30}[time_range]</code></pre>
</ul>

<h3>4.7 Performance Index Visualization</h3>

<ul>
<li>[ ] **Add Performance Index Gauge**</li>
<pre><code>  def _render_performance_index_gauge(self):
      """Render gauge chart for overall performance index."""
      import plotly.graph_objects as go

      current_index = self._get_latest_metric('performance_index')

      fig = go.Figure(go.Indicator(
          mode="gauge+number+delta",
          value=current_index,
          title={'text': "Performance Index"},
          delta={'reference': 80},  # Target index
          gauge={
              'axis': {'range': [None, 100]},
              'bar': {'color': "darkblue"},
              'steps': [
                  {'range': [0, 60], 'color': "lightgray"},
                  {'range': [60, 80], 'color': "yellow"},
                  {'range': [80, 100], 'color': "lightgreen"}
              ],
              'threshold': {
                  'line': {'color': "red", 'width': 4},
                  'thickness': 0.75,
                  'value': 90
              }
          }
      ))

      st.plotly_chart(fig, use_container_width=True)</code></pre>
</ul>

<h3>4.8 Phase 4 Verification</h3>

<ul>
<li>[ ] **Test UI Rendering**</li>
<pre><code>  streamlit run src/main.py
  # Navigate to Performance tab
  # Verify:
  # - Metric cards display correctly
  # - Trend charts render with data
  # - Alert table shows recent alerts
  # - Export button works</code></pre>
</ul>

<ul>
<li>[ ] **Test Interactivity**</li>
<li> - [ ] Change time range selector ‚Üí charts update</li>
<li> - [ ] Click acknowledge button ‚Üí alert disappears</li>
<li> - [ ] Export report ‚Üí CSV downloads</li>
</ul>

<ul>
<li>[ ] **Commit Phase 4**</li>
<pre><code>  git add src/ui/tabs/performance_tab.py
  git add src/ui/tabbed_interface.py  # Added performance tab
  git commit -m "feat(US-203): Phase 4 - Performance dashboard UI

  - Add Performance tab to Streamlit UI
  - Implement metric cards with current/target/trend
  - Add trend charts with baseline bands (plotly)
  - Add alert table with acknowledge functionality
  - Add export to CSV functionality
  - Add performance index gauge

  UI: Interactive dashboard for performance monitoring
  Charts: Time-series with baseline visualization
  Coverage: All metrics visible and actionable"</code></pre>
</ul>

<p>---</p>

<h2>Phase 5: CI/CD Integration (1 day) üöÄ MEDIUM</h2>

<h3>5.1 Performance Benchmark Script</h3>

<ul>
<li>[ ] **Create Benchmark Script**</li>
<li> - [ ] Create file: `scripts/monitoring/run_performance_benchmarks.py`</li>
<li> - [ ] Run all core operations (startup, validation, generation)</li>
<li> - [ ] Record metrics to JSON file</li>
<li> - [ ] Handle errors gracefully</li>
</ul>

<ul>
<li>[ ] **Test Benchmark Script**</li>
<pre><code>  python scripts/monitoring/run_performance_benchmarks.py --output results.json

  # Verify output
  cat results.json
  # Should contain all core metrics</code></pre>
</ul>

<h3>5.2 Baseline Comparison Script</h3>

<ul>
<li>[ ] **Create Comparison Script**</li>
<li> - [ ] Create file: `scripts/monitoring/compare_performance.py`</li>
<li> - [ ] Load current results (PR branch)</li>
<li> - [ ] Load baseline results (main branch)</li>
<li> - [ ] Compare each metric with threshold</li>
<li> - [ ] Output comparison JSON</li>
</ul>

<ul>
<li>[ ] **Test Comparison**</li>
<pre><code>  python scripts/monitoring/compare_performance.py \
      --current results.json \
      --baseline main_baseline.json \
      --threshold 10 \
      --output comparison.json

  cat comparison.json
  # Should show improved/regressed/unchanged metrics</code></pre>
</ul>

<h3>5.3 GitHub Actions Workflow</h3>

<ul>
<li>[ ] **Create Workflow File**</li>
<li> - [ ] Create file: `.github/workflows/performance-check.yml`</li>
<li> - [ ] Add trigger: `pull_request` on `main`</li>
<li> - [ ] Add steps:</li>
<li>   - [ ] Checkout code</li>
<li>   - [ ] Setup Python</li>
<li>   - [ ] Install dependencies</li>
<li>   - [ ] Run benchmarks on PR branch</li>
<li>   - [ ] Checkout main branch</li>
<li>   - [ ] Run benchmarks on main branch</li>
<li>   - [ ] Compare results</li>
<li>   - [ ] Post comment to PR</li>
</ul>

<ul>
<li>[ ] **Test Workflow Locally**</li>
<pre><code>  # Install act (GitHub Actions local runner)
  brew install act

  # Run workflow locally
  act pull_request</code></pre>
</ul>

<h3>5.4 PR Comment Template</h3>

<ul>
<li>[ ] **Create Comment Generator**</li>
<pre><code>  def generate_pr_comment(comparison: dict) -&gt; str:
      """Generate markdown comment for PR."""
      improved = comparison['improved']
      regressed = comparison['regressed']
      unchanged = comparison['unchanged']

      comment = f"""## üìä Performance Report

  **Branch:** `{comparison['branch']}`
  **Commit:** `{comparison['commit']}`
  **Baseline:** `main` (commit: `{comparison['baseline_commit']}`)

  ### Results

  | Metric | Current | Baseline | Change | Status |
  |--------|---------|----------|--------|--------|
  """

      for metric in comparison['all_metrics']:
          current = metric['current']
          baseline = metric['baseline']
          change = metric['change_percent']
          status = "‚úÖ" if change &lt; 0 else "‚ö†Ô∏è" if change &lt; 10 else "‚ùå"

          comment += f"| {metric['name']} | {current:.0f}{metric['unit']} | {baseline:.0f}{metric['unit']} | {change:+.1f}% | {status} |\n"

      comment += f"""
  ### Summary

  ‚úÖ **{len(improved)} metrics improved**
  ‚ö†Ô∏è **{len(regressed)} metrics regressed** (within threshold)
  ‚ùå **{len([m for m in regressed if m['change_percent'] &gt; 20])} critical regressions**

  **Overall:** {comparison['verdict']}
  """

      return comment</code></pre>
</ul>

<h3>5.5 Failure Criteria</h3>

<ul>
<li>[ ] **Define When to Fail CI**</li>
<pre><code>  def should_fail_ci(comparison: dict) -&gt; bool:
      """Determine if CI should fail based on performance regressions."""
      # Fail if any metric regressed &gt;20%
      critical_regressions = [
          m for m in comparison['regressed']
          if m['change_percent'] &gt; 20
      ]

      # Fail if performance index dropped &gt;15%
      perf_index_drop = comparison['metrics'].get('performance_index', {}).get('change_percent', 0)

      return len(critical_regressions) &gt; 0 or perf_index_drop &lt; -15</code></pre>
</ul>

<h3>5.6 Phase 5 Verification</h3>

<ul>
<li>[ ] **Test PR Workflow**</li>
</ul>
<ol>
<li>Create test PR with intentional regression</li>
<li>Verify workflow runs</li>
<li>Verify comment posted to PR</li>
<li>Verify CI fails on critical regression</li>
</ol>

<ul>
<li>[ ] **Test PR Workflow (Success)**</li>
</ul>
<ol>
<li>Create test PR with optimization</li>
<li>Verify workflow runs</li>
<li>Verify comment shows improvements</li>
<li>Verify CI passes</li>
</ol>

<ul>
<li>[ ] **Commit Phase 5**</li>
<pre><code>  git add .github/workflows/performance-check.yml
  git add scripts/monitoring/run_performance_benchmarks.py
  git add scripts/monitoring/compare_performance.py
  git commit -m "feat(US-203): Phase 5 - CI/CD performance checks

  - Add GitHub Actions workflow for PR performance checks
  - Implement benchmark script for automated testing
  - Implement comparison script with threshold checking
  - Add PR comment generation with markdown table
  - Fail CI on critical regressions (&gt;20%)

  CI/CD: Automated performance regression detection in PRs
  Threshold: Warning at 10%, fail at 20%
  Coverage: All core metrics checked on every PR"</code></pre>
</ul>

<p>---</p>

<h2>Final Verification & Cleanup</h2>

<h3>End-to-End Testing</h3>

<ul>
<li>[ ] **Test Complete Flow**</li>
</ul>
<ol>
<li>Start app ‚Üí verify baseline recorded</li>
<li>Check terminal output ‚Üí verify summary shown</li>
<li>Check database ‚Üí verify all metrics populated</li>
<li>Open Performance tab ‚Üí verify UI renders</li>
<li>Create intentional regression ‚Üí verify alert shown</li>
<li>Run CLI report ‚Üí verify data accurate</li>
<li>Export to CSV ‚Üí verify export works</li>
</ol>

<ul>
<li>[ ] **Performance Overhead Check**</li>
<pre><code>  # Measure total overhead
  import time
  start = time.perf_counter()
  # Run full app startup with tracking
  overhead = (time.perf_counter() - start) * 1000
  assert overhead &lt; 50, f"Overhead too high: {overhead}ms"
  print(f"‚úÖ Total overhead: {overhead:.1f}ms")</code></pre>
</ul>

<ul>
<li>[ ] **Database Size Check**</li>
<pre><code>  ls -lh data/definities.db
  # Should be reasonable size (a few MB after 30 days)</code></pre>
</ul>

<h3>Documentation</h3>

<ul>
<li>[ ] **Update CLAUDE.md**</li>
<li> - [ ] Add section on performance tracking</li>
<li> - [ ] Document CLI commands</li>
<li> - [ ] Document UI dashboard</li>
<li> - [ ] Link to design documents</li>
</ul>

<ul>
<li>[ ] **Update README.md**</li>
<li> - [ ] Add performance tracking section</li>
<li> - [ ] Add CLI examples</li>
<li> - [ ] Add screenshots of dashboard</li>
</ul>

<ul>
<li>[ ] **Create User Guide**</li>
<li> - [ ] Create file: `docs/guides/performance-tracking-guide.md`</li>
<li> - [ ] Explain how to use dashboard</li>
<li> - [ ] Explain how to interpret alerts</li>
<li> - [ ] Explain how to acknowledge alerts</li>
</ul>

<h3>Code Review Preparation</h3>

<ul>
<li>[ ] **Self-Review Checklist**</li>
<li> - [ ] All files have docstrings</li>
<li> - [ ] All functions have type hints</li>
<li> - [ ] All tests pass</li>
<li> - [ ] No hardcoded values (use config)</li>
<li> - [ ] No debug print statements</li>
<li> - [ ] Consistent code style (ruff + black)</li>
</ul>

<ul>
<li>[ ] **Run Code Quality Checks**</li>
<pre><code>  # Linting
  ruff check src/monitoring/ scripts/monitoring/

  # Formatting
  black --check src/monitoring/ scripts/monitoring/

  # Type checking (if using mypy)
  mypy src/monitoring/

  # Tests
  pytest tests/monitoring/ -v

  # Coverage
  pytest --cov=src/monitoring --cov-report=html</code></pre>
</ul>

<h3>Final Commit & PR</h3>

<ul>
<li>[ ] **Merge Commits into Squash Commit**</li>
<pre><code>  # Squash all phase commits
  git rebase -i main

  # Create final commit message
  git commit --amend -m "feat(US-203): Performance baseline tracking system

  Implement comprehensive performance tracking with automated
  regression detection and alerting.

  Features:
  - Track 13 core + 8 secondary metrics (21 total)
  - 3-tier regression detection (threshold + statistical + trend)
  - SQLite storage with 30-day retention
  - Streamlit dashboard with charts
  - CLI tools for querying and reporting
  - CI/CD integration with GitHub Actions

  Performance:
  - Overhead: &lt;50ms (&lt;1.25% of startup)
  - Database: ~300KB/month
  - Memory: &lt;100KB footprint

  Coverage:
  - APP-001: Application startup time
  - SVC-001/002: Container initialization
  - VAL-001/002/003: Validation metrics
  - GEN-001: Definition generation
  - API-001/002/003: API metrics
  - EXP-001: Export operations
  - MEM-001/002: Memory usage

  References:
  - Design: docs/architectuur/performance-baseline-tracking-design.md
  - Architecture: docs/architectuur/performance-baseline-tracking-architecture.md
  - User Guide: docs/guides/performance-tracking-guide.md

  Closes #203"</code></pre>
</ul>

<ul>
<li>[ ] **Create Pull Request**</li>
<pre><code>  git push origin feature/US-203-performance-tracking

  # Create PR via GitHub CLI
  gh pr create \
      --title "feat(US-203): Performance baseline tracking system" \
      --body "$(cat PR_TEMPLATE.md)" \
      --label "enhancement,performance"</code></pre>
</ul>

<ul>
<li>[ ] **PR Checklist**</li>
<li> - [ ] All tests pass</li>
<li> - [ ] Performance overhead verified (<50ms)</li>
<li> - [ ] Documentation updated</li>
<li> - [ ] Screenshots added to PR</li>
<li> - [ ] Breaking changes documented (none expected)</li>
<li> - [ ] Migration script tested</li>
</ul>

<p>---</p>

<h2>Post-Deployment</h2>

<h3>Monitoring</h3>

<ul>
<li>[ ] **Week 1: Monitor Adoption**</li>
<li> - [ ] Verify baselines being recorded (100% of runs)</li>
<li> - [ ] Check database growth (should be ~10KB/day)</li>
<li> - [ ] Monitor overhead (should be <50ms)</li>
<li> - [ ] Review initial alerts (tune thresholds if needed)</li>
</ul>

<ul>
<li>[ ] **Week 2: Gather Feedback**</li>
<li> - [ ] Survey developers on usefulness</li>
<li> - [ ] Identify false positives</li>
<li> - [ ] Adjust thresholds based on real data</li>
<li> - [ ] Fix any bugs reported</li>
</ul>

<ul>
<li>[ ] **Month 1: Evaluate Effectiveness**</li>
<li> - [ ] Count regressions caught early</li>
<li> - [ ] Measure API cost savings</li>
<li> - [ ] Assess developer satisfaction</li>
<li> - [ ] Plan improvements for Phase 6</li>
</ul>

<h3>Iteration</h3>

<ul>
<li>[ ] **Phase 6: Advanced Features (Future)**</li>
<li> - [ ] Anomaly detection (ML-based)</li>
<li> - [ ] Flame graph integration</li>
<li> - [ ] Slack/email alerting</li>
<li> - [ ] A/B test performance comparison</li>
<li> - [ ] Custom metric definitions</li>
</ul>

<p>---</p>

<h2>Success Criteria Summary</h2>

<h3>After 30 Days</h3>
<ul>
<li>[x] 100% of app starts record baselines</li>
<li>[x] 80% of core operations instrumented</li>
<li>[x] 50+ baselines per metric (high confidence)</li>
<li>[x] 10+ alerts triggered (system working)</li>
<li>[x] 5+ alerts acknowledged (team engaging)</li>
</ul>

<h3>After 60 Days</h3>
<ul>
<li>[x] 3+ regressions caught before production</li>
<li>[x] 2+ optimizations driven by data</li>
<li>[x] 20% API cost reduction</li>
<li>[x] 0 critical issues missed</li>
</ul>

<h3>Developer Experience</h3>
<ul>
<li>[x] 4+ stars: "Performance tracking is useful"</li>
<li>[x] 4+ stars: "Overhead is acceptable"</li>
<li>[x] 4+ stars: "UI dashboard is helpful"</li>
</ul>

<p>---</p>

<p><strong>Estimated Total Time:</strong> 4-5 days (core), 7-8 days (with UI)</p>
<p><strong>Risk Level:</strong> MEDIUM (manageable with phased approach)</p>
<p><strong>ROI:</strong> HIGH (catch regressions early, reduce API costs, data-driven optimization)</p>

<p><strong>Ready to start?</strong> Begin with Phase 1 (4 hours) and iterate!</p>

  </div>
</body>
</html>