<!doctype html>
<html lang="nl">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Performance Tracking Quick Reference Guide</title>
  <link rel="stylesheet" href="../portal.css" />
  <style>
    .doc { max-width: 900px; margin: 16px auto 48px auto; padding: 0 16px; }
    .doc h1,.doc h2,.doc h3,.doc h4{ color:#223 }
    .doc p{ line-height:1.6; color:#222 }
    .doc a{ color:#1d4ed8; text-decoration: underline; }
    .doc pre{ background:#0b1020; color:#e6edf3; padding:12px; overflow:auto; border-radius:8px }
    .doc code{ background:#f0f2f5; padding:2px 6px; border-radius:4px }
    .back{ display:inline-block; margin:12px 0; padding:6px 10px; border:1px solid #ccd; border-radius:6px; text-decoration:none; color:#223; background:#f8f9fb }
  </style>
</head>
<body>
  <div class="doc">
    <a class="back" href="../../index.html">‚Üê Terug naar Portal</a>
    <h1>Performance Tracking Quick Reference Guide</h1>

<h2>Overview</h2>

<p>DefinitieAgent includes automatic performance baseline tracking to detect performance regressions early. The system tracks metrics, calculates baselines, and alerts on degradations.</p>

<h2>Quick Start</h2>

<h3>View Performance Status</h3>

<pre><code>python -m src.cli.performance_cli status</code></pre>

<p>Output:</p>
<pre><code>=== Performance Status ===

üü¢ app_startup_ms
   Huidig:   220.5
   Baseline: 224.5
   Afwijking: -1.8%
   Status:   OK
   Confidence: 100% (20 samples)</code></pre>

<h3>View All Baselines</h3>

<pre><code>python -m src.cli.performance_cli baselines</code></pre>

<h3>View Metric History</h3>

<pre><code>python -m src.cli.performance_cli history app_startup_ms --limit 10</code></pre>

<h2>Understanding Alerts</h2>

<h3>Alert Levels</h3>

<p>| Level | Threshold | Meaning | Action |</p>
<p>|-------|-----------|---------|--------|</p>
<p>| üü¢ OK | <10% slower | Performance within acceptable range | No action needed |</p>
<p>| üü° WARNING | 10-20% slower | Noticeable degradation | Investigate cause |</p>
<p>| üî¥ CRITICAL | >20% slower | Significant regression | Fix immediately |</p>

<h3>Example Log Output</h3>

<pre><code># OK - no alert
INFO: Startup tijd: 220.5ms

# WARNING
WARNING: WARNING startup regressie: 258.1ms (&gt;10% slechter dan baseline)

# CRITICAL
WARNING: CRITICAL startup regressie: 280.6ms (&gt;20% slechter dan baseline)</code></pre>

<h2>How It Works</h2>

<h3>1. Automatic Tracking</h3>

<p>Every time the app starts, it automatically:</p>
<ol>
<li>Measures startup time</li>
<li>Stores the measurement in the database</li>
<li>Updates the baseline (if enough data)</li>
<li>Checks for regression vs baseline</li>
<li>Logs a warning if regression detected</li>
</ol>

<h3>2. Baseline Calculation</h3>

<p><strong>Algorithm</strong>: Median of last 20 samples</p>

<p><strong>Why median?</strong></p>
<ul>
<li>More robust against outliers</li>
<li>Performance spikes (GC, disk I/O) don't skew baseline</li>
<li>More stable than mean</li>
</ul>

<p><strong>Example</strong>:</p>
<pre><code>Samples: [200, 205, 210, 215, 220, 225, 230, 235, 240, 245, ...]
Baseline: 222.5ms (median)</code></pre>

<h3>3. Confidence Scoring</h3>

<p><strong>Formula</strong>: <code>confidence = sample_count / 20</code></p>

<p>| Samples | Confidence | Alert Threshold |</p>
<p>|---------|-----------|-----------------|</p>
<p>| 1-4 | 0-20% | No alerts (insufficient data) |</p>
<p>| 5-9 | 25-45% | No alerts (low confidence) |</p>
<p>| 10-19 | 50-95% | ‚úÖ Alerts enabled |</p>
<p>| 20+ | 100% | ‚úÖ Full confidence |</p>

<p><strong>Note</strong>: Alerts only trigger when confidence >= 50% (10+ samples)</p>

<h3>4. Sliding Window</h3>

<p>The system uses a <strong>sliding window</strong> of 20 samples:</p>
<ul>
<li>Always uses the **last 20 measurements**</li>
<li>Automatically adapts to gradual changes</li>
<li>Old measurements are archived but not used for baseline</li>
</ul>

<p><strong>Example</strong>:</p>
<pre><code>Sample 1-20:  Used for baseline (avg ~225ms)
Sample 21:    Replaces sample 1, new baseline calculated
Sample 22:    Replaces sample 2, new baseline calculated
...</code></pre>

<h2>Adding Custom Metrics</h2>

<h3>In Code</h3>

<pre><code>from src.monitoring.performance_tracker import get_tracker

# Track a metric
tracker = get_tracker()
tracker.track_metric(
    "definition_generation_ms",
    elapsed_ms,
    metadata={"model": "gpt-4", "length": 150}
)

# Check for regression
alert = tracker.check_regression("definition_generation_ms", elapsed_ms)
if alert:
    logger.warning(f"Performance alert: {alert}")</code></pre>

<h3>Best Practices</h3>

<ol>
<li>**Metric naming**: Use descriptive names with `_ms` suffix for time metrics</li>
</ol>
<ul>
<li>  - ‚úÖ `app_startup_ms`, `definition_generation_ms`</li>
<li>  - ‚ùå `time`, `perf`, `metric1`</li>
</ul>

<ol>
<li>**Metadata**: Include relevant context</li>
</ol>
<ul>
<li>  - ‚úÖ `{"model": "gpt-4", "version": "2.0"}`</li>
<li>  - ‚ùå `{}`</li>
</ul>

<ol>
<li>**Error handling**: Always wrap in try/except</li>
<pre><code>   try:
       tracker.track_metric(...)
   except Exception as e:
       logger.debug(f"Tracking error (non-critical): {e}")</code></pre>
</ol>

<ol>
<li>**Don't track**:</li>
</ol>
<ul>
<li>  - User input processing time (too variable)</li>
<li>  - Network latency (out of our control)</li>
<li>  - Cold start scenarios (first run ever)</li>
</ul>

<h2>Troubleshooting</h2>

<h3>"Geen baseline beschikbaar"</h3>

<p><strong>Cause</strong>: Fewer than 5 measurements recorded.</p>

<p><strong>Solution</strong>: Run the app 5+ times to generate baseline.</p>

<pre><code># Quick way to generate baseline
for i in {1..10}; do
    python -m streamlit run src/main.py --server.headless true &amp;
    sleep 5
    pkill -f streamlit
done</code></pre>

<h3>"Te weinig confidence"</h3>

<p><strong>Cause</strong>: Less than 10 samples, confidence < 50%.</p>

<p><strong>Solution</strong>: Run the app more times or wait for more data.</p>

<h3>"Performance tracking fout"</h3>

<p><strong>Cause</strong>: Database issue or corrupted data.</p>

<p><strong>Solution</strong>: Reset performance data:</p>
<pre><code>python -m src.cli.performance_cli reset-all</code></pre>

<p><strong>Warning</strong>: This deletes all performance history!</p>

<h3>False Alerts</h3>

<p><strong>Problem</strong>: Alert triggered but performance seems fine.</p>

<p><strong>Causes</strong>:</p>
<ol>
<li>**System load**: Other apps consuming resources</li>
<li>**Cold cache**: First run after reboot</li>
<li>**Background updates**: OS updates, backups</li>
</ol>

<p><strong>Solution</strong>:</p>
<ol>
<li>Check system load: `top` or Activity Monitor</li>
<li>Restart app a few times to warm up caches</li>
<li>If persistent, investigate code changes</li>
</ol>

<h2>Maintenance</h2>

<h3>Clean Old Data</h3>

<pre><code># Remove baseline for specific metric
python -m src.cli.performance_cli delete-baseline app_startup_ms

# Reset all data (careful!)
python -m src.cli.performance_cli reset-all</code></pre>

<h3>Database Location</h3>

<p>Performance data is stored in:</p>
<pre><code>data/definities.db
  ‚îú‚îÄ‚îÄ performance_metrics      (individual measurements)
  ‚îî‚îÄ‚îÄ performance_baselines    (calculated baselines)</code></pre>

<h3>Backup</h3>

<pre><code># Backup performance data
cp data/definities.db data/definities.db.backup

# Restore
cp data/definities.db.backup data/definities.db</code></pre>

<h2>Advanced Usage</h2>

<h3>Custom Thresholds</h3>

<p>Edit <code>src/monitoring/performance_tracker.py</code>:</p>

<pre><code>class PerformanceTracker:
    CRITICAL_THRESHOLD = 1.30  # 30% worse (more lenient)
    WARNING_THRESHOLD = 1.15   # 15% worse</code></pre>

<h3>Different Window Size</h3>

<pre><code>class PerformanceTracker:
    BASELINE_WINDOW = 50  # Use last 50 samples
    MIN_SAMPLES = 10      # Need 10 samples minimum</code></pre>

<h3>Custom Database Location</h3>

<pre><code>tracker = PerformanceTracker(db_path="/custom/path/metrics.db")</code></pre>

<h2>Metrics Currently Tracked</h2>

<p>| Metric | Description | Target | Critical |</p>
<p>|--------|-------------|--------|----------|</p>
<p>| <code>app_startup_ms</code> | App startup time | <250ms | >400ms |</p>

<p><strong>More metrics coming in Phase 2-4!</strong></p>

<h2>Performance Overhead</h2>

<p>| Operation | Overhead | Impact |</p>
<p>|-----------|----------|--------|</p>
<p>| Track metric | ~1-2ms | Negligible |</p>
<p>| Update baseline | ~3-5ms | Negligible |</p>
<p>| Check regression | ~1ms | Negligible |</p>
<p>| <strong>Total per startup</strong> | <strong>~5-10ms</strong> | <strong><4% of 250ms startup</strong> |</p>

<h2>Frequently Asked Questions</h2>

<h3>Q: How long until I get alerts?</h3>

<p><strong>A</strong>: After 10 app starts (to reach 50% confidence).</p>

<h3>Q: Can I disable performance tracking?</h3>

<p><strong>A</strong>: Not currently, but overhead is <10ms per startup. You can ignore alerts in logs.</p>

<h3>Q: What if baseline keeps changing?</h3>

<p><strong>A</strong>: This is normal if your performance is gradually improving/degrading. The sliding window adapts automatically.</p>

<h3>Q: How do I track custom metrics?</h3>

<p><strong>A</strong>: See "Adding Custom Metrics" section above.</p>

<h3>Q: Can I export performance data?</h3>

<p><strong>A</strong>: Use SQL to export from database:</p>
<pre><code>sqlite3 data/definities.db "SELECT * FROM performance_metrics" &gt; metrics.csv</code></pre>

<h3>Q: What's the difference between baseline and average?</h3>

<p><strong>A</strong>: Baseline uses <strong>median</strong> (middle value), average uses <strong>mean</strong> (sum/count). Median is more robust against outliers.</p>

<h2>Support</h2>

<p>For issues or questions:</p>
<ol>
<li>Check logs: `logs/app.log`</li>
<li>View full report: `docs/reports/performance-baseline-tracking-implementation.md`</li>
<li>Source code: `src/monitoring/performance_tracker.py`</li>
</ol>

<p>---</p>

<p><strong>Last updated</strong>: 2025-10-07</p>
<p><strong>Version</strong>: Phase 1 (Basic Tracking)</p>

  </div>
</body>
</html>