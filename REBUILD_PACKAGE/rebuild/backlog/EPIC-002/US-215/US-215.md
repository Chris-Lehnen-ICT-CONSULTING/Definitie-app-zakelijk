---
id: US-322
epic: EPIC-002
status: open
prioriteit: MEDIUM
story_points: 3
sprint: next
owner: backend
applies_to: definitie-app@current
canonical: false
last_verified: 2025-10-02
created_at: 2025-09-19
dependencies: [US-193, US-213]
---

# US-322 — Herstel Debug Informatie Verlies in Refactored Code

## Doel
Herstel verloren gegane debug informatie en logging na refactoring, zodat ontwikkelaars effectief kunnen troubleshooten en de applicatie monitoren in productie.

## Context
Uit de US-193 multi-agent review (P3-14) blijkt dat tijdens refactoring belangrijke debug informatie is verwijderd of verminderd. Dit maakt troubleshooting moeilijk en vermindert observability.

## Scope

### In scope
- Herstel alle verwijderde debug logging statements
- Implementeer structured logging met context
- Voeg correlation IDs toe voor request tracing
- Creëer debug mode flags voor verbose output
- Implementeer performance timing logs

### Niet in scope
- APM tooling integratie (aparte US)
- Log aggregatie infrastructuur
- Alerting en monitoring dashboards

## Acceptance Criteria

### Functioneel
- [ ] Debug logs beschikbaar op alle kritieke punten
- [ ] Correlation ID tracking door hele request flow
- [ ] Performance metrics gelogd voor slow operations
- [ ] Debug mode toggle via environment variable
- [ ] Structured JSON logging voor machine parsing

### Technisch
- [ ] Logger configuratie in `src/utils/logging_config.py`
- [ ] Debug decorators voor method tracing
- [ ] Context managers voor operation timing
- [ ] Log levels correct gebruikt (DEBUG/INFO/WARNING/ERROR)
- [ ] Sensitive data gemaskeerd in logs

## Implementatie

### 1. Enhanced Logging Configuration
```python
# src/utils/logging_config.py
import logging
import json
from typing import Any, Dict
from contextvars import ContextVar

correlation_id: ContextVar[str] = ContextVar('correlation_id', default='')

class StructuredFormatter(logging.Formatter):
    """JSON structured logging formatter."""

    def format(self, record: logging.LogRecord) -> str:
        log_obj = {
            'timestamp': self.formatTime(record),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'correlation_id': correlation_id.get(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno,
        }

        # Add extra fields
        for key, value in record.__dict__.items():
            if key not in ['name', 'msg', 'args', 'created', 'filename',
                          'funcName', 'levelname', 'levelno', 'lineno',
                          'module', 'msecs', 'message', 'pathname', 'process',
                          'processName', 'relativeCreated', 'thread', 'threadName']:
                log_obj[key] = value

        return json.dumps(log_obj)

def setup_logging(debug: bool = False) -> None:
    """Configure application-wide logging."""
    level = logging.DEBUG if debug else logging.INFO
    handler = logging.StreamHandler()
    handler.setFormatter(StructuredFormatter())

    logging.basicConfig(
        level=level,
        handlers=[handler]
    )
```

### 2. Debug Decorators
```python
# src/utils/debug_helpers.py
import functools
import logging
import time
from typing import Callable, Any

def debug_trace(logger: logging.Logger) -> Callable:
    """Decorator to trace method execution."""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            logger.debug(
                f"Entering {func.__name__}",
                extra={
                    'args': str(args)[:200],  # Truncate long args
                    'kwargs': str(kwargs)[:200]
                }
            )

            start = time.perf_counter()
            try:
                result = func(*args, **kwargs)
                elapsed = time.perf_counter() - start

                logger.debug(
                    f"Exiting {func.__name__}",
                    extra={
                        'elapsed_ms': elapsed * 1000,
                        'result_type': type(result).__name__
                    }
                )
                return result

            except Exception as e:
                elapsed = time.perf_counter() - start
                logger.error(
                    f"Error in {func.__name__}",
                    extra={
                        'elapsed_ms': elapsed * 1000,
                        'error': str(e),
                        'error_type': type(e).__name__
                    },
                    exc_info=True
                )
                raise

        return wrapper
    return decorator

class TimingContext:
    """Context manager for timing operations."""

    def __init__(self, operation: str, logger: logging.Logger):
        self.operation = operation
        self.logger = logger
        self.start = None

    def __enter__(self):
        self.start = time.perf_counter()
        self.logger.debug(f"Starting {self.operation}")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        elapsed = time.perf_counter() - self.start
        if exc_type:
            self.logger.error(
                f"Failed {self.operation}",
                extra={'elapsed_ms': elapsed * 1000, 'error': str(exc_val)}
            )
        else:
            self.logger.debug(
                f"Completed {self.operation}",
                extra={'elapsed_ms': elapsed * 1000}
            )
```

### 3. Enhanced Service Logging
```python
# Example: src/services/service_factory.py additions
logger = get_logger(__name__)

@debug_trace(logger)
def normalize_validation(self, result: Any) -> dict:
    """Normalize validation with enhanced debugging."""
    with TimingContext("validation_normalization", logger):
        # Log input state
        logger.debug(
            "Normalizing validation result",
            extra={
                'input_type': type(result).__name__,
                'has_score': hasattr(result, 'score') if result else False,
                'correlation_id': correlation_id.get()
            }
        )

        # ... existing normalization logic ...

        # Log output state
        logger.debug(
            "Validation normalized",
            extra={
                'output_score': output.get('overall_score'),
                'violations_count': len(output.get('violations', [])),
                'passed_rules_count': len(output.get('passed_rules', []))
            }
        )

        return output
```

### 4. Debug Mode Configuration
```python
# src/config/debug_config.py
import os
from dataclasses import dataclass

@dataclass
class DebugConfig:
    """Debug configuration settings."""
    enabled: bool = os.getenv('DEBUG_MODE', '').lower() == 'true'
    log_level: str = os.getenv('LOG_LEVEL', 'INFO')
    trace_methods: bool = os.getenv('TRACE_METHODS', '').lower() == 'true'
    log_sql: bool = os.getenv('LOG_SQL', '').lower() == 'true'
    profile_performance: bool = os.getenv('PROFILE_PERF', '').lower() == 'true'
    mask_sensitive_data: bool = os.getenv('MASK_SENSITIVE', 'true').lower() == 'true'

    # Specific debug flags
    debug_validation: bool = os.getenv('DEBUG_VALIDATION', '').lower() == 'true'
    debug_generation: bool = os.getenv('DEBUG_GENERATION', '').lower() == 'true'
    debug_web_lookup: bool = os.getenv('DEBUG_WEB_LOOKUP', '').lower() == 'true'

debug_config = DebugConfig()
```

## Recovery Points

### Identificeer Verloren Debug Info
1. Git diff tussen pre/post refactoring commits
2. Zoek verwijderde logger.debug() statements
3. Identificeer verwijderde print() statements
4. Check voor verloren exception details

### Herstel Prioriteit
1. **Critical**: Error handling en exception logging
2. **High**: Business logic decision points
3. **Medium**: Performance metrics en timings
4. **Low**: Verbose data structure logging

## Test Plan

### Debug Output Verification
```python
# tests/unit/test_debug_logging.py
def test_debug_logging_enabled():
    """Test debug logs are produced when enabled."""
    with override_env('DEBUG_MODE', 'true'):
        # ... perform operation ...
        # Assert debug logs contain expected information

def test_correlation_id_propagation():
    """Test correlation ID flows through system."""
    # Set correlation ID
    # Make requests
    # Verify all logs contain same correlation ID
```

## Rollout Strategy

1. **Phase 1**: Add debug configuration (no behavior change)
2. **Phase 2**: Restore critical debug logs
3. **Phase 3**: Add new structured logging
4. **Phase 4**: Implement tracing decorators
5. **Phase 5**: Performance profiling additions

## Metrics

### Before
- Debug log statements: ~50 (mostly removed)
- Troubleshooting time: 2-4 hours average
- Correlation tracking: None
- Performance visibility: Limited

### After
- Debug log statements: 200+
- Troubleshooting time: 30-60 minutes
- Correlation tracking: 100% coverage
- Performance visibility: All operations > 100ms

## Dependencies
- Python logging module
- contextvars for correlation
- Optional: structlog for enhanced structured logging

## Notities
- P3-14 uit US-193 multi-agent review
- Coordinate met ops team voor log format
- Consider ELK stack integration later
- Update CLAUDE.md met debug conventions

## Links
- [Python Logging Best Practices](https://docs.python.org/3/howto/logging.html)
- [Structured Logging](https://www.structlog.org/)
- [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)
- Parent: US-193