---
id: US-203
epic: EPIC-020
title: Reduce Prompt Token Count from 7250 to 3000 Through Optimization
status: open
priority: CRITICAL
story_points: 5
created: 2025-01-18
updated: 2025-01-18
owner: development-team
applies_to: definitie-app@current
canonical: false
last_verified: 2025-10-02
tags: [performance, optimization, tokens, ai, cost-reduction]
---

# US-203: Reduce Prompt Token Count from 7250 to 3000 Through Optimization

## User Story
**Als** product owner
**Wil ik** dat de prompt tokens gereduceerd worden van 7250 naar 3000
**Zodat** de API kosten met 60% dalen en de response tijd significant verbetert

## Probleem
Huidige prompt inefficiënties:
- **7250 tokens** per definitie generatie
- **Duplicated context** in meerdere prompt secties
- **Onnodige voorbeelden** die niet relevant zijn
- **Verbose instructies** die gecomprimeerd kunnen worden
- **Kosten**: €0.15 per generatie → Target: €0.06

## Acceptance Criteria

### Must Have
- [ ] Token count ≤3000 voor standaard definitie generatie
- [ ] Behoud van definitie kwaliteit (geen regressie)
- [ ] Dynamische prompt assembly based on context
- [ ] Token usage tracking en reporting

### Should Have
- [ ] Prompt caching voor herhaalde patronen
- [ ] Context-aware prompt pruning
- [ ] Token budget allocation per prompt sectie

### Performance Targets
- **Current**: 7250 tokens average
- **Target**: 3000 tokens maximum
- **Cost reduction**: 60% (€0.09 per generatie besparing)
- **Response time**: -40% (minder tokens = snellere response)

## Technical Implementation

### 1. Prompt Deduplication
```python
# src/services/prompt/optimized_prompt_builder.py
class OptimizedPromptBuilder:
    def __init__(self):
        self.base_context = self._compress_base_context()
        self.example_cache = {}

    def build_prompt(self, context: DefinitionContext) -> str:
        """Build optimized prompt with minimal tokens."""
        sections = []

        # Core instruction (compressed)
        sections.append(self._get_compressed_instruction())

        # Dynamic examples (only relevant ones)
        relevant_examples = self._select_relevant_examples(context, max_tokens=500)
        sections.append(relevant_examples)

        # Context (deduplicated)
        unique_context = self._deduplicate_context(context)
        sections.append(unique_context)

        # Target structure (minimal)
        sections.append(self._get_minimal_structure())

        return "\n".join(sections)

    def _compress_base_context(self) -> str:
        """Compress base instructions using abbreviations and references."""
        # Original: 2000 tokens → Compressed: 500 tokens
        return """
        TASK: Genereer juridische definitie
        STIJL: NL wetgeving, actief, geen afkortingen
        STRUCTUUR: [Term] = [genus] + [differentia]
        REGELS: Zie REF#1
        """
```

### 2. Dynamic Example Selection
```python
class ExampleSelector:
    def __init__(self, embeddings_model):
        self.embeddings = embeddings_model
        self.example_db = self._load_examples()

    def select_relevant(self, term: str, max_tokens: int = 500) -> List[str]:
        """Select only most relevant examples based on semantic similarity."""
        term_embedding = self.embeddings.encode(term)

        similarities = []
        for example in self.example_db:
            sim = cosine_similarity(term_embedding, example.embedding)
            similarities.append((sim, example))

        # Sort by relevance and select within token budget
        similarities.sort(reverse=True)
        selected = []
        token_count = 0

        for sim, example in similarities:
            example_tokens = count_tokens(example.text)
            if token_count + example_tokens <= max_tokens:
                selected.append(example.text)
                token_count += example_tokens
            else:
                break

        return selected
```

### 3. Context Compression
```python
class ContextCompressor:
    def compress(self, context: Dict) -> Dict:
        """Compress context by removing redundancy."""
        compressed = {}

        # Remove empty fields
        compressed = {k: v for k, v in context.items() if v}

        # Merge similar fields
        if 'wetsartikel' in compressed and 'juridische_context' in compressed:
            compressed['context'] = f"{compressed['wetsartikel']}: {compressed['juridische_context']}"
            del compressed['wetsartikel']
            del compressed['juridische_context']

        # Abbreviate long texts
        for key, value in compressed.items():
            if isinstance(value, str) and len(value) > 200:
                compressed[key] = self._smart_truncate(value, 200)

        return compressed

    def _smart_truncate(self, text: str, max_length: int) -> str:
        """Truncate text while preserving meaning."""
        if len(text) <= max_length:
            return text
        # Find natural break point
        sentences = text.split('.')
        truncated = ""
        for sentence in sentences:
            if len(truncated) + len(sentence) <= max_length:
                truncated += sentence + "."
            else:
                break
        return truncated + "..."
```

### 4. Prompt Caching Strategy
```python
@lru_cache(maxsize=100)
def get_cached_prompt_section(section_type: str, params_hash: str) -> str:
    """Cache frequently used prompt sections."""
    # Cache common sections like instructions, structures
    pass

class PromptCache:
    def __init__(self):
        self.cache = {}
        self.hit_rate = 0
        self.total_requests = 0

    def get_or_generate(self, key: str, generator_fn: Callable) -> str:
        """Get cached prompt or generate new one."""
        self.total_requests += 1

        if key in self.cache:
            self.hit_rate += 1
            return self.cache[key]

        result = generator_fn()
        self.cache[key] = result
        return result

    def get_stats(self) -> Dict:
        """Return cache statistics."""
        return {
            'hit_rate': self.hit_rate / max(self.total_requests, 1),
            'cache_size': len(self.cache),
            'memory_usage': sum(len(v) for v in self.cache.values())
        }
```

## Token Budget Allocation

| Section | Current | Target | Reduction |
|---------|---------|---------|-----------|
| Base Instructions | 2000 | 500 | 75% |
| Examples | 2500 | 800 | 68% |
| Context | 1500 | 700 | 53% |
| Structure Template | 800 | 300 | 62% |
| Validation Rules | 450 | 200 | 55% |
| **Total** | **7250** | **2500** | **65%** |

## Test Scenarios

### Quality Assurance Tests
```python
def test_definition_quality_maintained():
    """Ensure optimized prompts produce same quality."""
    test_terms = ["Verweerder", "Rechtspersoon", "Dwangsom"]

    for term in test_terms:
        original = generate_with_original_prompt(term)
        optimized = generate_with_optimized_prompt(term)

        # Quality metrics
        assert similarity_score(original, optimized) > 0.85
        assert passes_validation(optimized)
        assert token_count(optimized_prompt) < 3000

def test_token_counting_accuracy():
    """Verify token counting matches OpenAI's tokenizer."""
    prompt = build_optimized_prompt(sample_context)
    our_count = count_tokens(prompt)
    openai_count = tiktoken.encoding_for_model("gpt-4").encode(prompt)
    assert abs(our_count - len(openai_count)) < 50  # Within margin
```

## Key Files to Modify
1. `src/services/prompt/prompt_service_v2.py` - Core prompt building
2. `src/services/ai/ai_service_v2.py` - Token counting integration
3. `config/prompts/` - Prompt templates
4. `src/utils/token_counter.py` - New token utilities

## Definition of Done

- [ ] Token count consistently ≤3000 for all definition types
- [ ] Quality metrics show no degradation (>85% similarity)
- [ ] Token usage tracking implemented and visible in UI
- [ ] Cost reduction verified through API usage logs
- [ ] Response time improvement measured and documented
- [ ] Prompt caching shows >50% hit rate in production
- [ ] A/B testing completed comparing original vs optimized

## Dependencies
- US-201 and US-202 should be completed for accurate performance measurement

## Risks & Mitigations
| Risk | Impact | Mitigation |
|------|---------|------------|
| Quality degradation | HIGH | A/B testing, gradual rollout |
| Edge cases missing context | MEDIUM | Fallback to expanded prompt if needed |
| Token counting inaccuracy | LOW | Use official tiktoken library |

## Cost Impact Analysis
- **Current monthly cost**: ~€450 (3000 definitions × €0.15)
- **Target monthly cost**: ~€180 (3000 definitions × €0.06)
- **Monthly savings**: €270
- **Annual savings**: €3240

## Notes
- Most critical optimization for cost reduction
- Consider implementing progressive enhancement (start minimal, add if needed)
- Future: Implement prompt fine-tuning for even better compression
- Monitor quality metrics closely during rollout

## Implementation Phases
1. **Phase 1**: Deduplication and compression (Week 1)
2. **Phase 2**: Dynamic example selection (Week 1-2)
3. **Phase 3**: Caching layer implementation (Week 2)
4. **Phase 4**: A/B testing and quality validation (Week 2)

## Progress Tracking
- [ ] Token analysis completed
- [ ] Optimization strategy approved
- [ ] Implementation started
- [ ] Unit tests written
- [ ] Quality benchmarks established
- [ ] A/B testing framework ready
- [ ] Gradual rollout plan defined
- [ ] Production metrics dashboard ready